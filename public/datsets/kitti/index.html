<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: January 4, 2024 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  

  
  
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.d060e36f065b14306ff371728665eb02.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  

  <meta name="google-site-verification" content="Zv4l_ljWZhu4o0Z-kfZwQmuokpt40AvKXA78N8kynpc" />





<script async src="https://www.googletagmanager.com/gtag/js?id=G-55GQYC5GYC"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-55GQYC5GYC', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>




<script>
  (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','G-55GQYC5GYC');
</script>




















  
  
  






  <meta name="author" content="Arman Asgharpoor Golroudbari" />





  

<meta name="description" content="⇐ Datasets
Introduction Data Format Downloading the Dataset Using the KITTI Dataset in Python Prerequisites Install the Required Libraries Load the Dataset Understanding Calibration and Timestamp Data in 3D Vision Applications Intrinsic Matrix Extrinsic Matrix Calibration Data (calib." />



<link rel="alternate" hreflang="en-us" href="https://armanasq.github.io/datsets/kitti/" />
<link rel="canonical" href="https://armanasq.github.io/datsets/kitti/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@wowchemy" />
  <meta property="twitter:creator" content="@wowchemy" />
<meta property="twitter:image" content="https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="" />
<meta property="og:url" content="https://armanasq.github.io/datsets/kitti/" />
<meta property="og:title" content="KITTI Dataset | " />
<meta property="og:description" content="⇐ Datasets
Introduction Data Format Downloading the Dataset Using the KITTI Dataset in Python Prerequisites Install the Required Libraries Load the Dataset Understanding Calibration and Timestamp Data in 3D Vision Applications Intrinsic Matrix Extrinsic Matrix Calibration Data (calib." /><meta property="og:image" content="https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2023-04-01T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2023-04-01T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://armanasq.github.io/datsets/kitti/"
  },
  "headline": "KITTI Dataset",
  
  "datePublished": "2023-04-01T00:00:00Z",
  "dateModified": "2023-04-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Arman Asgharpoor Golroudbari"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "⇐ Datasets\nIntroduction Data Format Downloading the Dataset Using the KITTI Dataset in Python Prerequisites Install the Required Libraries Load the Dataset Understanding Calibration and Timestamp Data in 3D Vision Applications Intrinsic Matrix Extrinsic Matrix Calibration Data (calib."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>KITTI Dataset | </title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="4abe7ef5678df1e3f51ff93392b0b278" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/"></a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/"></a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/robotic"><span>Robotic</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/publication"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/certificates"><span>Certificates</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="mailto:a.asgharpoor1993@gmail.com" data-toggle="tooltip" data-placement="bottom" title="Drop me an email."  aria-label="Drop me an email.">
                <i class="fas fa-envelope" aria-hidden="true"></i>
              </a>
            </li>
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="https://github.com/armanasq" data-toggle="tooltip" data-placement="bottom" title="Follow Me on GitHub." target="_blank" rel="noopener" aria-label="Follow Me on GitHub.">
                <i class="fab fa-github" aria-hidden="true"></i>
              </a>
            </li>
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>KITTI Dataset</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 1, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    62 min read
  </span>
  

  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <p><img style="width:100%; margin: 0 auto;" src="/images/kitti-cover.png" alt="KITTI Datasets" /></p>
<p><a href="/datasets/">⇐ Datasets</a></p>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#data-format">Data Format</a></li>
<li><a href="#downloading-the-dataset">Downloading the Dataset</a></li>
<li><a href="#using-the-kitti-dataset-in-python">Using the KITTI Dataset in Python</a>
<ul>
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#install-the-required-libraries">Install the Required Libraries</a></li>
<li><a href="#load-the-dataset">Load the Dataset</a></li>
</ul>
</li>
<li><a href="#understanding-calibration-and-timestamp-data-in-3d-vision-applications">Understanding Calibration and Timestamp Data in 3D Vision Applications</a>
<ul>
<li><a href="#intrinsic-matrix">Intrinsic Matrix</a></li>
<li><a href="#extrinsic-matrix">Extrinsic Matrix</a></li>
<li><a href="#calibration-data-calibtxt">Calibration Data (calib.txt):</a></li>
<li><a href="#timestamp-data-timestxt">Timestamp Data (times.txt):</a></li>
</ul>
</li>
<li><a href="#processing-image-data-with-timestamps">Processing Image Data with Timestamps</a></li>
<li><a href="#depth-maps-and-visual-odometry">Depth Maps and Visual Odometry</a></li>
<li><a href="#calculating-baseline-and-focal-length-in-stereo-vision">Calculating Baseline and Focal Length in Stereo Vision</a>
<ul>
<li><a href="#baseline-calculation">Baseline Calculation</a></li>
<li><a href="#focal-length-calculation">Focal Length Calculation</a></li>
</ul>
</li>
<li><a href="#calculating-depth-from-stereo-pair-of-images">Calculating Depth from Stereo Pair of Images</a>
<ul>
<li><a href="#rectification">Rectification</a></li>
<li><a href="#correspondence-matching">Correspondence Matching</a></li>
<li><a href="#disparity-computation">Disparity Computation</a></li>
<li><a href="#depth-estimation">Depth Estimation</a></li>
<li><a href="#depth-refinement">Depth Refinement</a></li>
</ul>
</li>
<li><a href="#math-behind-stereo-depth">Math behind Stereo Depth</a>
<ul>
<li><a href="#stereobm-vs-stereosgbm">StereoBM Vs. StereoSGBM</a></li>
</ul>
</li>
<li><a href="#feature-extraction">Feature Extraction</a>
<ul>
<li><a href="#1-harris-corner-detection">1. Harris Corner Detection</a></li>
<li><a href="#2-shi-tomasi-corner-detector--good-features-to-track">2. Shi-Tomasi Corner Detector &amp; Good Features to Track</a></li>
<li><a href="#3-scale-invariant-feature-transform-sift">3. Scale-Invariant Feature Transform (SIFT)</a></li>
<li><a href="#4-speeded-up-robust-features-surf">4. Speeded-Up Robust Features (SURF)</a></li>
<li><a href="#5-fast-algorithm-for-corner-detection">5. FAST Algorithm for Corner Detection</a></li>
<li><a href="#6-brief-binary-robust-independent-elementary-features">6. BRIEF (Binary Robust Independent Elementary Features)</a></li>
<li><a href="#orb-oriented-fast-and-rotated-brief">ORB (Oriented FAST and Rotated BRIEF)</a></li>
</ul>
</li>
<li><a href="#feature-matching">Feature Matching</a>
<ul>
<li><a href="#descriptor-distance-measures">Descriptor Distance Measures</a></li>
<li><a href="#matching-techniques">Matching Techniques</a></li>
<li><a href="#code-example-feature-matching-with-brute-force-matching-and-euclidean-distance">Code Example: Feature Matching with Brute-Force Matching and Euclidean Distance</a></li>
<li><a href="#point-cloud-to-image-projection">Point Cloud to Image Projection</a></li>
<li><a href="#visualizing-lidar-pointcloud">Visualizing LiDAR Pointcloud</a></li>
<li><a href="#estimate-camera-motion">Estimate Camera Motion</a>
<ul>
<li><a href="#feature-point-correspondence">Feature Point Correspondence</a></li>
<li><a href="#camera-motion-estimation">Camera Motion Estimation</a></li>
<li><a href="#camera-motion-representation">Camera Motion Representation</a></li>
</ul>
</li>
<li><a href="#visual-odometry">Visual Odometry</a>
<ul>
<li><a href="#pipeline-overview">Pipeline Overview</a></li>
<li><a href="#feature-based-visual-odometry">Feature-Based Visual Odometry</a></li>
<li><a href="#visual-odometry-formulation">Visual Odometry Formulation</a></li>
<li><a href="#code-example">Code Example</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>KITTI is a popular computer vision dataset designed for autonomous driving research. It contains a diverse set of challenges for researchers, including object detection, tracking, and scene understanding. The dataset is derived from the autonomous driving platform developed by the Karlsruhe Institute of Technology and the Toyota Technological Institute at Chicago.</p>
<p>The KITTI dataset includes a collection of different sensors and modalities, such as stereo cameras, LiDAR, and GPS/INS sensors, which provides a comprehensive view of the environment around the vehicle. The data was collected over several days in the urban areas of Karlsruhe and nearby towns in Germany. The dataset includes more than 200,000 stereo images and their corresponding point clouds, as well as data from the GPS/INS sensors, which provide accurate location and pose information.</p>
<p>The dataset is divided into several different categories, each with its own set of challenges. These categories include object detection, tracking, scene understanding, visual odometry, and road/lane detection. Each category contains a set of challenges that researchers can use to evaluate their algorithms and compare their results with others in the field.</p>
<p>One of the strengths of the KITTI dataset is its accuracy and precision. The sensors used to collect the data provide a high level of detail and accuracy, making it possible to detect and track objects with high precision. Additionally, the dataset includes a large number of real-world scenarios, which makes it more representative of real-world driving conditions.</p>
<p>Another strength of the KITTI dataset is its large size. The dataset includes over 50 GB of data, which includes stereo images, point clouds, and GPS/INS data. This large amount of data makes it possible to train deep neural networks, which are known to perform well on large datasets.</p>
<p>Despite its strengths, the KITTI dataset also has some limitations. For example, the dataset only covers urban driving scenarios, which may not be representative of driving conditions in other environments. Additionally, the dataset is relatively small compared to other computer vision datasets, which may limit its applicability in certain domains.</p>
<p>In summary, the KITTI dataset is a valuable resource for researchers in the field of autonomous driving. Its accuracy, precision, and large size make it an ideal dataset for evaluating and comparing algorithms for object detection, tracking, and scene understanding. While it has some limitations, its strengths make it a popular and widely used dataset in the field.</p>
<h2 id="data-format">Data Format</h2>
<p>The KITTI dataset is available in two formats: raw data and preprocessed data. The raw data contains a large amount of sensor data, including images, LiDAR point clouds, and GPS/IMU measurements, and can be used for various research purposes. The preprocessed data provides more structured data, including object labels, and can be used directly for tasks such as object detection and tracking.</p>
<h2 id="downloading-the-dataset">Downloading the Dataset</h2>
<p>The KITTI dataset can be downloaded from the official website (http://www.cvlibs.net/datasets/kitti/) after registering with a valid email address. The website provides instructions on how to download and use the data, including the required software tools and file formats.</p>
<h2 id="using-the-kitti-dataset-in-python">Using the KITTI Dataset in Python</h2>
<h3 id="prerequisites">Prerequisites</h3>
<p>Before getting started, make sure you have the following prerequisites:</p>
<ol>
<li>Python 3.x installed</li>
<li>NumPy and Matplotlib libraries installed</li>
<li>KITTI dataset downloaded and extracted</li>
<li>OpenCV: for image and video processing</li>
<li>Pykitti: a Python library for working with the KITTI dataset</li>
</ol>
<h3 id="install-the-required-libraries">Install the Required Libraries</h3>
<p>Once you have downloaded the dataset, you will need to install the required libraries to work with it in Python. The following libraries are commonly used:</p>
<p>NumPy: for numerical operations and array manipulation
OpenCV: for image and video processing
Matplotlib: for data visualization
You can install these libraries using pip, the Python package manager, by running the following commands in your terminal:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">pip install numpy
</span></span><span class="line"><span class="cl">pip install opencv-python
</span></span><span class="line"><span class="cl">pip install matplotli
</span></span><span class="line"><span class="cl">pip install plotly
</span></span><span class="line"><span class="cl">pip install glob
</span></span><span class="line"><span class="cl">pip install progressbar
</span></span></code></pre></div><h3 id="load-the-dataset">Load the Dataset</h3>
<p>The KITTI Odometry dataset consists of multiple sequences, each containing a set of stereo image pairs and corresponding ground truth poses. We will load the data using the <code class="language-plaintext highlighter-rouge">cv2</code> library in Python.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Import the libraries</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">cv2</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">glob</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the paths to the data</span>
</span></span><span class="line"><span class="cl"><span class="n">path_img</span> <span class="o">=</span> <span class="s2">&#34;./data_odometry_gray/dataset/sequences/&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">path_pose</span> <span class="o">=</span> <span class="s2">&#34;./data_odometry_poses/dataset/poses/&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load the pose data using pandas</span>
</span></span><span class="line"><span class="cl"><span class="n">num</span> <span class="o">=</span> <span class="s2">&#34;00&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">poses</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path_pose</span> <span class="o">+</span> <span class="p">(</span><span class="s2">&#34;</span><span class="si">%s</span><span class="s2">.txt&#34;</span> <span class="o">%</span> <span class="n">num</span><span class="p">),</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Extract the ground truth coordinates from the pose data</span>
</span></span><span class="line"><span class="cl"><span class="n">gt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">poses</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">poses</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Extracting spatial coordinates from the gt tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">gt</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">3</span><span class="p">][:,</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">gt</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">3</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">gt</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">3</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Creating a 3D scatter plot using Plotly</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;lines&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Customize the layout of the plot</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">scene</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">zaxis_title</span><span class="o">=</span><span class="s1">&#39;Z&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">camera</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">eye</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="mf">0.6</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">projection</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">&#39;perspective&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">bgcolor</span><span class="o">=</span><span class="s1">&#39;whitesmoke&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">xaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">showgrid</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gridwidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">gridcolor</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">yaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">showgrid</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gridwidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">gridcolor</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">zaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">showgrid</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gridwidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">gridcolor</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Add a title to the plot for better context</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Spatial Trajectory&#39;</span><span class="p">,</span> <span class="n">title_font</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the spatial trajectory plot</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><iframe src="/kitti-gt-00.html" width="800" height="600" frameborder="0"></iframe>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Load and display a test image</span>
</span></span><span class="line"><span class="cl"><span class="n">test_img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">path_img</span> <span class="o">+</span> <span class="n">num</span> <span class="o">+</span> <span class="s1">&#39;/image_0/000000.png&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">test_img</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Test Image&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><img class="myImg" src="/kitti/00.png">
<h2 id="understanding-calibration-and-timestamp-data-in-3d-vision-applications">Understanding Calibration and Timestamp Data in 3D Vision Applications</h2>
<p>In 3D vision applications, accurate calibration and synchronized timestamps play a crucial role in aligning and mapping data from multiple sensors. In following, we will explore the contents of two important files: calib.txt and times.txt. These files provide essential information for camera calibration and synchronization of image pairs. Understanding these concepts is vital for accurately transforming and processing data in 3D vision applications.</p>
<p>In computer vision and camera calibration, intrinsic and extrinsic matrices are used to describe the properties and transformations of a camera.</p>
<h3 id="intrinsic-matrix">Intrinsic Matrix</h3>
<p>The intrinsic matrix, denoted as K, represents the internal properties of a camera which describes the mapping between the 3D world coordinates and the 2D image plane coordinates of a camera without considering any external factors. It includes parameters such as focal length $(f_x, f_y)$ principal point $(c_x, c_y)$, and skew $(s)$ which define the camera&rsquo;s optical properties, including how the image is formed on the camera sensor. The intrinsic matrix is typically a 3x3 matrix:</p>
<div>
$$
K = \begin{bmatrix}
    f_x & s & c_x \\
    0 & f_y & c_y \\
    0 & 0 & 1 \\
\end{bmatrix}
$$
</div>
<h3 id="extrinsic-matrix">Extrinsic Matrix</h3>
<p>The extrinsic matrix represents the external properties or transformations of a camera, specifically its position and orientation in the world which describes the transformation from the camera&rsquo;s coordinate system to a global coordinate system. It includes rotation and translation parameters. The rotation matrix, denoted as R, represents the rotation of the camera, while the translation vector, denoted as t, represents the translation (displacement) of the camera in the global coordinate system. The extrinsic matrix is typically a 3x4 matrix:</p>
<p>$$
\begin{bmatrix}
R &amp; | &amp;t
\end{bmatrix}
$$</p>
<p>The extrinsic matrix allows us to map points from the camera&rsquo;s coordinate system to the global coordinate system or vice versa. It is used to transform 3D world coordinates into the camera&rsquo;s coordinate system, enabling the projection of these points onto the image plane.</p>
<p>In summary, the intrinsic matrix captures the internal properties of a camera, such as focal length and principal point, while the extrinsic matrix describes the camera&rsquo;s position and orientation in the world. Together, these matrices enable the transformation between 3D world coordinates and 2D image plane coordinates.</p>
<h3 id="calibration-data-calibtxt">Calibration Data (calib.txt):</h3>
<p>The calib.txt file contains calibration data for the cameras used in the system, providing the projection matrices and a transformation matrix necessary for mapping points between different coordinate systems. Let&rsquo;s break down the contents of the calib.txt file:</p>
<ul>
<li>Projection Matrices ($P_0$, $P_1$, $P_2$, $P_3$):
The projection matrices, $P_0$, $P_1$, $P_2$, and $P_3$, are $3 \times 4$ matrices that represent the transformation from 3D world coordinates to 2D image plane coordinates for each camera in the system. $P_0$ corresponds to the left camera, $P_1$ corresponds to the right camera, $P_2$ corresponds to the third camera, and $P_3$ corresponds to the fourth camera. These matrices are crucial for rectifying and projecting the 3D point clouds onto the image planes which contain intrinsic information about each camera&rsquo;s focal length and optical center. Moreover, they also include transformation information that relates the coordinate frames of each camera to the global coordinate frame.</li>
</ul>
<p>The projection matrix could be defined as follows:</p>
<p>$$
\begin{equation}
P = K\begin{bmatrix}
R  | t
\end{bmatrix}
\end{equation}
$$</p>
<p>The projection matrix enables the projection of 3D coordinates from the global frame onto the image plane of the camera using the following equation:</p>
<div>
$$
\begin{equation}
\lambda \begin{bmatrix}
u \\ v \\ 1
\end{bmatrix} = P\begin{bmatrix}
    X \\ Y \\ Z \\ 1
\end{bmatrix}
\end{equation}
$$
</div>
<p>Here, $\lambda$ represents a scaling factor, $(u, v)$ are the coordinates of the projected point on the image plane, and $(X, Y, Z)$ are the coordinates of the 3D point in the global frame. The projection matrix $P$ encapsulates both the intrinsic and extrinsic parameters of the camera, allowing for the transformation between 3D world coordinates and 2D image plane coordinates.</p>
<p>By decomposing the projection matrix $P$ into intrinsic and extrinsic camera matrices, we can obtain a more explicit description of the process involved in projecting a 3D point from any coordinate frame onto the pixel coordinate frame of the camera. This breakdown allows us to separate the intrinsic parameters, which capture the internal characteristics of the camera such as focal length and optical center, from the extrinsic parameters that define the camera&rsquo;s position and orientation in the global coordinate system. By considering the intrinsic and extrinsic matrices individually, we gain a deeper understanding of how the camera&rsquo;s internal properties and its relationship to the global coordinate system influence the projection process.</p>
<div>
$$
\begin{equation}
\begin{bmatrix}
u \\ v \\ 1
\end{bmatrix} =  \frac{1}{\lambda} K \begin{bmatrix}R|t\end{bmatrix} \begin{bmatrix}X \\ Y \\ Z \\ 1 \end{bmatrix}
\end{equation}
$$
</div>
<p>Before examining the provided code, let&rsquo;s set some expectations based on standard projection matrices for each camera. If the projection matrices follow the standard convention, we would anticipate the extrinsic matrix to transform a point from the global coordinate frame into the camera&rsquo;s own coordinate frame. To illustrate this, let&rsquo;s consider the scenario where we take the origin of the global coordinate frame (corresponding to the origin of the left grayscale camera) and translate it into the coordinate frame of the right grayscale camera. In this case, we would expect the X coordinate to be -0.54 since the left camera&rsquo;s origin is positioned 0.54 meters to the left of the right camera&rsquo;s origin. To verify this expectation, we can decompose the projection matrix provided for the right camera into intrinsic matrix (k), rotation matrix (R), and translation vector (t) using the helpful function available in OpenCV. By printing these decomposed matrices and vectors, we can examine their values to determine if they align with our expectations.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Extracting intrinsic and extrinsic parameters from the projection matrix P1</span>
</span></span><span class="line"><span class="cl"><span class="n">P1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">calib</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;P1:&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">k1</span><span class="p">,</span> <span class="n">r1</span><span class="p">,</span> <span class="n">t1</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">decomposeProjectionMatrix</span><span class="p">(</span><span class="n">P1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">t1</span> <span class="o">=</span> <span class="n">t1</span> <span class="o">/</span> <span class="n">t1</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Displaying the results</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Intrinsic Matrix:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">k1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Rotation Matrix:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Translation Vector:&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">t1</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</span></span></code></pre></div><p>This code segment extracts the intrinsic and extrinsic parameters from the projection matrix <code>P1</code>. The <code>decomposeProjectionMatrix</code> function from the OpenCV library is used to decompose the matrix into the intrinsic matrix <code>k1</code>, rotation matrix <code>r1</code>, translation vector <code>t1</code>, and other related information. The translation vector is then normalized by dividing it by its fourth component. The extracted matrices and vector are then displayed, providing insight into the intrinsic properties (intrinsic matrix) of the camera, as well as its orientation (rotation matrix) and position (translation vector) in the 3D space.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">[[718.856    0.     607.1928]
</span></span><span class="line"><span class="cl"> [  0.     718.856  185.2157]
</span></span><span class="line"><span class="cl"> [  0.       0.       1.    ]]
</span></span><span class="line"><span class="cl">Rotation Matrix:
</span></span><span class="line"><span class="cl">[[1. 0. 0.]
</span></span><span class="line"><span class="cl"> [0. 1. 0.]
</span></span><span class="line"><span class="cl"> [0. 0. 1.]]
</span></span><span class="line"><span class="cl">Translation Vector:
</span></span><span class="line"><span class="cl">[[ 0.5372]
</span></span><span class="line"><span class="cl"> [ 0.    ]
</span></span><span class="line"><span class="cl"> [-0.    ]
</span></span><span class="line"><span class="cl"> [ 1.    ]]
</span></span></code></pre></div><p>Now that we understand the calibration matrices and their reference to the left grayscale camera&rsquo;s image plane, let&rsquo;s revisit the projection equation and discuss the lambda $ (λ)$ value. Lambda represents the depth of a 3D point after applying the transformation $[R|t]$ to the point. It signifies the point&rsquo;s distance from the camera. Since we are projecting onto a 2D plane, dividing each point by its depth effectively projects them onto a plane located one unit away from the camera&rsquo;s origin along the Z-axis, resulting in a Z value of 1 for all points. It is important to note that the division by lambda can be performed at any point during the operations without affecting the final result. We will demonstrate this concept by performing the computations in Python.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">some_point</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">transformed_point</span> <span class="o">=</span> <span class="n">gt</span><span class="p">[</span><span class="mi">14</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">some_point</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">depth_from_cam</span> <span class="o">=</span> <span class="n">transformed_point</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Original point:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">some_point</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Transformed point:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">transformed_point</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Depth from camera:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">depth_from_cam</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</span></span></code></pre></div><p>In this code, <code>some_point</code> represents a point measured in the coordinate frame of the left camera at its 14th pose. We then transform this point to the camera&rsquo;s coordinate frame using the transformation matrix <code>gt[14]</code> and extract the Z coordinate to obtain the depth from the camera. The code prints the original point, the transformed point, and the depth from the camera.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Original point:
</span></span><span class="line"><span class="cl"> [[1]
</span></span><span class="line"><span class="cl"> [2]
</span></span><span class="line"><span class="cl"> [3]
</span></span><span class="line"><span class="cl"> [1]]
</span></span><span class="line"><span class="cl">Transformed point:
</span></span><span class="line"><span class="cl"> [[ 0.2706]
</span></span><span class="line"><span class="cl"> [ 1.5461]
</span></span><span class="line"><span class="cl"> [15.0755]]
</span></span><span class="line"><span class="cl">Depth from camera:
</span></span><span class="line"><span class="cl"> [15.0755]
</span></span></code></pre></div><p>To project a 3D point onto the image plane, we have two approaches: either applying the intrinsic matrix after dividing by the depth or dividing by the depth first and then multiplying by the intrinsic matrix. Here&rsquo;s the code to demonstrate both ways:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Multiplying by intrinsic matrix k, then dividing by depth</span>
</span></span><span class="line"><span class="cl"><span class="n">pixel_coordinates1</span> <span class="o">=</span> <span class="n">k1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">transformed_point</span><span class="p">)</span> <span class="o">/</span> <span class="n">depth_from_cam</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Dividing by depth then multiplying by intrinsic matrix k</span>
</span></span><span class="line"><span class="cl"><span class="n">pixel_coordinates2</span> <span class="o">=</span> <span class="n">k1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">transformed_point</span> <span class="o">/</span> <span class="n">depth_from_cam</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Pixel Coordinates (Approach 1):&#39;</span><span class="p">,</span> <span class="n">pixel_coordinates1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Pixel Coordinates (Approach 2):&#39;</span><span class="p">,</span> <span class="n">pixel_coordinates2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span></span></code></pre></div><p>In this code, <code>pixel_coordinates1</code> represents the pixel coordinates obtained by first multiplying the transformed point by the intrinsic matrix <code>k1</code> and then dividing by the depth. Similarly, <code>pixel_coordinates2</code> represents the pixel coordinates obtained by dividing the transformed point by the depth and then multiplying by <code>k1</code>. The code prints the pixel coordinates for both approaches.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Pixel Coordinates (Approach 1): [[620.09802465 258.93763336   1.        ]]
</span></span><span class="line"><span class="cl">Pixel Coordinates (Approach 2): [[620.09802465 258.93763336   1.        ]]
</span></span></code></pre></div><ul>
<li>
<p>Transformation Matrix $(T_r)$:
The transformation matrix, $T_r$, performs the conversion from the Velodyne scanner&rsquo;s coordinate system to the left rectified camera&rsquo;s coordinate system. This transformation is necessary to map a point from the Velodyne scanner to the corresponding point in the left image plane.</p>
</li>
<li>
<p>Mapping a Point:
To map a point $X$ from the Velodyne scanner to a point $x$ in the $i_{th}$ image plane, you need to perform the following transformation:</p>
</li>
</ul>
<p>$$
\begin{equation}
x = P_i \cdot Tr \cdot X
\end{equation}
$$</p>
<p>Here, $P_i$ represents the projection matrix of the $i_{th}$ camera, and $X$ is the point in the Velodyne scanner&rsquo;s coordinate system.</p>
<h3 id="timestamp-data-timestxt">Timestamp Data (times.txt):</h3>
<p>The times.txt file provides timestamps for synchronized image pairs in the KITTI dataset. These timestamps are expressed in seconds and are crucial for analyzing the dynamics of the vehicle and understanding the timing of image acquisition. By considering the timing information, you can accurately associate images from different cameras, Lidar scans, or other sensors that are synchronized with the image acquisition system.</p>
<p>To access and utilize the timestamps in times.txt, you can read the file and load it into a suitable data structure in Python. Here&rsquo;s an example using the pandas library:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Read the times.txt file</span>
</span></span><span class="line"><span class="cl"><span class="n">times</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path_img</span> <span class="o">+</span> <span class="n">num</span> <span class="o">+</span><span class="s1">&#39;/times.txt&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the first few rows</span>
</span></span><span class="line"><span class="cl"><span class="n">times</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</span></span></code></pre></div><p>By loading the timestamps into a dataframe, you can effectively analyze the relationships between different sensor data and synchronize them based on the timestamps. This allows for accurate alignment and fusion of data from multiple sensors, enabling more robust and comprehensive analysis in 3D vision applications.</p>
<h2 id="processing-image-data-with-timestamps">Processing Image Data with Timestamps</h2>
<p>Once you have the synchronized timestamps from the times.txt file, you can use them to associate images from different cameras or perform temporal analysis of the data. Here&rsquo;s an example of how you can process image data with timestamps in Python:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">cv2</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Read the times.txt file</span>
</span></span><span class="line"><span class="cl"><span class="n">times</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path_img</span> <span class="o">+</span> <span class="n">num</span> <span class="o">+</span> <span class="s1">&#39;/times.txt&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the path to the image directory</span>
</span></span><span class="line"><span class="cl"><span class="n">image_dir</span> <span class="o">=</span> <span class="n">path_img</span> <span class="o">+</span> <span class="n">num</span> <span class="o">+</span> <span class="s1">&#39;/image_0/&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Get a list of image files in the directory</span>
</span></span><span class="line"><span class="cl"><span class="n">image_files</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">image_dir</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Iterate over the image files and process them</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">image_file</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">image_files</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Construct the image file path</span>
</span></span><span class="line"><span class="cl">    <span class="n">image_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">image_dir</span><span class="p">,</span> <span class="n">image_file</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Load and process the image</span>
</span></span><span class="line"><span class="cl">    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">image_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Get the corresponding timestep</span>
</span></span><span class="line"><span class="cl">    <span class="n">timestep</span> <span class="o">=</span> <span class="n">times</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Draw the timestep on the image</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv2</span><span class="o">.</span><span class="n">putText</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Timestep: </span><span class="si">{</span><span class="n">timestep</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Perform image processing tasks</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Display the image or perform further analysis</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s1">&#39;Image&#39;</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Wait for a key press (maximum delay of 10 milliseconds)</span>
</span></span><span class="line"><span class="cl">    <span class="n">key</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Break the loop if the &#39;ESC&#39; key is pressed (ASCII code 27)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">25</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">break</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="c1"># Release any open windows</span>
</span></span><span class="line"><span class="cl"><span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</span></span></code></pre></div><p>In this example, we first read the timestamps from the times.txt file using pandas. Then, we iterate over the timestamps and process the corresponding images. Inside the loop, we format the timestamp and construct the image filename based on the timestamp. We load the image using OpenCV&rsquo;s <code>cv2.imread()</code> function and perform any desired image processing tasks. You can apply various computer vision techniques, such as object detection, image segmentation, or feature extraction, to analyze the images.</p>
<p>After processing the image, you can display it using OpenCV&rsquo;s <code>cv2.imshow()</code> function or perform further analysis based on your requirements. In this example, we break the loop after processing a few images (for demonstration purposes) by checking the iteration count. However, you can remove this condition to process all the images.</p>
<p>Finally, we release any open windows using <code>cv2.destroyAllWindows()</code> to clean up after processing all the images.</p>
<p>By using the synchronized timestamps, you can ensure that the images from different cameras or sensors are processed and analyzed in the correct temporal order, enabling more accurate and meaningful results in your 3D vision applications.</p>
<p>Below is an example of a dataset handling object that can help in accessing and processing the data, while also explicitly decoding the Velodyne binaries as float32:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Dataset_Handler</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence</span><span class="p">,</span> <span class="n">lidar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl">        <span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl">        <span class="kn">import</span> <span class="nn">cv2</span>
</span></span><span class="line"><span class="cl">        <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">        <span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lidar</span> <span class="o">=</span> <span class="n">lidar</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">low_memory</span> <span class="o">=</span> <span class="n">low_memory</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">=</span> <span class="s1">&#39;./data_odometry_gray/dataset/sequences/</span><span class="si">{}</span><span class="s1">/&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">poses_dir</span> <span class="o">=</span> <span class="s1">&#39;./data_odometry_poses/dataset/poses/</span><span class="si">{}</span><span class="s1">.txt&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">poses</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">poses_dir</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">left_image_files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">right_image_files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">velodyne_files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;velodyne&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_frames</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">left_image_files</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lidar_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;velodyne/&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">calib</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;calib.txt&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">P0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">calib</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;P0:&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">P1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">calib</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;P1:&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">P2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">calib</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;P2:&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">P3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">calib</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;P3:&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">Tr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">calib</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;Tr:&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">times</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;times.txt&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">poses</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">poses</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">gt</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">poses</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">low_memory</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">reset_frames</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">first_image_left</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_0/&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">left_image_files</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">first_image_right</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_1/&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">right_image_files</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">second_image_left</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_0/&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">left_image_files</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lidar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">first_pointcloud</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lidar_path</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">velodyne_files</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                                                    <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                    <span class="n">count</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">imheight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_image_left</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">imwidth</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_image_left</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">progress_bar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_frames</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;Loading Images&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">images_right</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">pointclouds</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name_left</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">left_image_files</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">name_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">right_image_files</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_0/&#39;</span> <span class="o">+</span> <span class="n">name_left</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">images_right</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_1/&#39;</span> <span class="o">+</span> <span class="n">name_right</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lidar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">pointcloud</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lidar_path</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">velodyne_files</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="n">count</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">pointclouds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pointcloud</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">progress_bar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">progress_bar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">bar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">imheight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">imwidth</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">first_image_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">first_image_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_right</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">second_image_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lidar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">first_pointcloud</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointclouds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">progress_bar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_frames</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s1">&#39;Loading Images&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">images_right</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">pointclouds</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name_left</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">left_image_files</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">name_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">right_image_files</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_0/&#39;</span> <span class="o">+</span> <span class="n">name_left</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">images_right</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_1/&#39;</span> <span class="o">+</span> <span class="n">name_right</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lidar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">pointcloud</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lidar_path</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">velodyne_files</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                                             <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                             <span class="n">count</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">pointclouds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pointcloud</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">progress_bar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">progress_bar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">bar</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">imheight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">imwidth</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">first_image_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">first_image_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_right</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">second_image_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lidar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">first_pointcloud</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pointclouds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">reset_frames</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">images_left</span> <span class="o">=</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_0/&#39;</span> <span class="o">+</span> <span class="n">name_left</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                            <span class="k">for</span> <span class="n">name_left</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">left_image_files</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">images_right</span> <span class="o">=</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_dir</span> <span class="o">+</span> <span class="s1">&#39;image_1/&#39;</span> <span class="o">+</span> <span class="n">name_right</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                            <span class="k">for</span> <span class="n">name_right</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">right_image_files</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lidar</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">pointclouds</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lidar_path</span> <span class="o">+</span> <span class="n">velodyne_file</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                            <span class="n">count</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                                <span class="k">for</span> <span class="n">velodyne_file</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">velodyne_files</span><span class="p">)</span>
</span></span></code></pre></div><p>To run that, you can use the code below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">handler</span> <span class="o">=</span> <span class="n">Dataset_Handler</span><span class="p">(</span><span class="s1">&#39;04&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>We used the sequence <code>04</code> as its much smaller than other sequences which makes it proper for the excercise purposes.</p>
<h2 id="depth-maps-and-visual-odometry">Depth Maps and Visual Odometry</h2>
<h2 id="calculating-baseline-and-focal-length-in-stereo-vision">Calculating Baseline and Focal Length in Stereo Vision</h2>
<p>Stereo vision involves using a pair of cameras to capture images of a scene from slightly different viewpoints. By analyzing the disparities between corresponding points in the stereo images, we can estimate depth information and reconstruct the 3D structure of the scene. To perform accurate depth estimation, it is essential to know the baseline (distance between the cameras) and the focal length (effective focal length of the cameras). Let&rsquo;s explore how to calculate these parameters:</p>
<h3 id="baseline-calculation">Baseline Calculation</h3>
<p>The baseline is the distance between the two cameras in the stereo rigw hich determines the scale of the depth information captured by the cameras. To calculate the baseline, we need to determine the translation between the camera coordinate frames.</p>
<p>One way to obtain the translation is by decomposing the projection matrix of one of the cameras using OpenCV&rsquo;s <code>decomposeProjectionMatrix</code> function, which can extract the translation vector $t$ from the projection matrix $P$. The baseline is then given by the absolute value of the x-component of the translation vector:</p>
<p>$$
\begin{equation}
\text{baseline} = \text{abs}(t[0])
\end{equation}
$$</p>
<h3 id="focal-length-calculation">Focal Length Calculation</h3>
<p>The focal length represents the effective focal length of the cameras used in the stereo rig. It determines the scale at which the scene is captured. To calculate the focal length, we need to extract the intrinsic parameters from the camera calibration matrix.</p>
<p>The camera calibration matrix, denoted as $K$, contains the focal length and the principal point coordinates. By accessing the appropriate elements of the calibration matrix, we can obtain the focal length:</p>
<p>$$
\begin{equation}
\text{focal length} = K[0, 0]
\end{equation}
$$</p>
<p>By calculating the baseline and focal length, we can accurately estimate depth information using stereo vision. These parameters are crucial for various applications, such as 3D reconstruction, object detection, and visual odometry.</p>
<p>Remember that accurate calibration of the stereo rig is essential for reliable depth estimation. Calibration involves accurately determining the intrinsic and extrinsic parameters of the cameras. Calibration techniques such as chessboard calibration or pattern-based calibration can be used to obtain precise calibration parameters.</p>
<p>Here&rsquo;s an example Python code snippet that demonstrates how to calculate the baseline and focal length using the provided projection matrix:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">cv2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Projection matrix for the right camera</span>
</span></span><span class="line"><span class="cl"><span class="n">P1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">calib</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;P1:&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Decompose the projection matrix</span>
</span></span><span class="line"><span class="cl"><span class="n">k1</span><span class="p">,</span> <span class="n">r1</span><span class="p">,</span> <span class="n">t1</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">decomposeProjectionMatrix</span><span class="p">(</span><span class="n">P1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">t1</span> <span class="o">=</span> <span class="n">t1</span> <span class="o">/</span> <span class="n">t1</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate the baseline (distance between the cameras)</span>
</span></span><span class="line"><span class="cl"><span class="n">baseline</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">t1</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate the focal point (effective focal length of the cameras)</span>
</span></span><span class="line"><span class="cl"><span class="n">focal_length</span> <span class="o">=</span> <span class="n">k1</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Print the results</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Baseline:&#39;</span><span class="p">,</span> <span class="n">baseline</span><span class="p">,</span> <span class="s1">&#39;meters&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Focal Length:&#39;</span><span class="p">,</span> <span class="n">focal_length</span><span class="p">,</span> <span class="s1">&#39;pixels&#39;</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">For the seq &#34;00&#34;:
</span></span><span class="line"><span class="cl">Baseline: 0.5371657188644179 meters
</span></span><span class="line"><span class="cl">Focal Length: 718.8559999999999 pixels
</span></span></code></pre></div><h2 id="calculating-depth-from-stereo-pair-of-images">Calculating Depth from Stereo Pair of Images</h2>
<p>In stereo vision, depth can be estimated by calculating the disparity between corresponding pixels in a stereo pair of images. The disparity represents the horizontal shift of a pixel between the left and right images and is inversely proportional to the depth of the corresponding 3D point. Let&rsquo;s delve into the technical details and mathematical equations involved in this process:</p>
<h3 id="rectification">Rectification</h3>
<p>Before calculating disparity, it is crucial to rectify the stereo pair of images to ensure that corresponding epipolar lines are aligned. Rectification simplifies the correspondence matching process. It involves finding the epipolar geometry, computing rectification transformations, and warping the images accordingly.</p>
<p>The rectification process transforms the left and right images, denoted as $I_l$ and $I_r$ respectively, into rectified images $I_l&rsquo;$ and $I_r&rsquo;$. This transformation ensures that corresponding epipolar lines in the rectified images are aligned horizontally. The rectification transformations consist of rotation matrices $R_l$ and $R_r$ and translation vectors $T_l$ and $T_r$.</p>
<p>Given a point $(x_l, y_l)$ in the left image, its corresponding point $(x_r, y_r)$ in the right image can be obtained using the rectification transformations as follows:</p>
<div>
$$
\begin{equation}
\begin{pmatrix} x_r \\ y_r \\ 1 \end{pmatrix} = R_r \cdot R_l^T \cdot \begin{pmatrix} x_l \\ y_l \\ 1 \end{pmatrix} + T_r - R_r \cdot T_l
\end{equation}
$$
</div>
<p>After rectification, the corresponding epipolar lines in the rectified images are aligned horizontally, simplifying the correspondence matching process.</p>
<h3 id="correspondence-matching">Correspondence Matching</h3>
<p>Once the stereo images are rectified, the next step is to find corresponding points between the two images. This process is known as correspondence matching or stereo matching. The goal is to find the best matching pixel or pixel neighborhood in the right image for each pixel in the left image.</p>
<p>Various algorithms can be used for correspondence matching, such as block matching, semi-global matching, or graph cuts. These algorithms search for the best matching pixel or pixel neighborhood in the other image based on similarity measures like sum of absolute differences (SAD) or normalized cross-correlation (NCC).</p>
<p>The correspondence matching process involves searching for the disparity value that minimizes the dissimilarity measure between the left and right image patches. Let $P_l$ denote the pixel patch centered at $(x_l, y_l)$ in the left image, and $P_r$ denote the corresponding pixel patch centered at $(x_r, y_r)$ in the right image. The disparity $d$ for the pixel pair $(x_l, y_l)$ and $(x_r, y_r)$ can be computed as:</p>
<p>$$
\begin{equation}
d = x_l - x_r
\end{equation}
$$</p>
<h3 id="disparity-computation">Disparity Computation</h3>
<p>The disparity value represents the shift or offset between the corresponding points in the stereo pair of images. It is computed as the horizontal distance between the pixel coordinates in the rectified images. The disparity can be calculated using the formula:</p>
<p>$$
\begin{equation}
\text{{Disparity}} = x_l - x_r
\end{equation}
$$</p>
<p>where $x_l$ is the x-coordinate of the pixel in the left image, and $x_r$ is the x-coordinate of the corresponding pixel in the right image.</p>
<h3 id="depth-estimation">Depth Estimation</h3>
<p>Once the disparity is computed, the depth can be estimated using the disparity-depth relationship. This relationship is based on the geometry of the stereo camera setup and assumes a known baseline (distance between the left and right camera) and focal length.</p>
<p>Let $B$ denote the baseline (distance between the left and right camera), and $f$ denote the focal length (effective focal length of the cameras). The depth $Z$ can be calculated using the formula:</p>
<p>$$
\begin{equation}
Z = \frac{{B \cdot f}}{{\text{{Disparity}}}}
\end{equation}
$$</p>
<p>where the disparity represents the computed disparity value for the pixel pair.</p>
<p>The resulting depth map provides the depth information for each pixel in the rectified image, representing the 3D structure of the scene.</p>
<h3 id="depth-refinement">Depth Refinement</h3>
<p>The computed depth values may contain noise and outliers. To improve the accuracy and smoothness of the depth map, post-processing techniques can be applied.</p>
<p>One common technique is depth map filtering, where a filter is applied to each pixel or a neighborhood of pixels to refine the depth values. The filter can be based on techniques like weighted median filtering, bilateral filtering, or joint bilateral filtering.</p>
<p>Another approach is depth map inpainting, which fills in missing or erroneous depth values based on the surrounding valid depth values. Inpainting algorithms utilize neighboring information to estimate missing or unreliable depth values.</p>
<p>Guided filtering is another technique that can be used to refine the depth map. It uses a guidance image, such as the rectified left image, to guide the filtering process and preserve edges and details in the depth map.</p>
<p>By applying these post-processing techniques, the accuracy and quality of the depth map can be improved.</p>
<p>By following these steps and applying the corresponding formulas and equations, accurate depth maps can be generated from stereo pairs of images. These depth maps provide valuable information for various applications, including 3D reconstruction, scene understanding, and autonomous navigation.</p>
<p>Remember that the accuracy of depth estimation depends on factors such as image quality, camera calibration, and the chosen correspondence matching algorithm. Experimentation and fine-tuning of parameters may be necessary to achieve optimal results.</p>
<p>Apologies for the oversight. Here&rsquo;s the section you mentioned:</p>
<h2 id="math-behind-stereo-depth">Math behind Stereo Depth</h2>
<p>To review the essentials of the math behind stereo depth,
Using similar triangles, we can write the following equations:</p>
<p>$$
\begin{equation}
\frac{{X}}{{Z}} = \frac{{x_L}}{{f}}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\frac{{X - B}}{{Z}} = \frac{{x_R}}{{f}}
\end{equation}
$$</p>
<p>Where $X$ represents the 3D x-coordinate of the point, $Z$ represents the depth of the point, $x_L$ and $x_R$ represent the x-coordinates of the point in the left and right image planes respectively, $f$ represents the focal length, and $B$ represents the baseline (distance between the cameras).</p>
<p>We define disparity, $d$, as the difference between $x_L$ and $x_R$, which represents the difference in horizontal pixel location of the point projected onto the left and right image planes:</p>
<p>$$
\begin{equation}
d = x_L - x_R
\end{equation}
$$</p>
<p>Rearranging the similar triangles equations, we obtain:</p>
<p>$$
\begin{equation}
\frac{{X}}{{Z}} = \frac{{x_L}}{{f}}
\end{equation}
$$</p>
<p>$$
\begin{equation}
\frac{{X - B}}{{Z}} = \frac{{x_R}}{{f}}
\end{equation}
$$</p>
<p>By substituting $Z \cdot x_L$ into $f \cdot X$ in Equation 2, we get:</p>
<p>$$
\begin{equation}
\frac{{Z \cdot x_L - B \cdot x_L}}{{Z}} = \frac{{x_R}}{{f}}
\end{equation}
$$</p>
<p>This equation can be rewritten as:</p>
<p>$$
\begin{equation}
x_L - \frac{{B \cdot x_L}}{{Z}} = x_R
\end{equation}
$$</p>
<p>Next, we substitute the definition of disparity ($d = x_L - x_R$) into Equation 3 and solve for $Z$:</p>
<p>$$
\begin{equation}
x_L - \frac{{B \cdot x_L}}{{Z}} = x_L - d
\end{equation}
$$</p>
<p>$$
\begin{equation}
\frac{{B \cdot x_L}}{{Z}} = d
\end{equation}
$$</p>
<p>$$
\begin{equation}
\frac{{B}}{{Z}} = \frac{{d}}{{x_L}}
\end{equation}
$$</p>
<p>Finally, by rearranging Equation 4, we can solve for $Z$:</p>
<p>$$
\begin{equation}
Z = \frac{{B \cdot f}}{{d}}
\end{equation}
$$</p>
<p>Note that if the focal length and disparity are measured in pixels, the pixel units will cancel out, and if the baseline is measured in meters, the resulting $Z$ measurement will be in meters, which is desirable for reconstructing 3D coordinates later.</p>
<p>With the baseline (previously found to be 0.54m), the focal length of the x-direction in pixels (previously found to be 718.856px), and the disparity between the same points in the two images, we now have a simple way to compute the depth from our stereo pair of images.</p>
<p>To find the necessary disparity value, we can use the stereo matching algorithms available in OpenCV, such as StereoBM or StereoSGBM. StereoBM is faster, while StereoSGBM produces better results, as we will see in practice.</p>
<p>By applying these equations and utilizing the appropriate stereo matching algorithm, we can estimate the depth accurately and reconstruct 3D coordinates from the stereo pair of images.</p>
<p>At first we define two vital parameters, &ldquo;SAD window&rdquo; and &ldquo;block size&rdquo;. In the context of stereo vision and disparity mapping, the terms &ldquo;SAD window&rdquo; and &ldquo;block size&rdquo; refer to parameters that affect the matching algorithm used to calculate disparities between corresponding points in the left and right stereo images.</p>
<ol>
<li>SAD Window (Sum of Absolute Differences Window):
The SAD window, also known as the matching window or kernel, defines the neighborhood around a pixel in the left image that is considered for matching with the corresponding neighborhood in the right image. It represents the region within which the algorithm searches for similarities to determine the disparity.</li>
</ol>
<p>The size of the SAD window determines the spatial extent of the matching process. A larger window size allows for capturing more context and potentially leads to more accurate disparity estimation, but it also increases computational complexity. The SAD window is usually a square or rectangular region centered around each pixel.</p>
<ol start="2">
<li>Block Size:
The block size specifies the dimensions of the blocks or patches within the SAD window that are compared during the matching process. It defines the size of the local regions used for computing disparities.</li>
</ol>
<p>A larger block size means that more pixels are considered during matching, which can improve robustness against noise and texture variations. However, increasing the block size also increases the computational cost. Typically, the block size is an odd number to have a well-defined center pixel for comparison.</p>
<p>Both the SAD window size and block size are important parameters that influence the trade-off between accuracy and computational efficiency in stereo matching algorithms. The optimal values for these parameters depend on factors such as image resolution, scene complexity, and noise characteristics, and they may require experimentation and tuning for specific applications.</p>
<p>To compute the disparity map for the left image using the specified matcher we use the function below:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">compute_disp_left</span><span class="p">(</span><span class="n">img_left</span><span class="p">,</span> <span class="n">img_right</span><span class="p">,</span><span class="n">sad_window</span><span class="p">,</span><span class="n">block_size</span><span class="p">,</span> <span class="n">matcher_name</span><span class="o">=</span><span class="s1">&#39;bm&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Arguments:
</span></span></span><span class="line"><span class="cl"><span class="s1">    img_left -- image from the left camera
</span></span></span><span class="line"><span class="cl"><span class="s1">    img_right -- image from the right camera
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    Optional Argument:
</span></span></span><span class="line"><span class="cl"><span class="s1">    matcher_name -- (str) the matcher name, can be &#39;bm&#39; for StereoBM or &#39;sgbm&#39; for StereoSGBM matching
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s1">    disp_left -- disparity map for the left camera image
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Set the parameters for disparity calculation</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_disparities</span> <span class="o">=</span> <span class="n">sad_window</span> <span class="o">*</span> <span class="mi">16</span>    
</span></span><span class="line"><span class="cl">    <span class="c1"># Convert the input images to grayscale</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_left_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img_left</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_right_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img_right</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Create the stereo matcher based on the selected method</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">matcher_name</span> <span class="o">==</span> <span class="s1">&#39;bm&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">matcher</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">StereoBM_create</span><span class="p">(</span><span class="n">numDisparities</span><span class="o">=</span><span class="n">num_disparities</span><span class="p">,</span> <span class="n">blockSize</span><span class="o">=</span><span class="n">block_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">matcher_name</span> <span class="o">==</span> <span class="s1">&#39;sgbm&#39;</span><span class="p">:</span>                             
</span></span><span class="line"><span class="cl">        <span class="n">matcher</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">StereoSGBM_create</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">numDisparities</span><span class="o">=</span><span class="n">num_disparities</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">minDisparity</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">blockSize</span><span class="o">=</span><span class="n">block_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">P1</span><span class="o">=</span><span class="mi">8</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">sad_window</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">P2</span><span class="o">=</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">sad_window</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">mode</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">STEREO_SGBM_MODE_SGBM_3WAY</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Compute the disparity map</span>
</span></span><span class="line"><span class="cl">    <span class="n">disparity</span> <span class="o">=</span> <span class="n">matcher</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">img_left_gray</span><span class="p">,</span> <span class="n">img_right_gray</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Normalize the disparity map for visualization</span>
</span></span><span class="line"><span class="cl">    <span class="n">disparity_normalized</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">disparity</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">255</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_MINMAX</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">CV_8U</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Compute the disparity map for the left camera image and return it</span>
</span></span><span class="line"><span class="cl">    <span class="n">disp_left</span> <span class="o">=</span> <span class="n">disparity</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="mi">16</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">disp_left</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sad_window</span> <span class="o">=</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl"><span class="n">block_size</span> <span class="o">=</span> <span class="mi">11</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">disp_left</span> <span class="o">=</span> <span class="n">compute_disp_left</span><span class="p">(</span><span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span><span class="p">,</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_right</span><span class="p">,</span> <span class="n">sad_window</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">matcher_name</span><span class="o">=</span><span class="s1">&#39;bm&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Set the figure size</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Set the plot title</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Disparity Map (Left Image) - Stereo Block Matching&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Display the disparity map</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">disp_left</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Add axis labels</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Pixel Columns&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Pixel Rows&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Show the plot</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute the disparity map for the left image</span>
</span></span><span class="line"><span class="cl"><span class="n">disp_left</span> <span class="o">=</span> <span class="n">compute_disp_left</span><span class="p">(</span><span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span><span class="p">,</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_right</span><span class="p">,</span> <span class="n">sad_window</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">matcher_name</span><span class="o">=</span><span class="s1">&#39;sgbm&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Set the figure size</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Set the plot title</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Disparity Map (Left Image) - Stereo Semi-Global Block Matching&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Display the disparity map</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">disp_left</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Add axis labels</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Pixel Columns&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Pixel Rows&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Show the plot</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><img class="myImg" src="/kitti/kitti-StereoBM.png">
<img class="myImg" src="/kitti/kitti-StereoSGBM.png">
<h3 id="stereobm-vs-stereosgbm">StereoBM Vs. StereoSGBM</h3>
<p>StereoBM and StereoSGBM are algorithms used in stereo vision for computing the disparity map from a pair of stereo images which are both provided by the OpenCV library and offer different approaches to stereo matching.</p>
<ol>
<li>
<p>StereoBM (Stereo Block Matching):</p>
<ul>
<li>StereoBM is a block matching algorithm that operates on grayscale images.</li>
<li>It works by comparing blocks of pixels between the left and right images and finding the best matching block based on a predefined matching cost.</li>
<li>The disparity is then computed by calculating the difference in the horizontal pixel coordinates of the matching blocks.</li>
<li>StereoBM is relatively fast and suitable for real-time applications but may not handle challenging scenes with textureless or occluded regions as effectively.</li>
</ul>
</li>
<li>
<p>StereoSGBM (Stereo Semi-Global Block Matching):</p>
<ul>
<li>StereoSGBM is an extension of the StereoBM algorithm that addresses some of its limitations.</li>
<li>It incorporates additional steps such as semi-global optimization to refine the disparity map and handle occlusions and textureless regions more robustly.</li>
<li>StereoSGBM also considers a wider range of disparities during the matching process, allowing for more accurate depth estimation.</li>
<li>It takes into account various cost factors, such as the uniqueness of matches and the smoothness of disparities, to improve the overall quality of the disparity map.</li>
<li>However, StereoSGBM is computationally more expensive than StereoBM due to the additional optimization steps involved.</li>
</ul>
</li>
</ol>
<p>Both StereoBM and StereoSGBM are widely used in stereo vision applications for tasks such as 3D reconstruction, depth estimation, object detection, and robot navigation. The choice between the two algorithms depends on the specific requirements of the application, including the scene complexity, computational resources available, and desired accuracy.</p>
<p>The code below is used to visualize the disparity map obtained from stereo image pairs. The disparity map contains depth information about the scene, enabling us to perceive the 3D structure of objects within the images. By overlaying the disparity map onto the main image, we can visualize the depth variations and understand the relative distances of objects in the scene. This visualization aids in applications such as depth estimation, 3D reconstruction, object detection, and scene understanding.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute the disparity map</span>
</span></span><span class="line"><span class="cl"><span class="n">disp_left</span> <span class="o">=</span> <span class="n">compute_disp_left</span><span class="p">(</span><span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span><span class="p">,</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_right</span><span class="p">,</span> <span class="n">sad_window</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">matcher_name</span><span class="o">=</span><span class="s1">&#39;bm&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the main image with the disparity map as overlay</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Disparity Map Overlay: Visualizing Depth Information in Stereo Images (BM)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">disp_left</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;jet&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Add axis labels</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Pixel Columns&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Pixel Rows&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Show the plot</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute the disparity map</span>
</span></span><span class="line"><span class="cl"><span class="n">disp_left</span> <span class="o">=</span> <span class="n">compute_disp_left</span><span class="p">(</span><span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span><span class="p">,</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_right</span><span class="p">,</span> <span class="n">sad_window</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">matcher_name</span><span class="o">=</span><span class="s1">&#39;sgbm&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the main image with the disparity map as overlay</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Disparity Map Overlay: Visualizing Depth Information in Stereo Images (SGBM)&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">disp_left</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;jet&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Add axis labels</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Pixel Columns&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Pixel Rows&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Show the plot</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><img class="myImg" src="/kitti/real_depth_img_bm.png">
<img class="myImg" src="/kitti/real_depth_img_sgbm.png">
<h2 id="feature-extraction">Feature Extraction</h2>
<p>Feature extraction plays a critical role in computer vision tasks by capturing and representing distinct patterns or characteristics of images or image regions. These features serve as meaningful representations that enable higher-level analysis and interpretation of visual data. In this section, we will explore some advanced techniques for feature extraction in computer vision.</p>
<h3 id="1-harris-corner-detection">1. Harris Corner Detection</h3>
<p>Harris Corner Detection, proposed by  [<a id="Harris" href="#HarrisRef">Harris and Stephens</a>] in 1988. It is a widely used algorithm for detecting corners in images. Corner points represent the junctions of two or more edges, which are highly informative for image matching and tracking. The Harris Corner Detection algorithm consists of the following steps:</p>
<ol>
<li>
<p><strong>Image Gradient Calculation:</strong> Compute the gradients of the image in the x and y directions using techniques such as Sobel or Prewitt operators.</p>
</li>
<li>
<p><strong>Structure Tensor Computation:</strong> Based on the gradients, construct the structure tensor for each pixel. The structure tensor is a matrix that represents the local image structure at a given point.</p>
</li>
<li>
<p><strong>Corner Response Function:</strong> Compute the corner response function using the structure tensor. The corner response function measures the likelihood of a pixel being a corner based on the eigenvalues of the structure tensor.</p>
</li>
<li>
<p><strong>Non-maximum Suppression:</strong> Apply non-maximum suppression to the corner response function to select the most prominent corners while suppressing weaker nearby corners.</p>
</li>
</ol>
<p>The Harris Corner Detection algorithm can be described using the following equations:</p>
<ul>
<li>
<p>Gradient calculation:
$$ \begin{equation} I_x = \frac{{\partial I}}{{\partial x}} \quad \text{and} \quad I_y = \frac{{\partial I}}{{\partial y}} \end{equation} $$</p>
</li>
<li>
<p>Structure tensor computation:
$$ \begin{equation} M = \begin{bmatrix} I_x^2 &amp; I_xI_y \ I_xI_y &amp; I_y^2 \end{bmatrix} \end{equation} $$</p>
</li>
<li>
<p>Corner response function:
$$ \begin{equation} R = \text{det}(M) - k \cdot \text{trace}(M)^2 \end{equation} $$</p>
</li>
<li>
<p>Non-maximum suppression:</p>
<ul>
<li>Select local maxima of R by comparing R with a threshold and considering neighboring pixels.</li>
</ul>
</li>
</ul>
<p>where, $I_x$  and $I_y$ are image derivatives in x and y directions respectively, $det(M)=λ1λ2$ $\text{trace}(M)=λ1+λ2$, $λ1$ and $λ2$ are the eigenvalues of $M$</p>
<p>Harris Corner Detection is a fundamental technique in many computer vision applications, including feature matching, image alignment, and camera calibration. The detected corners serve as robust landmarks that can be used for subsequent analysis and understanding of image content.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">img</span> <span class="o">=</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Load the input image</span>
</span></span><span class="line"><span class="cl"><span class="n">Image</span> <span class="o">=</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Harris Corner Detection</span>
</span></span><span class="line"><span class="cl"><span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">Image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">gray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">gray</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cornerHarris</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Dilate the result for marking the corners</span>
</span></span><span class="line"><span class="cl"><span class="n">dst</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dilate</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a copy of the image to display the detected corners</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Threshold for an optimal value, it may vary depending on the image.</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span><span class="p">[</span><span class="n">dst</span><span class="o">&gt;</span><span class="mf">0.01</span><span class="o">*</span><span class="n">dst</span><span class="o">.</span><span class="n">max</span><span class="p">()]</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the image with detected corners</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Harris Corner Detection&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><img class="myImg" src="/kitti/Harris.png">
<h3 id="2-shi-tomasi-corner-detector--good-features-to-track">2. Shi-Tomasi Corner Detector &amp; Good Features to Track</h3>
<p>The Shi-Tomasi Corner Detector, also known as the [<a id="Shi" href="#ShiRef">Good Features to Track</a>] algorithm, is an improvement over the Harris Corner Detection algorithm. It provides a more robust and accurate method for detecting corners in an image. This algorithm selects the most distinctive corners based on a corner response measure. The steps involved in the Shi-Tomasi Corner Detection algorithm are as follows:</p>
<ol>
<li>
<p><strong>Image Gradient Calculation:</strong> Compute the gradients of the image in the x and y directions using techniques such as Sobel or Prewitt operators.</p>
</li>
<li>
<p><strong>Structure Tensor Computation:</strong> Based on the gradients, construct the structure tensor for each pixel.</p>
</li>
<li>
<p><strong>Eigenvalue Calculation:</strong> Compute the eigenvalues of the structure tensor for each pixel. The eigenvalues represent the principal curvatures of the local image structure.</p>
</li>
<li>
<p><strong>Corner Response Calculation:</strong> Calculate the corner response measure using the eigenvalues. The corner response measure is defined as the minimum eigenvalue or a combination of the eigenvalues.</p>
</li>
<li>
<p><strong>Non-maximum Suppression:</strong> Apply non-maximum suppression to select the most significant corners while suppressing weaker nearby corners.</p>
</li>
</ol>
<p>The Shi-Tomasi Corner Detector algorithm can be described using the following equations:</p>
<ul>
<li>
<p>Gradient calculation:
$$ \begin{equation} I_x = \frac{{\partial I}}{{\partial x}} \quad \text{and} \quad I_y = \frac{{\partial I}}{{\partial y}} \end{equation} $$</p>
</li>
<li>
<p>Structure tensor computation:
$$ \begin{equation} M = \begin{bmatrix} I_x^2 &amp; I_xI_y \ I_xI_y &amp; I_y^2 \end{bmatrix} \end{equation} $$</p>
</li>
<li>
<p>Eigenvalue calculation:
$$ \begin{equation} \lambda_1, \lambda_2 = \text{eigenvalues}(M) \end{equation} $$</p>
</li>
<li>
<p>Corner response calculation:
$$ \begin{equation} R = \min(\lambda_1, \lambda_2) \end{equation} $$</p>
</li>
<li>
<p>Non-maximum suppression:</p>
<ul>
<li>Select local maxima of R by comparing R with a threshold and considering neighboring pixels.</li>
</ul>
</li>
</ul>
<p>The Shi-Tomasi Corner Detector algorithm is widely used in feature tracking, motion estimation, and image registration tasks. It provides more accurate and reliable corner detection compared to the Harris Corner Detector.</p>
<p>Here&rsquo;s an example code snippet for implementing the Shi-Tomasi Corner Detector using OpenCV:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">cv2</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load the input image</span>
</span></span><span class="line"><span class="cl"><span class="n">Image</span> <span class="o">=</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a copy of the image to display the detected corners</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set the maximum number of corners to detect</span>
</span></span><span class="line"><span class="cl"><span class="n">max_corners</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set the quality level for corner detection</span>
</span></span><span class="line"><span class="cl"><span class="n">quality_level</span> <span class="o">=</span> <span class="mf">0.01</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set the minimum Euclidean distance between corners</span>
</span></span><span class="line"><span class="cl"><span class="n">min_distance</span> <span class="o">=</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Apply Shi-Tomasi Corner Detector</span>
</span></span><span class="line"><span class="cl"><span class="n">corners</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">goodFeaturesToTrack</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="n">max_corners</span><span class="p">,</span> <span class="n">quality_level</span><span class="p">,</span> <span class="n">min_distance</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Convert corners to integer coordinates</span>
</span></span><span class="line"><span class="cl"><span class="n">corners</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int0</span><span class="p">(</span><span class="n">corners</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Draw detected corners on the image</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">corner</span> <span class="ow">in</span> <span class="n">corners</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">corner</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">cv2</span><span class="o">.</span><span class="n">circle</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the image with detected corners using matplotlib</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Shi-Tomasi Corner Detection&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p>In this code, we use the <code>cv2.goodFeaturesToTrack()</code> function in OpenCV to apply the Shi-Tomasi Corner Detector. It takes the image, maximum number of corners, quality level, and minimum distance as parameters. The function returns the detected corners as a NumPy array. We then convert the corners to integers and draw them on the image using circles. Finally, we display the image with the detected corners.</p>
<img class="myImg" src="/kitti/Shi-Tomasi.png">
<h3 id="3-scale-invariant-feature-transform-sift">3. Scale-Invariant Feature Transform (SIFT)</h3>
<p>The Scale-Invariant Feature Transform (SIFT) algorithm, introduced by [<a id="Lowe" href="#LoweRef">Lowe</a>] in 2004, which extract keypoints and compute its descriptors. SIFT provides robustness to changes in scale, rotation, and affine transformations, making it highly effective in various real-world scenarios.</p>
<p>The SIFT algorithm consists of the following main steps:</p>
<ol>
<li>
<p><strong>Scale-space Extrema Detection:</strong> SIFT applies a Difference of Gaussian (DoG) algorithm to detect potential keypoints in different scales and locations. The DoG is obtained by subtracting blurred versions of an image at multiple scales.</p>
</li>
<li>
<p><strong>Keypoint Localization:</strong> Keypoints are refined by eliminating low-contrast and poorly localized points based on the local extrema in the DoG scale space.</p>
</li>
<li>
<p><strong>Orientation Assignment:</strong> Each keypoint is assigned a dominant orientation based on local image gradients. This orientation provides invariance to image rotation.</p>
</li>
<li>
<p><strong>Descriptor Generation:</strong> SIFT computes a descriptor for each keypoint by considering the local image gradients and orientations. The descriptor captures the distinctive local image properties.</p>
</li>
<li>
<p><strong>Keypoint Matching:</strong> Once the descriptors are computed for keypoints in multiple images, a matching algorithm is used to find corresponding keypoints between the images. This step allows for tasks such as image alignment, object tracking, and image stitching.</p>
</li>
</ol>
<p>The SIFT algorithm incorporates several mathematical techniques to achieve scale invariance, robustness to noise, and distinctive feature representation. Some of the key equations and techniques used in SIFT are:</p>
<ul>
<li>
<p>Difference of Gaussian (DoG):
$$  \begin{equation} D(x, y, \sigma) = (G(x, y, k\sigma) - G(x, y, \sigma)) \ast I(x, y)  \end{equation} $$</p>
</li>
<li>
<p>Gaussian function:
$$ \begin{equation}  G(x, y, \sigma) = \frac{1}{{2\pi\sigma^2}} \exp\left(-\frac{{x^2 + y^2}}{{2\sigma^2}}\right)  \end{equation} $$</p>
</li>
<li>
<p>Keypoint orientation assignment:
$$  \begin{equation} \theta = \text{atan2}(M_y, M_x)  \end{equation} $$</p>
</li>
<li>
<p>Descriptor generation:</p>
<ul>
<li>Divide the region around the keypoint into sub-regions.</li>
<li>Compute gradient magnitude and orientation for each pixel in each sub-region.</li>
<li>Construct a histogram of gradient orientations in each sub-region.</li>
<li>Concatenate the histograms to form the final descriptor.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Load the input image</span>
</span></span><span class="line"><span class="cl"><span class="n">Image</span> <span class="o">=</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a copy of the image</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create SIFT object</span>
</span></span><span class="line"><span class="cl"><span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Detect keypoints and compute descriptors</span>
</span></span><span class="line"><span class="cl"><span class="n">keypoints</span><span class="p">,</span> <span class="n">descriptors</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Draw keypoints on the image</span>
</span></span><span class="line"><span class="cl"><span class="n">image_with_keypoints</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">keypoints</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image_with_keypoints</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Display the image with keypoints using matplotlib</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Image with SIFT Keypoints&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><img class="myImg" src="/kitti/SIFT.png">
<h3 id="4-speeded-up-robust-features-surf">4. Speeded-Up Robust Features (SURF)</h3>
<p>The [<a id="SURF" href="#SURFRef">Speeded-Up Robust Features (SURF)</a>] algorithm, introduced by Bay et al. in 2006, provides an efficient alternative to SIFT. SURF extracts robust and distinctive features while achieving faster computation. The main steps of SURF are as follows:</p>
<ol>
<li>
<p><strong>Scale-space Extrema Detection:</strong> SURF applies a Hessian matrix-based approach to detect keypoints at multiple scales. The determinant of the Hessian matrix is used to identify potential interest points.</p>
</li>
<li>
<p><strong>Orientation Assignment:</strong> Similar to SIFT, SURF assigns a dominant orientation to each keypoint using the sum of Haar wavelet responses.</p>
</li>
<li>
<p><strong>Descriptor Generation:</strong> SURF computes a descriptor based on the distribution of Haar wavelet responses in the neighborhood of each keypoint. The wavelet responses capture the local image properties.</p>
</li>
</ol>
<p>The SURF algorithm involves the following equations:</p>
<ul>
<li>
<p>Hessian matrix:
$$ \begin{equation}
H(x, y, \sigma) = \begin{bmatrix} L_{xx}(x, y, \sigma) &amp; L_{xy}(x, y, \sigma) \ L_{xy}(x, y, \sigma) &amp; L_{yy}(x, y, \sigma) \end{bmatrix} \end{equation}  $$</p>
</li>
<li>
<p>Determinant of the Hessian matrix:
$$ \begin{equation}\text{Det}(H(x, y, \sigma)) = L_{xx}(x, y, \sigma) \cdot L_{yy}(x, y, \sigma) - (L_{xy}(x, y, \sigma))^2  \end{equation} $$</p>
</li>
<li>
<p>Keypoint orientation assignment:
$$ \begin{equation}\theta = \text{atan2}\left(\sum w \cdot M_y, \sum w \cdot M_x\right) \end{equation} $$</p>
</li>
<li>
<p>Descriptor generation:</p>
<ul>
<li>Divide the region around the keypoint into sub-regions.</li>
<li>Compute Haar wavelet responses in each sub-region.</li>
<li>Construct a descriptor by combining and normalizing the wavelet responses.</li>
</ul>
</li>
</ul>
<p>These advanced feature extraction techniques, SIFT and SURF, offer powerful capabilities for identifying distinctive image features invariant to various transformations. They form the foundation for many computer vision applications, including object recognition, image stitching, and 3D reconstruction.</p>
<p>Since the SURF algorithm is not available in the current version of OpenCV, you can not use it.</p>
<h3 id="5-fast-algorithm-for-corner-detection">5. FAST Algorithm for Corner Detection</h3>
<p>The [<a id="FAST" href="#FASTRef">FAST (Features from Accelerated Segment Test)</a>] algorithm is a popular corner detection method known for its efficiency and effectiveness. It aims to identify corners in images by comparing the intensity of pixels in a circular neighborhood around each pixel. The FAST algorithm is widely used in computer vision applications such as tracking, image registration, and 3D reconstruction.</p>
<p>The key steps involved in the FAST algorithm are as follows:</p>
<ol>
<li>
<p><strong>Feature Point Selection:</strong> The algorithm selects a pixel in the image as a potential corner point.</p>
</li>
<li>
<p><strong>Feature Point Classification:</strong> A circular neighborhood of 16 pixels is examined around the selected point. Based on the intensities of these pixels, the algorithm classifies the selected point as a corner or not.</p>
</li>
<li>
<p><strong>Non-Maximum Suppression:</strong> If the selected point is classified as a corner, non-maximum suppression is applied to remove neighboring points that have similar intensities.</p>
</li>
</ol>
<p>The FAST algorithm utilizes a score-based approach for corner classification. It compares the intensity of the selected pixel with the intensities of the surrounding pixels to determine whether it is a corner or not. The algorithm employs a threshold value that determines the minimum intensity difference required for a pixel to be classified as a corner.</p>
<p>The corner detection process in the FAST algorithm involves several computations and comparisons to achieve efficiency and accuracy. The key equations and techniques used in the FAST algorithm are as follows:</p>
<ul>
<li>
<p><strong>Segment Test:</strong> The algorithm performs a binary test by comparing the intensities of a selected pixel with a threshold value. If the pixel&rsquo;s intensity is greater or smaller than the threshold value by a certain amount, it is considered as a candidate for a corner.</p>
</li>
<li>
<p><strong>Corner Classification:</strong> The algorithm compares the segment test results of neighboring pixels to classify the selected pixel as a corner. If a contiguous set of pixels in a circle satisfies the segment test criteria, the selected pixel is classified as a corner.</p>
</li>
<li>
<p><strong>Non-Maximum Suppression:</strong> After identifying potential corner points, non-maximum suppression is applied to select the most salient corners. This process eliminates neighboring corners that have similar intensities, retaining only the most distinctive corners.</p>
</li>
</ul>
<p>Here&rsquo;s an example code snippet demonstrating the FAST algorithm using Python and the matplotlib library for visualization:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Load the input image</span>
</span></span><span class="line"><span class="cl"><span class="n">Image</span> <span class="o">=</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a copy of the image</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create FAST object</span>
</span></span><span class="line"><span class="cl"><span class="n">fast</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FastFeatureDetector_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Detect keypoints</span>
</span></span><span class="line"><span class="cl"><span class="n">keypoints</span> <span class="o">=</span> <span class="n">fast</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Draw keypoints on the image</span>
</span></span><span class="line"><span class="cl"><span class="n">image_with_keypoints</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">keypoints</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the image with keypoints using matplotlib</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image_with_keypoints</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;FAST Corner Detection&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p>In this code, we first load the image and create a FAST object using <code>cv2.FastFeatureDetector_create()</code>. We then use the <code>detect()</code> function to detect keypoints in the image. Finally, we draw the keypoints on the image using <code>cv2.drawKeypoints()</code> and display the result using <code>plt.imshow()</code>.</p>
<img class="myImg" src="/kitti/FAST.png">
<h3 id="6-brief-binary-robust-independent-elementary-features">6. BRIEF (Binary Robust Independent Elementary Features)</h3>
<p>[<a id="BRIEF" href="#BRIEFRef">BRIEF (Binary Robust Independent Elementary Features)</a>] generates a binary feature descriptor that encodes the local image properties around key points. These descriptors are efficient to compute and compare, making them suitable for real-time applications.</p>
<p>The BRIEF algorithm consists of the following steps:</p>
<ol>
<li>
<p><strong>Keypoint Selection:</strong> BRIEF selects key points in the image using a corner detection algorithm such as Harris Corner Detection or FAST Algorithm.</p>
</li>
<li>
<p><strong>Descriptor Construction:</strong> For each key point, BRIEF constructs a binary feature descriptor by comparing pairs of pixel intensities in a predefined pattern around the key point.</p>
</li>
<li>
<p><strong>Descriptor Matching:</strong> The binary feature descriptors are compared using a matching algorithm such as Hamming distance or Euclidean distance to find correspondences between keypoints in different images.</p>
</li>
</ol>
<p>The BRIEF algorithm employs a pre-defined sampling pattern to select pairs of pixels around each key point. These pixel pairs are then compared, and the results are encoded into a binary string, forming the feature descriptor.</p>
<p>Here are the key equations and techniques used in the BRIEF algorithm:</p>
<ul>
<li>
<p><strong>Sampling Pattern:</strong> BRIEF uses a pre-defined sampling pattern to select pairs of pixels around each key point. The pattern specifies the pixel locations relative to the key point where the intensity comparisons will be made.</p>
</li>
<li>
<p><strong>Intensity Comparison:</strong> BRIEF compares the pixel intensities at the selected pairs of locations using a simple binary test. For example, the algorithm may compare whether the intensity of the first pixel is greater than the intensity of the second pixel.</p>
</li>
<li>
<p><strong>Encoding:</strong> The results of the intensity comparisons are encoded into a binary string to form the feature descriptor. Each bit in the string represents the outcome of a particular intensity comparison.</p>
</li>
<li>
<p><strong>Descriptor Matching:</strong> BRIEF uses a distance metric, such as Hamming distance or Euclidean distance, to compare the binary feature descriptors of keypoints in different images. The distance measure determines the similarity between descriptors and helps in matching keypoints.</p>
</li>
</ul>
<p>Here&rsquo;s an example code snippet demonstrating the BRIEF algorithm using Python and the matplotlib library for visualization:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Load the input image</span>
</span></span><span class="line"><span class="cl"><span class="n">Image</span> <span class="o">=</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a copy of the image</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initiate FAST detector</span>
</span></span><span class="line"><span class="cl"><span class="n">star</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">xfeatures2d</span><span class="o">.</span><span class="n">StarDetector_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Initiate BRIEF extractor</span>
</span></span><span class="line"><span class="cl"><span class="n">brief</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">xfeatures2d</span><span class="o">.</span><span class="n">BriefDescriptorExtractor_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># find the keypoints with STAR</span>
</span></span><span class="line"><span class="cl"><span class="n">kp</span> <span class="o">=</span> <span class="n">star</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># compute the descriptors with BRIEF</span>
</span></span><span class="line"><span class="cl"><span class="n">kp</span><span class="p">,</span> <span class="n">des</span> <span class="o">=</span> <span class="n">brief</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">kp</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Draw keypoints on the image</span>
</span></span><span class="line"><span class="cl"><span class="n">image_with_keypoints</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">kp</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the image with keypoints using matplotlib</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image_with_keypoints</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;BRIEF Feature Extraction&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p>In this code, we first load the image and create a BRIEF object using <code>cv2.xfeatures2d.BriefDescriptorExtractor_create()</code>. We then use the <code>detect()</code> function to detect keypoints in the image and compute the BRIEF descriptors using <code>brief.compute()</code>. Finally, we draw the keypoints on the image using <code>cv2.drawKeypoints()</code> and display the result using <code>plt.imshow()</code>.</p>
<img class="myImg" src="/kitti/BRIEF.png">
<h3 id="orb-oriented-fast-and-rotated-brief">ORB (Oriented FAST and Rotated BRIEF)</h3>
<p>[<a id="ORB" href="#ORBRef">ORB (Oriented FAST and Rotated BRIEF)</a>]  combines the efficiency of the FAST corner detector with the robustness of the BRIEF descriptor. It introduces modifications to both the corner detection and descriptor computation steps to improve the performance and robustness of feature extraction.</p>
<p>The ORB algorithm consists of the following steps:</p>
<ol>
<li>
<p><strong>Keypoint Detection:</strong> ORB detects keypoints in the image using the FAST corner detection algorithm. FAST efficiently identifies corner-like features based on intensity variations in a circular neighborhood.</p>
</li>
<li>
<p><strong>Orientation Assignment:</strong> ORB assigns an orientation to each keypoint to make the feature descriptor rotation-invariant. It computes a dominant orientation based on the intensity distribution in the region surrounding the keypoint.</p>
</li>
<li>
<p><strong>Descriptor Construction:</strong> ORB constructs a binary feature descriptor by comparing pairs of pixel intensities in a predefined pattern around the keypoint. The BRIEF descriptor is modified to account for the assigned orientation, making it rotation-invariant.</p>
</li>
<li>
<p><strong>Descriptor Matching:</strong> The binary feature descriptors of keypoints in different images are compared using a distance metric such as Hamming distance or Euclidean distance to establish correspondences between keypoints.</p>
</li>
</ol>
<p>The ORB algorithm incorporates several optimizations and modifications to enhance its performance, including efficient sampling, orientation estimation, and scale pyramid representation.</p>
<p>Here are the key equations and techniques used in the ORB algorithm:</p>
<ul>
<li>
<p><strong>FAST Corner Detection:</strong> The FAST algorithm compares the intensity of pixels in a circular neighborhood around each pixel to identify corners. It employs a threshold and requires a sufficient number of contiguous pixels to be brighter or darker than the central pixel for it to be considered a corner.</p>
</li>
<li>
<p><strong>BRIEF Descriptor:</strong> ORB uses a binary descriptor similar to BRIEF, which compares pairs of pixel intensities. However, ORB modifies BRIEF to account for the assigned orientation of keypoints. This modification involves rotating the sampling pattern according to the keypoint orientation.</p>
</li>
<li>
<p><strong>Orientation Assignment:</strong> ORB assigns an orientation to each keypoint by estimating the dominant direction of intensity variations around the keypoint. It calculates the intensity centroid and computes the principal orientation using the moments of the intensity distribution.</p>
</li>
<li>
<p><strong>Descriptor Matching:</strong> ORB compares the binary feature descriptors of keypoints using a distance metric such as Hamming distance or Euclidean distance. The distance measure determines the similarity between descriptors and aids in matching keypoints.</p>
</li>
</ul>
<p>Here&rsquo;s an example code snippet demonstrating the ORB algorithm using Python and the matplotlib library for visualization:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Load the input image</span>
</span></span><span class="line"><span class="cl"><span class="n">Image</span> <span class="o">=</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a copy of the image</span>
</span></span><span class="line"><span class="cl"><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create ORB object</span>
</span></span><span class="line"><span class="cl"><span class="n">orb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">ORB_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Detect keypoints</span>
</span></span><span class="line"><span class="cl"><span class="n">keypoints</span> <span class="o">=</span> <span class="n">orb</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute ORB descriptors</span>
</span></span><span class="line"><span class="cl"><span class="n">keypoints</span><span class="p">,</span> <span class="n">descriptors</span> <span class="o">=</span> <span class="n">orb</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">keypoints</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Draw keypoints on the image</span>
</span></span><span class="line"><span class="cl"><span class="n">image_with_keypoints</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">keypoints</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the image with keypoints using matplotlib</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image_with_keypoints</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ORB Feature Extraction&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p>In this code, we first load the image and create an ORB object using <code>cv2.ORB_create()</code>. We then use the <code>detect()</code> function to detect keypoints in the image and compute the ORB descriptors using <code>orb.compute()</code>. Finally, we draw the keypoints on the image</p>
<p>using <code>cv2.drawKeypoints()</code> and display the result using <code>plt.imshow()</code>.</p>
<img class="myImg" src="/kitti/ORB.png">
<h2 id="feature-matching">Feature Matching</h2>
<p>Feature matching is a fundamental task in computer vision that aims to establish correspondences between features in different images or frames. It plays a crucial role in various applications such as image alignment, object recognition, motion tracking, and 3D reconstruction. The goal is to identify corresponding features across different images or frames, which enables us to understand the spatial relationships and transformations between them.</p>
<h3 id="descriptor-distance-measures">Descriptor Distance Measures</h3>
<p>Descriptor distance measures play a vital role in feature matching by quantifying the similarity or dissimilarity between feature descriptors. These measures help determine the quality and accuracy of matches. Here, we will discuss three commonly used descriptor distance measures:</p>
<ol>
<li>
<p><strong>Euclidean Distance</strong>: The Euclidean distance is a fundamental distance measure used in many feature matching algorithms. It calculates the straight-line distance between two feature descriptors in the descriptor space. Mathematically, the Euclidean distance between two descriptors $x$ and $y$ with $N$ dimensions can be expressed as:</p>
<p>$$\begin{equation}d(x, y) = \sqrt{\sum_{i=1}^{N}(x_i - y_i)^2} \end{equation}$$</p>
<p>The Euclidean distance is suitable for continuous-valued feature descriptors.</p>
</li>
<li>
<p><strong>Hamming Distance</strong>: The Hamming distance is specifically used for binary feature descriptors, where each bit represents a certain feature characteristic. It measures the number of differing bits between two binary strings. For example, it is commonly used with binary descriptors like BRIEF or ORB. Mathematically, the Hamming distance between two binary descriptors $x$ and $y$ with $N$ bits can be calculated as:</p>
<p>$$\begin{equation}d(x, y) = \sum_{i=1}^{N}(x_i \oplus y_i) \end{equation}$$</p>
<p>Here, $x_i$ and $y_i$ represent the bits of the binary strings $x$ and $y$, respectively, and $\oplus$ denotes the XOR operation.</p>
</li>
<li>
<p><strong>Cosine Similarity</strong>: Cosine similarity is often employed in high-dimensional feature spaces. It measures the cosine of the angle between two feature vectors in the descriptor space, providing a similarity measure rather than a distance measure. Mathematically, the cosine similarity between two feature vectors $x$ and $y$ can be computed as:</p>
<p>$$\begin{equation}\text{similarity}(x, y) = \frac{x \cdot y}{|x| \cdot |y|} \end{equation}$$</p>
<p>Here, $x \cdot y$ represents the dot product of $x$ and $y$, and $$|x|$ and $|y|$ denote the Euclidean norms of $x$ and $y$, respectively.</p>
</li>
</ol>
<h3 id="matching-techniques">Matching Techniques</h3>
<p>Matching techniques aim to establish correspondences between feature descriptors, enabling the identification of similar features in different images or frames. Below, we discuss three common matching techniques:</p>
<ol>
<li>
<p><strong>Brute-Force Matching</strong>: Brute-force matching is a straightforward approach where each feature in one image is compared with all the features in the other image using a chosen distance measure. The best match is determined based on the minimum distance or maximum similarity score. Brute-force matching exhaustively searches for the most similar features, ensuring the best match. However, it can be computationally expensive for large-scale matching tasks.</p>
</li>
<li>
<p><strong>FLANN (Fast Library for Approximate Nearest Neighbors) Matching</strong>: FLANN is an efficient algorithm for approximate nearest neighbor search. It constructs an index structure, such as a kd-tree or an approximate k-means tree, to enable fast retrieval of approximate nearest neighbors. FLANN is particularly useful in high-dimensional feature spaces where the computational cost of exact matching methods, such as brute-force matching, becomes prohibitive. By providing approximate matches, FLANN significantly reduces the time required for matching.</p>
</li>
<li>
<p><strong>Ratio Test</strong>: The ratio test is a post-processing step commonly applied to feature matching to filter out ambiguous matches and enhance matching accuracy. It involves comparing the distance to the best match with the distance to the second-best match. If the ratio between these distances is below a specified threshold, the match is considered valid. By setting a threshold,</p>
</li>
</ol>
<p>the ratio test ensures that matches are distinctive and significantly better than alternative matches. This test helps reduce false positives and improves the robustness of feature matching.</p>
<h3 id="code-example-feature-matching-with-brute-force-matching-and-euclidean-distance">Code Example: Feature Matching with Brute-Force Matching and Euclidean Distance</h3>
<p>Here is an example code snippet that demonstrates feature matching using the brute-force matching technique with Euclidean distance as the descriptor distance measure:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">image1</span> <span class="o">=</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span>
</span></span><span class="line"><span class="cl"><span class="n">image2</span> <span class="o">=</span> <span class="n">handler</span><span class="o">.</span><span class="n">first_image_right</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create feature detector and descriptor extractor</span>
</span></span><span class="line"><span class="cl"><span class="n">detector</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">ORB_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">descriptor</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">ORB_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Detect and compute keypoints and descriptors for both images</span>
</span></span><span class="line"><span class="cl"><span class="n">keypoints1</span><span class="p">,</span> <span class="n">descriptors1</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">image1</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">keypoints2</span><span class="p">,</span> <span class="n">descriptors2</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">image2</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a brute-force matcher with Euclidean distance</span>
</span></span><span class="line"><span class="cl"><span class="n">matcher</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">BFMatcher</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_L2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Perform matching</span>
</span></span><span class="line"><span class="cl"><span class="n">matches</span> <span class="o">=</span> <span class="n">matcher</span><span class="o">.</span><span class="k">match</span><span class="p">(</span><span class="n">descriptors1</span><span class="p">,</span> <span class="n">descriptors2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sort matches by distance</span>
</span></span><span class="line"><span class="cl"><span class="n">matches</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">matches</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">distance</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Draw top N matches</span>
</span></span><span class="line"><span class="cl"><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl"><span class="n">matched_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawMatches</span><span class="p">(</span><span class="n">image1</span><span class="p">,</span> <span class="n">keypoints1</span><span class="p">,</span> <span class="n">image2</span><span class="p">,</span> <span class="n">keypoints2</span><span class="p">,</span> <span class="n">matches</span><span class="p">[:</span><span class="n">N</span><span class="p">],</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Convert the image to RGB format for display</span>
</span></span><span class="line"><span class="cl"><span class="n">matched_image_rgb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">matched_image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display the matched image using matplotlib</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">matched_image_rgb</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p>In this code example, we use the ORB feature detector and descriptor extractor to detect keypoints and compute descriptors for both images. Then, a brute-force matcher with Euclidean distance is created to perform matching on the descriptors. The matches are sorted based on distance, and the top N matches are visualized using <code>cv2.drawMatches()</code>. Finally, the matched image is displayed using <code>plt.imshow()</code>.</p>
<p>Note: Make sure to replace <code>'image1.jpg'</code> and <code>'image2.jpg'</code> with the paths to your own image files.</p>
<p>This feature matching code demonstrates one possible implementation using the brute-force matching technique with Euclidean distance. It can be customized by selecting different distance measures, employing alternative matching techniques like FLANN, or adjusting parameters such as the ratio threshold in the ratio test to suit specific requirements and achieve optimal matching results.</p>
<img class="myImg" src="/kitti/Feature-Matching.png">
<p>[<a  target="_blank" href="/kitti/Feature-Matching.png"> Click hear to see full screed </a>]</p>
<h3 id="point-cloud-to-image-projection">Point Cloud to Image Projection</h3>
<p>Point cloud to image projection is a crucial step in 3D reconstruction and visualization tasks. It involves mapping the points in a 3D point cloud onto a 2D image plane to generate a corresponding image representation. This projection allows us to visualize the 3D scene from different viewpoints or perspectives. Here, we discuss the process of projecting a point cloud onto an image plane.</p>
<p>Given a point cloud composed of 3D points $P_i = (X_i, Y_i, Z_i)$ and an intrinsic camera matrix $\mathbf{K}$, the projection of a 3D point onto a 2D image plane can be computed using the following steps:</p>
<ol>
<li>
<p><strong>Homogeneous Coordinate Transformation</strong>: The 3D point $P_i$ is represented in homogeneous coordinates as $\mathbf{P}_i = (X_i, Y_i, Z_i, 1)$. Homogeneous coordinates allow us to perform transformations using matrix operations.</p>
</li>
<li>
<p><strong>World to Camera Transformation</strong>: The point $\mathbf{P}_i$ is transformed from the world coordinate system to the camera coordinate system using the camera extrinsic matrix $\mathbf{E}$. The transformation can be expressed as $\mathbf{P}_i&rsquo; = \mathbf{E} \cdot \mathbf{P}_i$.</p>
</li>
<li>
<p><strong>Perspective Projection</strong>: The transformed point $\mathbf{P}_i&rsquo;$ in the camera coordinate system is projected onto the image plane using perspective projection. This projection accounts for the camera intrinsic parameters, such as focal length and principal point, defined by the intrinsic camera matrix $\mathbf{K}$. The projection can be computed as $\mathbf{p}_i = \mathbf{K} \cdot \mathbf{P}_i&rsquo;$.</p>
</li>
<li>
<p><strong>Normalization</strong>: The projected point $\mathbf{p}_i$ is normalized by dividing its $x$ and $y$ coordinates by its $z$ coordinate to obtain the 2D image coordinates. The normalized coordinates are given by $x = \frac{p_x}{p_z}$ and $y = \frac{p_y}{p_z}$.</p>
</li>
<li>
<p><strong>Clipping</strong>: The projected point is checked against the image boundaries to ensure that it lies within the valid image region. Points outside the image boundaries are typically discarded or handled using appropriate techniques.</p>
</li>
</ol>
<p>By repeating these steps for each point in the point cloud, we can project the entire point cloud onto the image plane, generating a corresponding image representation.</p>
<p>Code Example: Point Cloud to Image Projection</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">project_point_cloud_to_image</span><span class="p">(</span><span class="n">point_cloud</span><span class="p">,</span> <span class="n">image_height</span><span class="p">,</span> <span class="n">image_width</span><span class="p">,</span> <span class="n">transformation_matrix</span><span class="p">,</span> <span class="n">projection_matrix</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Projects a point cloud onto an image plane by transforming the X, Y, Z coordinates of the points
</span></span></span><span class="line"><span class="cl"><span class="s2">    to the camera frame using the transformation matrix, and then projecting them using the camera&#39;s
</span></span></span><span class="line"><span class="cl"><span class="s2">    projection matrix.
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    point_cloud -- array of shape Nx4 containing (X, Y, Z, reflectivity) coordinates of the point cloud
</span></span></span><span class="line"><span class="cl"><span class="s2">    image_height -- height (in pixels) of the image plane
</span></span></span><span class="line"><span class="cl"><span class="s2">    image_width -- width (in pixels) of the image plane
</span></span></span><span class="line"><span class="cl"><span class="s2">    transformation_matrix -- 3x4 transformation matrix between the lidar (X, Y, Z, 1) homogeneous and camera (X, Y, Z)
</span></span></span><span class="line"><span class="cl"><span class="s2">                             coordinate frames
</span></span></span><span class="line"><span class="cl"><span class="s2">    projection_matrix -- projection matrix of the camera (should have identity transformation if the
</span></span></span><span class="line"><span class="cl"><span class="s2">                         transformation matrix is used)
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">    rendered_image -- a (image_height x image_width) array containing depth (Z) information from the lidar scan
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Ignore points with X values less than or equal to zero (behind the lidar)</span>
</span></span><span class="line"><span class="cl">    <span class="n">point_cloud</span> <span class="o">=</span> <span class="n">point_cloud</span><span class="p">[</span><span class="n">point_cloud</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Drop the last column (reflectivity) and replace it with ones to make coordinates homogeneous</span>
</span></span><span class="line"><span class="cl">    <span class="n">point_cloud_homogeneous</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">point_cloud</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">point_cloud</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Transform the point cloud into the camera coordinate frame</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_cloud</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">transformation_matrix</span><span class="p">,</span> <span class="n">point_cloud_homogeneous</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Ignore points behind the camera</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_cloud</span> <span class="o">=</span> <span class="n">transformed_cloud</span><span class="p">[</span><span class="n">transformed_cloud</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Extract the Z coordinates (depth) from the camera</span>
</span></span><span class="line"><span class="cl">    <span class="n">depth</span> <span class="o">=</span> <span class="n">transformed_cloud</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Project the coordinates in the camera frame onto a flat plane at Z=1 by dividing by Z</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_cloud</span> <span class="o">/=</span> <span class="n">transformed_cloud</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Add a row of ones to make the 3D coordinates on the plane homogeneous for dotting with the projection matrix</span>
</span></span><span class="line"><span class="cl">    <span class="n">transformed_cloud_homogeneous</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">transformed_cloud</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">transformed_cloud</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Get the pixel coordinates (X, Y) in the camera coordinate frame</span>
</span></span><span class="line"><span class="cl">    <span class="n">projected_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">projection_matrix</span><span class="p">,</span> <span class="n">transformed_cloud_homogeneous</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">projected_points</span> <span class="o">=</span> <span class="n">projected_points</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Round the pixel coordinates to integers for indexing</span>
</span></span><span class="line"><span class="cl">    <span class="n">pixel_coordinates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">projected_points</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Limit the pixel coordinates to those within the image plane boundaries</span>
</span></span><span class="line"><span class="cl">    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">pixel_coordinates</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">image_width</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                       <span class="o">&amp;</span> <span class="p">(</span><span class="n">pixel_coordinates</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                       <span class="o">&amp;</span> <span class="p">(</span><span class="n">pixel_coordinates</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">image_height</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                       <span class="o">&amp;</span> <span class="p">(</span><span class="n">pixel_coordinates</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">pixel_coordinates</span> <span class="o">=</span> <span class="n">pixel_coordinates</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Create an empty image, then fill it with the depths of each point</span>
</span></span><span class="line"><span class="cl">    <span class="n">rendered_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">image_height</span><span class="p">,</span> <span class="n">image_width</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pixel_coordinates</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">u</span> <span class="o">&lt;</span> <span class="n">image_width</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">image_height</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">rendered_image</span><span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="n">depth</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Fill zero values with a large distance so they will be ignored</span>
</span></span><span class="line"><span class="cl">    <span class="n">rendered_image</span><span class="p">[</span><span class="n">rendered_image</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3861.45</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">rendered_image</span>
</span></span></code></pre></div><p>In this code example, the function <code>project_point_cloud_to_image</code> takes a point cloud, camera extrinsic matrix, camera intrinsic matrix, image width, and image height as input parameters. It performs the steps described above to project the point cloud onto the image plane. The projected points and valid indices are returned for further processing or visualization.</p>
<p>Note: The actual implementation may vary depending on the library or framework used for point cloud and image processing. The provided code serves as a general guide to the projection process and may require modification to suit specific requirements and conventions.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">depth_lidar</span> <span class="o">=</span> <span class="n">pointcloud2image</span><span class="p">(</span><span class="n">handler</span><span class="o">.</span><span class="n">first_pointcloud</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="n">handler</span><span class="o">.</span><span class="n">imheight</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                            <span class="n">handler</span><span class="o">.</span><span class="n">imwidth</span><span class="p">,</span><span class="n">handler</span><span class="o">.</span><span class="n">Tr</span><span class="p">,</span> <span class="n">handler</span><span class="o">.</span><span class="n">P0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">depth_stereo</span> <span class="o">=</span> <span class="n">compute_disp_left</span><span class="p">(</span><span class="n">handler</span><span class="o">.</span><span class="n">first_image_left</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                            <span class="n">handler</span><span class="o">.</span><span class="n">first_image_right</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="mi">5</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span> <span class="n">matcher_name</span><span class="o">=</span><span class="s1">&#39;bm&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">depth_lidar</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Lidar Depth Map&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax1</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ax2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">depth_stereo</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Stereo Depth Map&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax2</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><img class="myImg" src="/kitti/Depth_map.png">
<h3 id="visualizing-lidar-pointcloud">Visualizing LiDAR Pointcloud</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">visualize_pointcloud</span><span class="p">(</span><span class="n">pointcloud</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Visualizes a lidar point cloud using Plotly
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    Arguments:
</span></span></span><span class="line"><span class="cl"><span class="s2">    pointcloud  --  array of shape Nx4 containing (X, Y, Z, reflectivity) 
</span></span></span><span class="line"><span class="cl"><span class="s2">                    coordinates of the point cloud
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Extract coordinates from the point cloud</span>
</span></span><span class="line"><span class="cl">    <span class="n">xs</span> <span class="o">=</span> <span class="n">pointcloud</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">ys</span> <span class="o">=</span> <span class="n">pointcloud</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">zs</span> <span class="o">=</span> <span class="n">pointcloud</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Create a 3D scatter plot</span>
</span></span><span class="line"><span class="cl">    <span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xs</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">ys</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">zs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                          <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">zs</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                          <span class="n">colorscale</span><span class="o">=</span><span class="s1">&#39;Viridis&#39;</span><span class="p">))])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Set the layout options</span>
</span></span><span class="line"><span class="cl">    <span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">scene</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">aspectmode</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">xaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">showgrid</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                            <span class="n">showticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">yaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">showgrid</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                            <span class="n">showticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">zaxis</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">showgrid</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                            <span class="n">showticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">                                 <span class="n">showlegend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Show the plot</span>
</span></span><span class="line"><span class="cl">    <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><iframe src="/kitti/pointcloud.html" width="800" height="800" frameborder="0"></iframe>
<p>Let&rsquo;s compare the diffrence between Stereo Depth and LiDAR Depth.
A comparison of depth estimation methods, specifically stereo estimation and lidar technology, offers valuable insights into the accuracy and reliability of these techniques for capturing depth information. Stereo estimation involves leveraging multiple images taken from slightly different perspectives to triangulate depth, while lidar utilizes laser beams to measure distances between the sensor and objects in its field of view. Understanding the nuances and discrepancies between these depth estimation approaches is of utmost importance in a wide range of applications, including autonomous driving, 3D mapping, and robotics.</p>
<p>By conducting a comparative analysis of the depth outputs obtained from stereo estimation and lidar, we can evaluate their consistency and identify potential systematic errors. The provided code allows us to filter lidar depths based on a threshold and extract corresponding depths from stereo estimation. Subsequently, the difference in depth between the two methods can be calculated and visualized through a scatter plot.</p>
<p>Analyzing the depth difference provides a means to assess the relative accuracy of stereo estimation and lidar. A depth difference close to zero signifies a strong agreement between the two methods, while significant deviations shed light on limitations or biases inherent to each technique. This insight is vital for comprehending the strengths and weaknesses associated with each approach and facilitates informed decision-making when choosing the most suitable method for specific applications.</p>
<p>By visualizing the depth difference, valuable insights can be gained regarding the particular areas or scenarios where one method excels over the other. Such analysis plays a pivotal role in driving advancements in depth sensing technologies and aids in the development of more accurate and robust systems for depth perception.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Filter depths from lidar below a threshold</span>
</span></span><span class="line"><span class="cl"><span class="n">lidar_threshold</span> <span class="o">=</span> <span class="mi">3000</span>
</span></span><span class="line"><span class="cl"><span class="n">valid_depth_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">depth_lidar</span> <span class="o">&lt;</span> <span class="n">lidar_threshold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">depth_indx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">valid_depth_indices</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Extract depths for comparison</span>
</span></span><span class="line"><span class="cl"><span class="n">stereo_depths</span> <span class="o">=</span> <span class="n">depth_stereo</span><span class="p">[</span><span class="n">depth_indx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">depth_indx</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">lidar_depths</span> <span class="o">=</span> <span class="n">depth_lidar</span><span class="p">[</span><span class="n">depth_indx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">depth_indx</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate the difference between stereo depth and lidar depth</span>
</span></span><span class="line"><span class="cl"><span class="n">depth_difference</span> <span class="o">=</span> <span class="n">stereo_depths</span> <span class="o">-</span> <span class="n">lidar_depths</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create scatter plot for depth difference</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">stereo_depths</span><span class="p">,</span> <span class="n">depth_difference</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Depth Difference&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Stereo Depth&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Depth Difference&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Difference between Stereo Depth and Lidar Depth&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><img class="myImg" src="/kitti/stereoVSlidar.png">
<h3 id="estimate-camera-motion">Estimate Camera Motion</h3>
<p>Estimating camera motion is a fundamental task in computer vision and robotics, where the goal is to determine the transformation between consecutive camera frames. This transformation represents the camera&rsquo;s motion in terms of rotation and translation, allowing us to understand how the camera has moved between frames. Several techniques exist to estimate camera motion, including feature-based methods and direct methods. Here, we will discuss a common feature-based method called <strong>feature point correspondence and motion estimation</strong>.</p>
<h4 id="feature-point-correspondence">Feature Point Correspondence</h4>
<p>Feature point correspondence is the process of identifying corresponding points between consecutive camera frames. These corresponding points serve as the basis for estimating camera motion. Commonly used feature detection algorithms such as Harris Corner Detection, Shi-Tomasi Corner Detector, or FAST Algorithm can be employed to extract feature points from each frame.</p>
<p>Once the feature points are detected, a technique called feature point matching is applied to find corresponding points between frames. Various matching algorithms exist, such as nearest neighbor matching and RANSAC (Random Sample Consensus). These algorithms compare the feature descriptors of each detected point to find the best matches.</p>
<h4 id="camera-motion-estimation">Camera Motion Estimation</h4>
<p>Once the corresponding feature points are identified, camera motion can be estimated using methods such as <strong>epipolar geometry</strong> or <strong>homography estimation</strong>.</p>
<ol>
<li>
<p><strong>Epipolar Geometry</strong>: Epipolar geometry is based on the geometric relationship between two camera views. Given corresponding feature points in two frames, epipolar geometry utilizes the epipolar constraint to estimate camera motion. The epipolar constraint states that the projection of a 3D point onto two camera views lies on a line called the epipolar line. By estimating the essential matrix or fundamental matrix, which encapsulates the epipolar geometry, the camera motion can be computed.</p>
</li>
<li>
<p><strong>Homography Estimation</strong>: Homography estimation assumes that the camera undergoes a pure planar motion. In this case, the corresponding feature points lie on a planar surface. By estimating the homography matrix that relates the points on the plane between the frames, the camera motion can be obtained.</p>
</li>
</ol>
<h4 id="camera-motion-representation">Camera Motion Representation</h4>
<p>The estimated camera motion is typically represented using a transformation matrix that combines rotation and translation. There are different ways to represent camera motion, including:</p>
<ul>
<li><strong>Rotation Matrix (R)</strong>: A 3x3 matrix that represents the rotation component of the camera motion. It describes how the camera has rotated between frames.</li>
<li><strong>Translation Vector (t)</strong>: A 3D vector that represents the translation component of the camera motion. It indicates the camera&rsquo;s displacement in 3D space.</li>
<li><strong>Essential Matrix (E)</strong> or <strong>Fundamental Matrix (F)</strong>: Matrices that encapsulate the geometric relationship between two camera views and encode the camera motion. They are used in epipolar geometry-based motion estimation methods.</li>
</ul>
<p>The camera motion can be further decomposed into its individual components, such as rotation angles or Euler angles, to provide a more detailed representation of the camera&rsquo;s motion.</p>
<p>Code Example: Camera Motion Estimation</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">cv2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">estimate_camera_motion</span><span class="p">(</span><span class="n">frame1</span><span class="p">,</span> <span class="n">frame2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Detect feature points in frame1 and frame2</span>
</span></span><span class="line"><span class="cl">    <span class="n">feature_detector</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FastFeatureDetector_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">keypoints1</span> <span class="o">=</span> <span class="n">feature_detector</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">frame1</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">keypoints2</span> <span class="o">=</span> <span class="n">feature_detector</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">frame2</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Extract feature descriptors</span>
</span></span><span class="line"><span class="cl">    <span class="n">descriptor_extractor</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">BRISK_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">descriptors1</span> <span class="o">=</span> <span class="n">descriptor_extractor</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">frame1</span><span class="p">,</span> <span class="n">keypoints1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">descriptors2</span> <span class="o">=</span> <span class="n">descriptor_extractor</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">frame2</span><span class="p">,</span> <span class="n">keypoints2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Match feature descriptors</span>
</span></span><span class="line"><span class="cl">    <span class="n">matcher</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">BFMatcher</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_HAMMING</span><span class="p">,</span> <span class="n">crossCheck</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">matches</span> <span class="o">=</span> <span class="n">matcher</span><span class="o">.</span><span class="k">match</span><span class="p">(</span><span class="n">descriptors1</span><span class="p">,</span> <span class="n">descriptors2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Estimate camera motion</span>
</span></span><span class="line"><span class="cl">    <span class="n">essential_matrix</span><span class="p">,</span> <span class="n">_</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findEssentialMat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">kp</span><span class="o">.</span><span class="n">pt</span> <span class="k">for</span> <span class="n">kp</span> <span class="ow">in</span> <span class="n">keypoints1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">kp</span><span class="o">.</span><span class="n">pt</span> <span class="k">for</span> <span class="n">kp</span> <span class="ow">in</span> <span class="n">keypoints2</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">focal</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pp</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">method</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">RANSAC</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">prob</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">threshold</span><span class="o">=</span><span class="mf">1.0</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">rotation_matrix</span><span class="p">,</span> <span class="n">translation_vector</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">recoverPose</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">essential_matrix</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">kp</span><span class="o">.</span><span class="n">pt</span> <span class="k">for</span> <span class="n">kp</span> <span class="ow">in</span> <span class="n">keypoints1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">kp</span><span class="o">.</span><span class="n">pt</span> <span class="k">for</span> <span class="n">kp</span> <span class="ow">in</span> <span class="n">keypoints2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">rotation_matrix</span><span class="p">,</span> <span class="n">translation_vector</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Usage</span>
</span></span><span class="line"><span class="cl"><span class="n">frame1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;frame1.jpg&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">frame2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;frame2.jpg&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rotation_matrix</span><span class="p">,</span> <span class="n">translation_vector</span> <span class="o">=</span> <span class="n">estimate_camera_motion</span><span class="p">(</span><span class="n">frame1</span><span class="p">,</span> <span class="n">frame2</span><span class="p">)</span>
</span></span></code></pre></div><p>In this code example, we use the OpenCV library to estimate camera motion between two frames. We detect feature points using the FAST algorithm and compute feature descriptors using the BRISK algorithm. Feature point matching is performed using the brute-force matching technique. The camera motion is estimated using the findEssentialMat and recoverPose functions, which utilize the RANSAC algorithm for robustness.</p>
<p>Note: The actual implementation may vary depending on the library or framework used for feature detection, descriptor extraction, and motion estimation. The provided code serves as a general guide to the process and may require modification to suit specific requirements and conventions.</p>
<h3 id="visual-odometry">Visual Odometry</h3>
<p>Visual Odometry is a technique used in computer vision and robotics to estimate the motion of a camera by analyzing consecutive images. It plays a crucial role in applications such as autonomous navigation, simultaneous localization and mapping (SLAM), and augmented reality. Visual odometry algorithms use the information extracted from the camera images to estimate the camera&rsquo;s position and orientation changes, providing valuable information for navigation and mapping tasks.</p>
<h4 id="pipeline-overview">Pipeline Overview</h4>
<p>The visual odometry pipeline typically consists of the following steps:</p>
<ol>
<li>
<p><strong>Feature Detection</strong>: Detect distinctive features in the image frames, such as corners or keypoints. These features serve as landmarks for tracking camera motion.</p>
</li>
<li>
<p><strong>Feature Tracking</strong>: Establish correspondences between features in consecutive frames. This step ensures consistent tracking of features across frames.</p>
</li>
<li>
<p><strong>Motion Estimation</strong>: Estimate the camera&rsquo;s motion between frames using the tracked features. This involves computing the relative transformation (rotation and translation) between frames.</p>
</li>
<li>
<p><strong>Scale Estimation</strong>: Determine the scale factor to recover the absolute scale of the camera motion. This is essential for accurate mapping and navigation.</p>
</li>
<li>
<p><strong>Trajectory Estimation</strong>: Accumulate the estimated camera poses over time to compute the camera&rsquo;s trajectory.</p>
</li>
</ol>
<h4 id="feature-based-visual-odometry">Feature-Based Visual Odometry</h4>
<p>Feature-based visual odometry methods focus on extracting and tracking distinctive features in the images. These features can be detected using algorithms such as Harris Corner Detection, Shi-Tomasi Corner Detector, or modern deep learning-based detectors like SIFT or SURF.</p>
<p>Once the features are detected, they are tracked across consecutive frames. This can be achieved using techniques like optical flow, where the displacement of each feature is estimated. Alternatively, feature descriptors can be computed for each feature and matched between frames to establish correspondences.</p>
<p>After establishing feature correspondences, the camera&rsquo;s motion can be estimated by solving the perspective-n-point (PnP) problem. The PnP problem aims to find the camera&rsquo;s pose (rotation and translation) given a set of 3D points and their corresponding 2D image projections. RANSAC (Random Sample Consensus) is often used to robustly estimate the camera motion by filtering out outliers.</p>
<p>Once the camera motion is estimated, the scale factor can be obtained by comparing the motion of the camera to other sources of information, such as a known baseline distance or sensor data (e.g., IMU). This step is crucial for accurately reconstructing the scene and estimating the camera trajectory.</p>
<h4 id="visual-odometry-formulation">Visual Odometry Formulation</h4>
<p>The visual odometry problem can be formulated as follows:</p>
<ul>
<li>
<p>Given two consecutive images, denoted as (I_t) and (I_{t+1}), we aim to estimate the relative camera motion between them.</p>
</li>
<li>
<p>Let (P_t = {p_1, p_2, \ldots, p_n}) be the set of 3D feature points in the world coordinate system, and (p_t^i) and (p_{t+1}^i) be their corresponding 2D projections in images (I_t) and (I_{t+1}), respectively.</p>
</li>
<li>
<p>The goal is to estimate the camera&rsquo;s pose transformation (T) from the coordinate system of (I_t) to the coordinate system of (I_{t+1}), represented as (T = (R, t)), where (R) is the rotation matrix and (t) is the translation vector.</p>
</li>
<li>
<p>The estimated camera motion can be represented as (T_{t \rightarrow t+1}), indicating the camera&rsquo;s motion from time (t) to (t+1).</p>
</li>
<li>
<p>By accumulating the estimated camera motions, we can compute the camera&rsquo;s trajectory over time.</p>
</li>
</ul>
<p>Visual odometry algorithms often make assumptions such as camera motion smoothness, feature trackability, and limited scene depth variation to enhance the</p>
<p>estimation accuracy and robustness. Advanced techniques, such as bundle adjustment, loop closure detection, and map refinement, can also be incorporated to improve the visual odometry performance and handle challenging scenarios.</p>
<h4 id="code-example">Code Example</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">cv2</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">estimate_camera_motion</span><span class="p">(</span><span class="n">frame1</span><span class="p">,</span> <span class="n">frame2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Feature detection</span>
</span></span><span class="line"><span class="cl">    <span class="n">detector</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT_create</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">keypoints1</span><span class="p">,</span> <span class="n">descriptors1</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">frame1</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">keypoints2</span><span class="p">,</span> <span class="n">descriptors2</span> <span class="o">=</span> <span class="n">detector</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">frame2</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Feature matching</span>
</span></span><span class="line"><span class="cl">    <span class="n">matcher</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">BFMatcher</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">NORM_L2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">matches</span> <span class="o">=</span> <span class="n">matcher</span><span class="o">.</span><span class="k">match</span><span class="p">(</span><span class="n">descriptors1</span><span class="p">,</span> <span class="n">descriptors2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Extract matching keypoints</span>
</span></span><span class="line"><span class="cl">    <span class="n">matched_kp1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">keypoints1</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">queryIdx</span><span class="p">]</span><span class="o">.</span><span class="n">pt</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">matched_kp2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">keypoints2</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">trainIdx</span><span class="p">]</span><span class="o">.</span><span class="n">pt</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Estimate camera motion</span>
</span></span><span class="line"><span class="cl">    <span class="n">essential_matrix</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">findEssentialMat</span><span class="p">(</span><span class="n">matched_kp1</span><span class="p">,</span> <span class="n">matched_kp2</span><span class="p">,</span> <span class="n">focal</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">pp</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">rotation_matrix</span><span class="p">,</span> <span class="n">translation_vector</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">recoverPose</span><span class="p">(</span><span class="n">essential_matrix</span><span class="p">,</span> <span class="n">matched_kp1</span><span class="p">,</span> <span class="n">matched_kp2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">rotation_matrix</span><span class="p">,</span> <span class="n">translation_vector</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Usage</span>
</span></span><span class="line"><span class="cl"><span class="n">frame1</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;frame1.jpg&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">frame2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;frame2.jpg&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rotation_matrix</span><span class="p">,</span> <span class="n">translation_vector</span> <span class="o">=</span> <span class="n">estimate_camera_motion</span><span class="p">(</span><span class="n">frame1</span><span class="p">,</span> <span class="n">frame2</span><span class="p">)</span>
</span></span></code></pre></div><p>In this code example, we use the OpenCV library to estimate the camera motion between two consecutive frames. We detect keypoints using the SIFT algorithm and compute their descriptors. Feature matching is performed using the brute-force matcher. The camera motion is estimated using the findEssentialMat and recoverPose functions, which utilize the RANSAC algorithm for robustness.</p>
<p>Note: The actual implementation may vary depending on the library or framework used for feature detection, descriptor extraction, and motion estimation. The provided code serves as a general guide to the process and may require modification to suit specific requirements and conventions.</p>
<p><a href="https://github.com/Armanasq/kitti-dataset-tutorial" target="_blank" rel="noopener">You can find the Jupet-Notebook .ipynb file at github repository</a></p>
<h2 id="references">References</h2>
<ul>
  <li>Geiger, Andreas, Philip Lenz, and Raquel Urtasun. “Are we ready for autonomous driving? The KITTI vision benchmark suite.” Conference on Computer Vision and Pattern Recognition (CVPR), 2012.</li>
  <li>KITTI Vision Benchmark Suite. http://www.cvlibs.net/datasets/kitti/</li>
  <li><a id="HarrisRef" href="#Harris">Harris, Chris, and Mike Stephens. "A combined corner and edge detector." Alvey vision conference. Vol. 15. No. 50. 1988.</a></li>
  <li><a id="ShiRef" href="#Shi">Shi, Jianbo. "Good features to track." 1994 Proceedings of IEEE conference on computer vision and pattern recognition. IEEE, 1994.</a></li>
  <li><a id="LoweRef" href="#Lowe">Lowe, David G. "Distinctive image features from scale-invariant keypoints." International journal of computer vision 60 (2004): 91-110.</a></li>
  <li><a id="SURFRef" href="#SURF">Bay, Herbert, Tinne Tuytelaars, and Luc Van Gool. "Surf: Speeded up robust features." Lecture notes in computer science 3951 (2006): 404-417.</a></li>
  <li><a id="FASTRef" href="#FAST">Rosten, Edward, and Tom Drummond. "Machine learning for high-speed corner detection." Computer Vision–ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part I 9. Springer Berlin Heidelberg, 2006.</a></li>
  <li><a id="BRIEFRef" href="#BRIEF">Calonder, Michael, et al. "Brief: Binary robust independent elementary features." Computer Vision–ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11. Springer Berlin Heidelberg, 2010.</a></li>
  <li><a id="ORBRef" href="#ORB">Rublee, Ethan, et al. "ORB: An efficient alternative to SIFT or SURF." 2011 International conference on computer vision. Ieee, 2011.</a></li>
</ul>
    </div>

    







<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Farmanasq.github.io%2Fdatsets%2Fkitti%2F&amp;text=KITTI&#43;Dataset" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Farmanasq.github.io%2Fdatsets%2Fkitti%2F&amp;t=KITTI&#43;Dataset" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=KITTI%20Dataset&amp;body=https%3A%2F%2Farmanasq.github.io%2Fdatsets%2Fkitti%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Farmanasq.github.io%2Fdatsets%2Fkitti%2F&amp;title=KITTI&#43;Dataset" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=KITTI&#43;Dataset%20https%3A%2F%2Farmanasq.github.io%2Fdatsets%2Fkitti%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Farmanasq.github.io%2Fdatsets%2Fkitti%2F&amp;title=KITTI&#43;Dataset" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://armanasq.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu423262b037e945bf3d00a3d75617f940_247637_270x270_fill_q75_lanczos_center.jpeg" alt="Arman Asgharpoor Golroudbari"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://armanasq.github.io/">Arman Asgharpoor Golroudbari</a></h5>
      <h6 class="card-subtitle">Space-AI Researcher</h6>
      <p class="card-text">My research interests revolve around planetary rovers and spacecraft vision-based navigation.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-comment-alt"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:a.asgharpoor1993@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=IlAgF9UAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/armanasq" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://linkedin.com/in/asgharpoor" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/my-orcid?orcid=0000-0001-6271-4533" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.webofscience.com/wos/author/record/IAN-3152-2023" target="_blank" rel="noopener">
        <i class="ai ai-publons"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://researchgate.net/profile/Arman_Asgharpoor" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/uploads/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  

  

  
  <section id="comments">
    
  
  <script src="https://giscus.app/client.js"
          data-repo="Armanasq/Armanasq.github.io"
          data-repo-id="R_kgDOJi13ZQ"
          data-category="[ENTER CATEGORY NAME HERE]"
          data-category-id="[ENTER CATEGORY ID HERE]"
          data-mapping="pathname"
          data-strict="0"
          data-reactions-enabled="1"
          data-emit-metadata="0"
          data-input-position="top"
          data-theme="preferred_color_scheme"
          data-lang="en"
          data-loading="lazy"
          crossorigin="anonymous"
          async>
  </script>


  </section>
  










  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2024 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.85070d5fe00d43eaedff44310b81dc2c.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>













  
    
      
      <!DOCTYPE html>
<html>
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-55GQYC5GYC"></script>

<style>
  .myImg {
    border-radius: 5px;
    cursor: pointer;
    transition: 0.3s;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  .myImg:hover {
    opacity: 0.7;
    cursor: pointer;
  }

   
  .modal-img {
    display: none;  
    position: fixed;  
    z-index: 1;  
    padding-top: 150px;  
    left: 0;
    top: 0px;
    width: 100%;  
    height: 100%;  
    overflow: visible;  
    background-color: rgb(0, 0, 0);  
    background-color: rgba(0, 0, 0, 0.6);  
    margin-left: auto;
    margin-right: auto;
  }

   
  .modal-content {
    display: block;
    margin-left: auto;
    margin-right: auto;
    max-width: 80%;
    max-height: 80%;

  }

   
  #caption {
    margin-left: auto;
    margin-right: auto;
    width: 80%;
    max-width: 700px;
    text-align: center;
    padding: 10px 0;

  }

   
  .modal-content,
  #caption {
    -webkit-animation-name: zoom;
    -webkit-animation-duration: 0.6s;
    animation-name: zoom;
    animation-duration: 0.6s;
    margin-left: auto;
    margin-right: auto;
  }

  @-webkit-keyframes zoom {
    from {
      -webkit-transform: scale(0);
    }
    to {
      -webkit-transform: scale(1);
    }
  }

  @keyframes zoom {
    from {
      transform: scale(0);
    }
    to {
      transform: scale(1);
    }
  }

   
  
 .modal-close {
    position: absolute;
    top: -55px;
    right: 0;
    font-size: 40px;
    font-weight: bold;
    transition: 0.3s;
    cursor: pointer;
  }

  .modal-close:hover,
  .modal-close:focus {
    color: #bbb;
    text-decoration: none;
  }

   
  @media only screen and (max-width: 900px) {
    .modal-content {
      width: 90%;
    }
  }

  .test:hover {
    scale: 1.2;
  }







  .navbar-nav {
    font-size:20px;
    font-family: Merriweather,sans-serif;
  }

  .robotic-section-container {
    display: flex;
    flex-wrap: wrap;
    justify-content: space-between;
    max-width: 1200px;
    margin: 0 auto;
  }
  
  .robotic-section {
    flex-basis: calc(30.33% - 12px);
    margin: 10px;
    background-color: #fff;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    transition: box-shadow 0.3s ease-in-out;
    overflow: hidden;
  }
  
  .robotic-section:hover {
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
  }
  
  .robotic-section-content {
    text-align: center;
    padding: 20px;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    height: 100%;
  }
  
  .robotic-section-content .image-placeholder {
    width: 300px;
    height: 300px;
    margin: 0 auto;
    display: flex;
    align-items: center;
    justify-content: center;
    background-color: #f1f1f1;
  }
  
  .robotic-section-content .image-placeholder img {
    max-width: 100%;
    max-height: 100%;
    object-fit: contain;
  }
  
  .robotic-section-content-h2 {
    margin-top: 10px;
    font-size: 1.rem;
    font-weight: bold;
    color: #333;
  }
  
  .robotic-section-content-h2 :hover{
    font-size: 10px
  }
  .robotic-section-content-h2 {
    margin-top: 10px;
    color: #777;
    font-size: 1.2rem;
  }
  
  .robotic-section-content .text-placeholder {
    height: 80px;
    background-color: #f1f1f1;
  }
  
  .robotic-section-content a {
    display: inline-block;
    margin-top: 20px;
    padding: 10px 20px;
    background-color: #FF4081;
    color: #fff;
    text-decoration: none;
    border-radius: 4px;
    font-weight: bold;
    transition: background-color 0.3s ease-in-out;
  }
  
  .robotic-section-content a:hover {
    background-color: #E91E63;
  }
  
   
  @media (max-width: 768px) {
    .robotic-section {
      flex-basis: calc(50% - 40px);
    }
  }
  
   
  @media (max-width: 480px) {
    .robotic-section {
      flex-basis: 100%;
    }
  }
</style>
</head>
<body>

<div id="myModal" class="modal-img">
  <div class="modal-content">
    <span class="modal-close">&times;</span>
    <img id="img01" style="margin-left: auto; margin-right: auto;">
    <div id="caption"></div>
  </div>
</div>




<script>
    
    var modal = document.getElementById("myModal");
    
    
    var images = document.querySelectorAll("img.myImg");
    
    
    var modalImg = document.getElementById("img01");
    var captionText = document.getElementById("caption");
    
    
    for (var i = 0; i < images.length; i++) {
      
      images[i].setAttribute("data-src", images[i].src);
      
      images[i].addEventListener("click", function() {
        
        modalImg.src = this.getAttribute("data-src");
        captionText.innerHTML = this.alt;
        
        modal.style.display = "block";
      });
    }
    
    
    var modalClose = document.querySelector(".modal-content .modal-close");
    
    
    modalClose.onclick = function() {
      modal.style.display = "none";
    };
    
















    
    </script>
    
    </body>
    </html>
      
    
  






</body>
</html>
