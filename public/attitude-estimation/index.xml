<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Attitude Estimation | </title>
    <link>https://armanasq.github.io/attitude-estimation/</link>
      <atom:link href="https://armanasq.github.io/attitude-estimation/index.xml" rel="self" type="application/rss+xml" />
    <description>Attitude Estimation</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 25 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_512x512_fill_lanczos_center_3.png</url>
      <title>Attitude Estimation</title>
      <link>https://armanasq.github.io/attitude-estimation/</link>
    </image>
    
    <item>
      <title>CNN Based Attitude Estimation</title>
      <link>https://armanasq.github.io/Attitude-Estimation-CNN/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://armanasq.github.io/Attitude-Estimation-CNN/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#problem-definition&#34;&gt;Problem definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#attitude&#34;&gt;Attitude&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attitude-determination-from-inertial-sensors&#34;&gt;Attitude Determination from Inertial Sensors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#methodology&#34;&gt;Methodology&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#deep-learning-model&#34;&gt;Deep Learning Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-function&#34;&gt;Loss Function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#experiment&#34;&gt;Experiment&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#repoimu-t-stick&#34;&gt;RepoIMU T-stick&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#repoimu-t-pendulum&#34;&gt;RepoIMU T-pendulum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sassari&#34;&gt;Sassari&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#oxiod&#34;&gt;OxIOD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mav-dataset&#34;&gt;MAV Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#euroc-mav&#34;&gt;EuRoC MAV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tum-vi&#34;&gt;TUM-VI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kitti&#34;&gt;KITTI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ridi&#34;&gt;RIDI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ronin&#34;&gt;RoNIN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#broad&#34;&gt;BROAD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;This article discusses the importance of accurate and precise attitude determination in navigation for air and space vehicles. Various instruments and sensors have been developed over the last few decades to achieve this goal. However, the cost and complexity of these instruments can be prohibitive. To address this issue, Multi-Data Sensor Fusion (MSDF) techniques have been developed, which allow for the use of multiple sensors to sense a quantity from different perspectives or sense multiple quantities to reduce errors and uncertainties. This article explores the use of MEMS-based Inertial Measurement Units (IMUs) in attitude determination and discusses the challenges associated with noise and bias. Finally, the article describes different forms of attitude representation, including Tait-Bryan angles, rotation matrices, and quaternions.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Achieving accurate and precise attitude determination or estimation is essential for successful navigation of air and space vehicles. To achieve this goal, each vehicle must determine and control its attitude based on mission requirements. A wide variety of instruments, sensors, and algorithms have been developed over the last few decades, distinguished by their cost and complexity. However, using an accurate sensor can exponentially increase the cost, which may exceed the budget.&lt;/p&gt;
&lt;p&gt;A low-cost solution for achieving high accuracy is to use multiple sensors (homogeneous or heterogeneous) to sense a quantity from different perspectives or sense multiple quantities to reduce errors and uncertainties. Multiple sensors fuse their data to achieve a more accurate quantity, a technique called Multi-Data Sensor Fusion (MSDF). MSDF uses mathematical methods to reduce noise and uncertainty and to estimate the quantity based on prior data. MSDF can also be used for attitude determination.&lt;/p&gt;
&lt;p&gt;Attitude determination methods can be broadly divided into two classes: single-point and recursive estimation. The first method calculates the attitude by using two or more vector measurements at a single point in time. In contrast, recursive methods use the combination of measurements over time and the system&amp;rsquo;s mathematical model. Obtaining precise attitude determination is challenging due to errors in system modeling, processes, and measurements. Increasing the sensor&amp;rsquo;s precision may exponentially increase the cost, and sometimes, achieving the required precision may only be possible at an exorbitant cost.&lt;/p&gt;
&lt;p&gt;One approach for determining attitude is to use inertial navigation algorithms based on inertial sensors. Inertial navigation is based on the Dead Reckoning method, which uses different types of inertial sensors, such as accelerometers and gyroscopes, known as Inertial Measurement Units (IMUs). The position, velocity, and attitude of a moving object can be determined using numerical integration of IMU measurements.&lt;/p&gt;
&lt;p&gt;The use of low-cost Micro Electro Mechanical Systems (MEMS) based IMUs has grown in the past decade. Due to recent advances in MEMS technology, IMUs have become smaller, cheaper, and more accurate, making them available for use in mobile robots, smartphones, drones, and autonomous vehicles. However, these sensors suffer from noise and bias, which directly affect the performance of attitude estimation algorithms.&lt;/p&gt;
&lt;p&gt;To tackle this problem and increase the accuracy and reliability of attitude estimation techniques, different MSDF techniques and Deep Learning models have been developed in the past few decades.&lt;/p&gt;
&lt;p&gt;Attitude can be represented in many different forms. The Tait-Bryan angles (also called Euler angles) are the most familiar form and are known as yaw, pitch, and roll (or heading, elevation, and bank). Engineers widely use rotation matrices and quaternions, but quaternions are less intuitive.&lt;/p&gt;
&lt;p&gt;Related Work
In the past decade, extensive research has been conducted on inertial navigation techniques. These studies can be roughly divided into three categories: estimation methods, Multi-Data Sensor Fusion (MSDF) techniques, and evolutionary/AI algorithms. The Kalman Filter family (i.e., EKF, UKF, MEKF), as well as other commonly used algorithms such as Madgwick and Mahony, are based on the dynamic model of the system. The Kalman filter was first introduced in [], and its variants such as EKF, UKF, and MEKF have been implemented for attitude estimation applications.&lt;/p&gt;
&lt;p&gt;In [], Carsuo et al. compared different sensor fusion algorithms for inertial attitude estimation. This comparative study showed that the performance of Sensor Fusion Algorithms (SFA) is highly dependent on parameter tuning, and fixed parameter values are not suitable for all applications. Therefore, parameter tuning is one of the disadvantages of conventional attitude estimation methods. This problem could be tackled by using evolutionary algorithms such as fuzzy logic and deep learning. Most of the deep learning approaches in inertial navigation have focused on inertial odometry, and only a few of them have attempted to solve the inertial attitude estimation problem. Deep learning methods are usually used for visual or visual-inertial based navigation. Chen et al.,&lt;/p&gt;
&lt;p&gt;Rochefort et al. proposed a neural networks-based satellite attitude estimation algorithm using a quaternion neural network. This study presents a new way of integrating the neural network into the state estimator and develops a training procedure that is easy to implement. This algorithm provides the same accuracy as the EKF with significantly lower computational complexity. In [Chang 2011], a Time-Varying Complementary Filter (TVCF) has been proposed to use a fuzzy logic inference system for CF parameter adjustment for attitude estimation. Chen et al. used deep recurrent neural networks for estimating the displacement of a user over a specified time window. OriNet [], introduced by Esfahani et al., estimates the orientation in quaternion form based on LSTM layers and IMU measurements.&lt;/p&gt;
&lt;p&gt;[300] developed a sensor fusion method to provide pseudo-GPS position information by using empirical mode decomposition threshold filtering (EMDTF) for IMU noise elimination and a long short-term memory (LSTM) neural network for pseudo-GPS position prediction during GPS outages.&lt;/p&gt;
&lt;p&gt;Dhahbane et al. [301] developed a neural network-based complementary filter (NNCF) with ten hidden layers and trained by Bayesian Regularization Backpropagation (BRB) training algorithm to improve the generalization qualities and solve the overfitting problem. In this method, the output of the complementary filter is used as the neural network input.&lt;/p&gt;
&lt;p&gt;Li et al. proposed an adaptive Kalman filter with a fuzzy neural network for a trajectory estimation system that mitigates measurement noise and undulation for the implementation of the touch interface. An Adaptive Unscented Kalman Filter (AUKF) method was introduced to combine sensor fusion algorithms with deep learning to achieve high-precision attitude estimation based on low-cost, small size IMU in high dynamic environments. Deep Learning has been used in [] to denoise the gyroscope measurements for an open-loop attitude estimation algorithm.&lt;/p&gt;
&lt;p&gt;Weber et al. [] present a real-time-capable neural network for robust IMU-based attitude estimation. In this study, the accelerometer, gyroscope, and IMU sampling rate were used as inputs to the neural network, and the output is the attitude in quaternion form. This model is only suitable for estimating the roll and pitch angle. Sun et al. introduced a two-stage deep learning framework for inertial odometry based on LSTM and FFNN architecture. In this study, the first stage is used to estimate the orientation, and the second stage is used to estimate the position.&lt;/p&gt;
&lt;p&gt;A Neural Network model has been developed by Santos et al. [] for static attitude determination based on PointNet architecture. They used attitude profile matrix as input. This model uses Swish activation function and Adam as its optimizer.&lt;/p&gt;
&lt;p&gt;A deep learning model has been developed to estimate the Multirotor Unmanned Aerial Vehicle (MUAV) based on Kalman filter and Feed Forward Neural Network (FFNN) in []. LSTM framework has been used in [] the Euler angles using acceleromter, gyroscope and magnetometer but the sensor sampling rate has not been considered.&lt;/p&gt;
&lt;p&gt;In the below table, we summarized some of the related works in the field of navigation using deep learning.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Year/Month&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Application&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PoseNet&lt;/td&gt;
&lt;td&gt;2015/12&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Relocalization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VINet&lt;/td&gt;
&lt;td&gt;2017/02&lt;/td&gt;
&lt;td&gt;Vision +Inertial&lt;/td&gt;
&lt;td&gt;Visual Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DeepVO&lt;/td&gt;
&lt;td&gt;2017/05&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Visual Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VidLoc&lt;/td&gt;
&lt;td&gt;2017/07&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Relocalization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PoseNet+&lt;/td&gt;
&lt;td&gt;2017/07&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Relocalization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SfmLearner&lt;/td&gt;
&lt;td&gt;2017/07&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Visual Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IONet&lt;/td&gt;
&lt;td&gt;2018/02&lt;/td&gt;
&lt;td&gt;Inertial Only&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UnDeepVO&lt;/td&gt;
&lt;td&gt;2018/05&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Visual Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VLocNet&lt;/td&gt;
&lt;td&gt;2018/05&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Relocalization, Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RIDI&lt;/td&gt;
&lt;td&gt;2018/09&lt;/td&gt;
&lt;td&gt;Inertial Only&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SIDA&lt;/td&gt;
&lt;td&gt;2019/01&lt;/td&gt;
&lt;td&gt;Inertial Only&lt;/td&gt;
&lt;td&gt;Domain Adaptation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VIOLearner&lt;/td&gt;
&lt;td&gt;2019/04&lt;/td&gt;
&lt;td&gt;Vision + Inertial&lt;/td&gt;
&lt;td&gt;Visual Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brossard et al.&lt;/td&gt;
&lt;td&gt;2019/05&lt;/td&gt;
&lt;td&gt;Inertial Only&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SelectFusion&lt;/td&gt;
&lt;td&gt;2019/06&lt;/td&gt;
&lt;td&gt;Vision + Inertial + LIDAR&lt;/td&gt;
&lt;td&gt;VIO andSensor Fusion&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LO-Net&lt;/td&gt;
&lt;td&gt;2019/06&lt;/td&gt;
&lt;td&gt;LIDAR&lt;/td&gt;
&lt;td&gt;LIDAR Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;L3-Net&lt;/td&gt;
&lt;td&gt;2019/06&lt;/td&gt;
&lt;td&gt;LIDAR&lt;/td&gt;
&lt;td&gt;LIDAR Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lima et al.&lt;/td&gt;
&lt;td&gt;2019/8&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DeepVIO&lt;/td&gt;
&lt;td&gt;2019/11&lt;/td&gt;
&lt;td&gt;Vision+Inertial&lt;/td&gt;
&lt;td&gt;Visual Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OriNet&lt;/td&gt;
&lt;td&gt;2020/4&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GALNet&lt;/td&gt;
&lt;td&gt;2020/5&lt;/td&gt;
&lt;td&gt;Inertial, Dynamic and Kinematic&lt;/td&gt;
&lt;td&gt;Autonomous Cars&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PDRNet&lt;/td&gt;
&lt;td&gt;2021/3&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Pedestrian Dead Reckoning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kim et al.&lt;/td&gt;
&lt;td&gt;2021/4&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RIANN&lt;/td&gt;
&lt;td&gt;2021/5&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Attitude Estimation&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;problem-definition&#34;&gt;Problem definition&lt;/h2&gt;
&lt;p&gt;This study addressed the real time inertial attitude estimation based on gyroscope and accelerometer measuerments. The IMU sensor considered to rigidly attached to the object of interest. The estimaation is based on the current and pervious measurements of gyroscope and accelerometer which is used to fed into a Neural Network model to estimate the attitude. Despite almost all pervious studies, we do not consider any initial reset period for filter convergence. Usually, to aviod any singularites and have the least number of redundant parameters, quanternion representation with the componnets $[w, x, y, z]$ is used, instead of Direction Cosine Matrix (DCM) or Euler angles. But as the angles have different features and dependencies to the sensor readings, we convert quaternions to Euler angles and tried to estimate the roll and pitch based on the accelerometer and gyroscope readings. The quaternions could be converted to Euler angles using the following equations:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\phi = \arctan \left(\frac{2 \left(q_{w} q_{x} + q_{y} q_{z}\right)}{1 - 2 \left(q_{x}^{2} + q_{y}^{2}\right)}\right) \\
\end{gathered}
\end{equation}
$$
&lt;/div&gt;&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\theta = \arcsin \left(2 \left(q_{w} q_{y} - q_{z} q_{x}\right)\right) \\
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;h3 id=&#34;attitude&#34;&gt;Attitude&lt;/h3&gt;
&lt;p&gt;Attitude is the mathematical representation of the orientation in space related to the reference frames. Attitude parameters (attitude coordinates) refer to sets of parameters (coordinates) that fully describe a rigid body&amp;rsquo;s attitude, which are not unique expressions. There are many ways to represent the attitude of a rigid body. The most common are the Euler angles, the rotation matrix, and the quaternions. The Euler angles are the most familiar form and known as yaw, pitch, and roll (or heading, elevation, and bank). Engineers widely use rotation matrix and quaternions, but the quaternions are less intuitive. The Euler angles are defined as the rotations about the three orthogonal axes of the body frame. But, the Euler angles suffer from the problem of gimbal lock. The rotation matrix is a 3x3 matrix that represents the orientation of the body frame with respect to the inertial frame which leads to have 6 redundant parameters. The quaternions are a 4x1 vector which are more suitable for attitude estimation because they are not subject to the gimbal lock problem and have the least redundant parameters. The quaternions are defined as the following:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\mathbf{q} =
\begin{bmatrix}
q_0 \\
q_1 \\
q_2 \\
q_3
\end{bmatrix}
\end{gathered}
\end{equation}
$$
&lt;div&gt;
&lt;p&gt;where $q_0$ is the scalar part and $q_1$, $q_2$, and $q_3$ are the vector part. And the following equation shows the relationship between the quaternions and the euler angles:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\mathbf{q} =
\begin{bmatrix}
\cos(\phi/2) \cos(\theta/2) \cos(\psi/2) + \sin(\phi/2) \sin(\theta/2) \sin(\psi/2) \\
\sin(\phi/2) \cos(\theta/2) \cos(\psi/2) - \cos(\phi/2) \sin(\theta/2) \sin(\psi/2) \\
\cos(\phi/2) \sin(\theta/2) \cos(\psi/2) + \sin(\phi/2) \cos(\theta/2) \sin(\psi/2) \\
\cos(\phi/2) \cos(\theta/2) \sin(\psi/2) - \sin(\phi/2) \sin(\theta/2) \cos(\psi/2)
\end{bmatrix}
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;where $\phi$, $\theta$, and $\psi$ are the Euler angles.&lt;/p&gt;
&lt;p&gt;Attitude determination and control play a vital role in Aerospace engineering. Most aerial or space vehicles have subsystem(s) that must be pointed to a specific direction, known as pointing modes, e.g., Sun pointing, Earth pointing. For example, communications satellites, keeping satellites antenna pointed to the Earth continuously, is the key to the successful mission. That will be achieved only if we have proper knowledge of the vehicle’s orientation; in other words, the attitude must be determined. Attitude determination methods can be divided in two categories: static and dynamic.&lt;/p&gt;
&lt;p&gt;Static attitude determination is a point-to-point time independent attitude determining method with the memoryless approach is called attitude determination. It is the observations or measurements processing to obtain the information for describing the object&amp;rsquo;s orientation relative to a reference frame. It could be determined by measuring the directions from the vehicle to the known points, i.e., Attitude Knowledge. Due to accuracy limit, measurement noise, model error, and process error, most deterministic approaches are inefficient for accurate prospects; in this situation, using statistical methods will be a good solution&lt;/p&gt;
&lt;p&gt;Dynamic attitude determination methods also known as Attitude estimation refers to using mathematical methods and techniques (e.g., statistical and probabilistic) to predict and estimate the future attitude based on a dynamic model and prior measurements. These techniques fuse data that retain a series of measurements using algorithms such as filtering, Multi-Sensor-Data-Fusion. The most commonly use attitude estimation methods are Extended Kalman Filter, Madgwick, and Mahony.&lt;/p&gt;
&lt;h3 id=&#34;attitude-determination-from-inertial-sensors&#34;&gt;Attitude Determination from Inertial Sensors&lt;/h3&gt;
&lt;p&gt;Attitude could be measured based on accelerometer and gyroscope readings. Gyroscope meaesures the angular velocity in body frame about the three orthogonal axes (i.e., x,y,z) usually denotd by $p$, $q$, and $r$ and relays on the principle of the angular momentum conservation. The gyroscope output, body rates with respect to the inertial frame which expressed in body frame is:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\mathbf{\omega} =
\begin{bmatrix}
\omega_x \\
\omega_y \\
\omega_z
\end{bmatrix}
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;where $\omega_x$, $\omega_y$, and $\omega_z$ are the angular velocity about the x, y, and z axes, respectively. The accelerometer measures the linear acceleration in body frame about the three orthogonal axes (i.e., x,y,z) usually denotd by $a_x$, $a_y$, and $a_z$ and relays on the principle of Newton&amp;rsquo;s second law. The accelerometer output, linear acceleration with respect to the inertial frame which expressed in body frame is:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\mathbf{a} =
\begin{bmatrix}
a_x \\
a_y \\
a_z
\end{bmatrix}
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;Attitude can be determined from the accelerometer and gyroscope readings using the following equations:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\phi = \arctan\left(\frac{a_y}{a_z}\right) \\
\theta = \arctan\left(\frac{-a_x}{\sqrt{a_y^2 + a_z^2}}\right) \\
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;Attitude update using gyroscope readings:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\dot{\phi} = p + q \sin(\phi) \tan(\theta) + r \cos(\phi) \tan(\theta) \\
\dot{\theta} = q \cos(\phi) - r \sin(\phi) \\
\dot{\psi} = \frac{q \sin(\phi)}{\cos(\theta)} + \frac{r \cos(\phi)}{\cos(\theta)} \\
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;where $\phi$, $\theta$, and $\psi$ are the Euler angles. Or in the quaternion form:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\mathbf{\dot{q}} = \frac{1}{2} \mathbf{q} \otimes \mathbf{\omega}
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;where $\mathbf{\dot{q}}$ is the quaternion derivative, $\mathbf{q}$ is the quaternion, and $\mathbf{\omega}$ is the angular velocity. It is necessary to mention that heading angle $\psi$ is not determined from the accelerometer, and gyroscope readings only can be used to measure the rate of change of the heading angle.&lt;/p&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;p&gt;An eficiant way to handel the sequnetial data such as IMU sensor measurements is to use seqential modeling. This type of modeling can be used to carry out time series data. In this project, we will use the Long Short-Term Memory (LSTM) network to model the sequential data. The LSTM network is a type of recurrent neural network (RNN) that is capable of learning order dependence in sequence prediction problems. It is a complex network of artificial neurons, arranged in a long chain. Each unit in the chain contains a memory cell that has three gates: input gate, forget gate, and output gate. The LSTM network is trained using backpropagation through time and overcomes the vanishing gradient problem. The LSTM network is able to learn long-term dependencies and is therefore very well suited to predict the next value in a sequence.&lt;/p&gt;
&lt;h3 id=&#34;deep-learning-model&#34;&gt;Deep Learning Model&lt;/h3&gt;
&lt;p&gt;The proposed method for attitude estimation based on inertial measurements, takes a sequence of accelerometer and gyroscope readings and its corexponding time stamps as an input, and outputs roll and pitch angles. This end-to-end deep learning framework, implicitly handels the IMU measruments noise and bias. This solution is based on CNN layers combined with LSTM layers. The CNN layers are used to extract the features from the accelerometer and gyroscope readings, and the LSTM layers are used to learn the temporal dependencies between the extracted features. The input to the network is a sequence of accelerometer and gyroscope readings in a window of 200 readings. Accelerometer and gyroscope measurements are processed seprately by 1-dimentional CNN layers with the kernel size of 11 and 128 filters. The output of the CNN layers are concatinated and fed to the LSTM layer after max pooling layer of size 3. This bi-stack LSTM layer has 128 unit. The output of this layer seprately fed to two LSTM layer, each with 128 units and the outputs are fed to two fully connected layers with 1 units. The output of the fully connected layers are the roll and pitch angles. After each LSTM layer, a dropout layer with 0.25 probability is added to prevent overfitting. This layer is used to randomly drop out 25% of the units in the layer during training. The input in each timp step is a window of 200 accelerometer and gyroscope readings which consists of 100 past and 100 future readings. The stride of window is 10 frames which led the model to estimate the attitud every 10 frames. The network is trained using the Adam optimizer with the learning rate of [lr] and the loss function is the mean squared error. The network is trained on the BROAD and OxIOD datasets for [epochs] epochs with the batch size of [batch_size]. The network is implemented using the Keras library with the TensorFlow backend.&lt;/p&gt;
&lt;h3 id=&#34;loss-function&#34;&gt;Loss Function&lt;/h3&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment&lt;/h2&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;IMU Datasets are used to evaluate the performance of the attitude estimation algorithms. The datasets are divided into two categories: synthetic and real-world. The synthetic datasets are generated by simulating the IMU measurements. The real-world datasets are collected from the real-world experiments and could be divided into two categories: indoor and outdoor. The indoor experiments are conducted in a controlled environment, e.g., a laboratory. The outdoor experiments are conducted in an uncontrolled environment, e.g., a car. Also, to train, validate and test any neural network model, we need a database including accurate input and output. A Deep Learning model&amp;rsquo;s performance will be directly affected by the data that is used for it. So, to train the Deep Learning model we need a database containing the input and output parameters with following conditions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The input and output parameters should be accurate.&lt;/li&gt;
&lt;li&gt;The amount of data must be sufficient to train the Deep Learning model&lt;/li&gt;
&lt;li&gt;The data should be diverse enough to cover all the possible scenarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the following sections, we will present some of the most commonly used IMU Datasets.&lt;/p&gt;
&lt;h2 id=&#34;repoimu-t-stick&#34;&gt;RepoIMU T-stick&lt;/h2&gt;
&lt;p&gt;The RepoIMU T-stick [&lt;a id=&#34;d1&#34; href=&#34;#repoT&#34;&gt;1&lt;/a&gt;] is a small, low-cost, and high-performance inertial measurement unit (IMU) that can be used for a wide range of applications. The RepoIMU T-stick is a 9-axis IMU that measures the acceleration, angular velocity, and magnetic field. This database contains two separate sets of experiments recorded with a T-stick and a pendulum. A total of 29 trials were collected on the T-stick, and each trial lasted approximately 90 seconds. As the name suggests, the IMU is attached to a T-shaped stick equipped with six reflective markers. Each experiment consists of slow or fast rotation around a principal sensor axis or translation along a principal sensor axis. In this scenario, the data from the Vicon Nexus OMC system and the XSens MTi IMU are synchronized and provided at a frequency of 100 Hz. The authors clearly state that the IMU coordinate system and the ground trace are not aligned and propose a method to compensate for one of the two required rotations based on quaternion averaging. Unfortunately, some experiments contain gyroscope clipping and ground tracking, which significantly affect the obtained errors. Therefore, careful pre-processing and removal of some trials should be considered when using the dataset to evaluate the model&amp;rsquo;s accuracy. The dataset is available at &lt;a href=&#34;https://github.com/agnieszkaszczesna/RepoIMU&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;repoimu-t-pendulum&#34;&gt;RepoIMU T-pendulum&lt;/h2&gt;
&lt;p&gt;The second part of the RepoIMU dataset contains data from a triple pendulum on which the IMUs are mounted. Measurement data is provided at 90 Hz or 166 Hz. However, the IMU data contains duplicate samples. This is usually the result of artificial sampling or transmission problems where missed samples are replaced by duplicating the last sample received, effectively reducing the sampling rate. The sampling rate achieved when discarding frequent samples is about 25 Hz and 48 Hz for the accelerometer and gyroscope, respectively. Due to this issue, it is not recommended to use this database for model training and evaluation. Due to this fact, we cannot recommend using pendulum tests to evaluate the accuracy of IOE with high precision.&lt;/p&gt;
&lt;h2 id=&#34;sassari&#34;&gt;Sassari&lt;/h2&gt;
&lt;p&gt;The Sassari dataset published in [&lt;a id=&#34;d2&#34; href=&#34;#sassari&#34;&gt;2&lt;/a&gt;] aims to validate a parameter tuning approach based on the orientation difference of two IMUs of the same model. To facilitate this, six IMUs from three manufacturers (Xsens, APDM, Shimmer) are placed on a wooden board. Rotation around specific axes and free rotation around all axes are repeated at three different speeds. Data is synchronized and presented at 100 Hz. Local coordinate frames are aligned by precise manual placement. There are 18 experiments (3 speeds, 3 IMU models, and 2 IMUs of each model) in this dataset.&lt;/p&gt;
&lt;p&gt;According to these points, this database seems to be a suitable option for training, evaluating, and testing the model, but some essential points should be paid attention to. The inclusion of different speeds and different types of IMUs helped to diversify the data set. However, all motions occur in a homogeneous magnetic field and do not involve pure translational motions. Therefore, this data set does not have a robust variety in terms of the type of movement and the variety of magnetic data. Therefore, the model trained with it cannot be robust and general. However, it can be used to evaluate the model.&lt;/p&gt;
&lt;p&gt;The total movement duration of all three trials is 168 seconds, with the most extended movement phase lasting 30 seconds. For this reason, considering the short time, it is not a suitable option for training. The dataset is available at &lt;a href=&#34;https://ieee-dataport.org/documents/mimuopticalsassaridataset&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;oxiod&#34;&gt;OxIOD&lt;/h2&gt;
&lt;p&gt;The Oxford Inertial Odometry Dataset (OxIOD) [&lt;a id=&#34;d3&#34; href=&#34;#oxiod&#34;&gt;3&lt;/a&gt;] is a large set of inertial data recorded by smartphones (mainly iPhone 7 Plus) at 100 Hz. The suite consists of 158 tests and covers a distance of over 42 km, with OMC ground track available for 132 tests. The purpose of this set is inertial odometry. Therefore, it does not include pure rotational movements and pure translational movements, which are helpful for systematically evaluating the model&amp;rsquo;s performance under different conditions; however, it covers a wide range of everyday movements.&lt;/p&gt;
&lt;p&gt;Due to the different focus, some information (for example, the alignment of the coordinate frames) is not accurately described. In addition, the orientation of the ground trace contains frequent irregularities (e.g., jumps in orientation that are not accompanied by similar jumps in the IMU data). The dataset is available at &lt;a href=&#34;http://deepio.cs.ox.ac.uk/&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;mav-dataset&#34;&gt;MAV Dataset&lt;/h2&gt;
&lt;p&gt;Most datasets suitable for the simultaneous localization and mapping problem are collected from sensors such as wheel encoders and laser range finders mounted on ground robots. For small air vehicles, there are few datasets, and MAV Dataset [&lt;a id=&#34;d4&#34; href=&#34;#mav&#34;&gt;4&lt;/a&gt;] is one of them. This data set was collected from the sensor array installed on the &amp;ldquo;Pelican&amp;rdquo; quadrotor platform in an environment. The sensor suite includes a forward-facing camera, a downward-facing camera, an inertial measurement unit, and a Vicon ground-tracking system. Five synchronized datasets are presented&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt; 1LoopDown &lt;/li&gt;
&lt;li&gt; 2LoopsDown &lt;/li&gt;
&lt;li&gt; 3LoopsDown &lt;/li&gt;
&lt;li&gt; hoveringDown &lt;/li&gt;
&lt;li&gt; randomFront &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These datasets include camera images, accelerations, heading rates, absolute angles from the IMU, and ground tracking from the Vicon system. The dataset is available at &lt;a href=&#34;https://sites.google.com/site/sflyorg/mav-datasets&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;euroc-mav&#34;&gt;EuRoC MAV&lt;/h2&gt;
&lt;p&gt;The EuRoC MAV dataset [&lt;a id=&#34;d5&#34; href=&#34;#euroc&#34;&gt;5&lt;/a&gt;] is a large dataset collected from a quadrotor MAV. The dataset contains the internal flight data of a small air vehicle (MAV) and is designed to reconstruct the visual-inertial 3D environment. The six experiments performed in the chamber and synchronized and aligned using the OMC-based Vicon ground probe are suitable for training and evaluating the model&amp;rsquo;s accuracy. It should be noted that camera images and point clouds are also included.&lt;/p&gt;
&lt;p&gt;This set does not include magnetometer data, which limits the evaluation of three degrees of freedom and is only for two-way models (including accelerometer and gyroscope). Due to the nature of the data, most of the movement consists of horizontal transfer and rotation around the vertical axis. This slope does not change much during the experiments. For this reason, it does not have a suitable variety for model training. Since flight-induced vibrations are clearly visible in the raw accelerometer data, the EuRoC MAV dataset provides a unique test case for orientation estimation with perturbed accelerometer data. The dataset is available at &lt;a href=&#34;https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;tum-vi&#34;&gt;TUM-VI&lt;/h2&gt;
&lt;p&gt;The TUM Visual-Inertial Dataset [&lt;a id=&#34;d6&#34; href=&#34;#tumvi&#34;&gt;6&lt;/a&gt;] suitable for optical-inertial odometry consists of 28 experiments with a handheld instrument equipped with a camera and IMU. Due to this application focus, most experiments only include OMC ground trace data at the beginning and at the end of the experiment. However, the six-chamber experiments include complete OMC data. They are suitable for evaluating the accuracy of the neural network model. Similar to the EuRoC MAV data, the motion consists mainly of horizontal translation and rotation about the vertical axis, and magnetometer data is not included. The dataset is available at &lt;a href=&#34;https://vision.in.tum.de/data/datasets/visual-inertial-dataset&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;kitti&#34;&gt;KITTI&lt;/h2&gt;
&lt;p&gt;The KITTI Vision Benchmark Suite [&lt;a id=&#34;d7&#34; href=&#34;#kitti&#34;&gt;7&lt;/a&gt;] is a large set of data collected from a stereo camera and a laser range finder mounted on a car. The dataset includes 11 sequences with a total of 20,000 images. The dataset is suitable for evaluating the accuracy of the model in the presence of optical flow. However, the dataset does not include magnetometer data, which limits the evaluation of three degrees of freedom and is only for two-way models (including accelerometer and gyroscope). The dataset is available at &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_odometry.php&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ridi&#34;&gt;RIDI&lt;/h2&gt;
&lt;p&gt;RIDI datasets were collected over 2.5 hours on 10 human subjects using smartphones equipped with a 3D tracking capability to collect IMU-motion data placed on four different surfaces (e.g., the hand, the bag, the leg pocket, and the body). The ground-truth motion data was produced by the Visual Inertial SLAM technique. They recorded linear accelerations, angular velocities, gravity directions, device orientations (via Android APIs), and 3D camera poses with a Google Tango phone, Lenovo Phab2 Pro. Visual Inertial Odometry on Tango provides camera poses that are accurate enough for inertial odometry purposes (less than 1 meter after 200 meters of tracking).The dataset is available at &lt;a href=&#34;http://www.cvlibs.net/datasets/kitti/eval_odometry.php&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;ronin&#34;&gt;RoNIN&lt;/h2&gt;
&lt;p&gt;The RoNIN dataset [&lt;a id=&#34;d9&#34; href=&#34;#ridi&#34;&gt;9&lt;/a&gt;] contains over 40 hours of IMU sensor data from 100 human subjects with 3D ground-truth trajectories under natural human movements. This data set provides measurements of the accelerometer, gyroscope, dipstick, GPS, and ground track, including direction and location in 327 sequences and at a frequency of 200 Hz. A two-device data collection protocol was developed. A harness was used to attach one phone to the body for 3D tracking, allowing subjects to control the other phone to collect IMU data freely. It should be noted that the ground track can only be obtained using the 3D tracker phone attached to the harness. In addition, the body trajectory is estimated instead of the IMU. The dataset is available at &lt;a href=&#34;https://yanhangpublic.github.io/ridi/index.html&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;broad&#34;&gt;BROAD&lt;/h2&gt;
&lt;p&gt;The Berlin Robust Orientation Evaluation (BROAD) dataset [&lt;a id=&#34;d10&#34; href=&#34;#broad&#34;&gt;10&lt;/a&gt;] includes a diverse set of experiments covering a variety of motion types, velocities, undisturbed motions, and motions with intentional accelerometer perturbations as well as motions performed in the presence of magnetic perturbations. This data set includes 39 experiments (23 undisturbed experiments with different movement types and speeds and 16 experiments with various intentional disturbances). The data of the accelerometer, gyroscope, magnetometer, quaternion, and ground tracks, are provided in an ENU frame with a frequency of 286.3 Hz. The dataset is available at &lt;a href=&#34;https://github.com/dlaidig/broad&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Based on datasets preprocessing requierments, diversity of motion types, and the availability of ground truth, we selected the BROAD, OxIOD, RIDI, and RoNIN datasets for our experiments and analysis. This combination of datasets provides a wide spectrum of motion types and speeds, as well as the presence of intentional disturbances such as vibration and acceleration. Also, as each dataset has its own sampling frequency, it led us to  train our model on different sampling frequencies which is a key factor for sampling rate robustness. These datasets are come from various applications and motion patterns, which makes them suitable for evaluating the accuracy of the model in different scenarios. The details of the datasets are summarized in Table [1]. Figure [1] shows the collection of the datasets composed of the BROAD, OxIOD, RIDI, and RoNIN datasets which are splited into, training, validation, and testing sets. The validation dataset is used to evaluate the model during training, and the test dataset is used to evaluate the model performace after training.&lt;/p&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;We implementede this model in Keras [v] and Tensorflow [v], also Adam optimizer was used for training with learning rate starts from 0.0001 and controled via ReduceLROnPlateau. The model was trained on a single NVIDIA GeForce GTX 1070 GPU. The model was trained for 100 epochs with a batch size of 500. The training data has been shuffled and split into 75% training and 25% validation. The training and validation loss and accuracy are shown in the following figure. Using EarlyStopping callback, help us to stop the training when no improvement in the validation loss is observed. In addition, checkpoints are saved during training to restore the model to the best validation loss and ensure that the best model is saved.&lt;/p&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;For performance evaluation, we compared the proposed model with RIANN model, EKF, Madgwick, Mahony, and Complementary filter. The results of the proposed model are shown in the following figure. The performance of the propesd model shows that it could be considered as a good alternative to the state-of-the-art methods and convetional filters as the model performed well on a wide range of motion types, patterns, and speeds. While the convetional filters require parameter optimiztion for each motion type&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attitude and Heading Estimation</title>
      <link>https://armanasq.github.io/Attitude-Heading-Estimation/</link>
      <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://armanasq.github.io/Attitude-Heading-Estimation/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#problem-defenition&#34;&gt;Problem Defenition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#literature-review&#34;&gt;Literature Review&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backgroud&#34;&gt;Backgroud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#methodology&#34;&gt;Methodology&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#deep-learning-model&#34;&gt;Deep Learning Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-function&#34;&gt;Loss Function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#experiment&#34;&gt;Experiment&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Despite recent advancments in Micro-Electro Mechanical Systems (MEMS) inertial and magnetic sensors, percices and accurate attitude estimation is a challenging task, especillay in the existance of magnetic distubances or high dynamic motions. This problem  cannot be significantly tackled by conventional methods and clasical estimators. In this paper, an end-to-end deep learning framework is develped to estimate the attitude and heading using inertial and magentic sensors obtained from a low-cost IMU. The proposed model consists of two-layer stacked bidirectional Long-Short Term Mermory (LSTM) and Feed Forward Neural Network layers. The model is trained using a large dataset of IMU measurements collected from publicly availabe datasets inertial orientaion and inertial odometry datasets.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h2 id=&#34;problem-defenition&#34;&gt;Problem Defenition&lt;/h2&gt;
&lt;h2 id=&#34;literature-review&#34;&gt;Literature Review&lt;/h2&gt;
&lt;h2 id=&#34;backgroud&#34;&gt;Backgroud&lt;/h2&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;h3 id=&#34;deep-learning-model&#34;&gt;Deep Learning Model&lt;/h3&gt;
&lt;h3 id=&#34;loss-function&#34;&gt;Loss Function&lt;/h3&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment&lt;/h2&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Attitude Estimation</title>
      <link>https://armanasq.github.io/attitude-estimation/attitude-estimation/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://armanasq.github.io/attitude-estimation/attitude-estimation/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#related-works&#34;&gt;Related works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#problem-definition&#34;&gt;Problem definition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#attitude&#34;&gt;Attitude&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attitude-determination-from-inertial-sensors&#34;&gt;Attitude Determination from Inertial Sensors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#methodology&#34;&gt;Methodology&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#deep-learning-model&#34;&gt;Deep Learning Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-function&#34;&gt;Loss Function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#experiment&#34;&gt;Experiment&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Achieving accurate and precise attitude determination or estimation is needed to perform successful navigation. Each flying vehicle either in air or space, needs to determine and control its attitude based on mission requirements. Vast variety of instruments/sensors and algorithm have been developed in the last decades; they are distinct by their cost and complexity. Use an accurate sensor will exponentially increase the cost which could exceed the budget. A solution for increase the accuracy with low cost is to use multi sensors (homogenous or heterogenous); multiple sensors could sense a quantity from different perspective or sense multi quantities to reduce the error and uncertainty. Multiple sensors fuse their data to achieve more accurate quantity, this method usually called as Multi-Data Sensor Fusion (MSDF). MSDF use mathematical methods to reduce noise, uncertainty and also estimate the quantity based on priori data and it could be utlized for attitude determiation. Attitude determination methods could be broadly divided in two classes, single-point and recursive estimation. First method calculates the attitude by use of two or more vector measurements at a single point of time. Instead, recursive methods use the combination of measurements over time and the system mathematical model. A precise attitude determination is dependent on sensor’s precision, accurate system modeling, and the information processing method. Obtaining this precision is considered a challenging navigation problem due to system modeling, process, and measurements errors. Increase the sensor’s precision may exponentially increase the cost; sometimes, achieving the precision requirements will only be possible for an exorbitant cost.&lt;/p&gt;
&lt;p&gt;One approach for determining the attitude is using inertial navigation algorithms based inertial sensors. Inertial Navigation is based on the Dead Reckoning method. In this method, different types of inertial sensors are used such as accelerometer and gyroscope which called Inertial Measurement Unit (IMU). A moving object&amp;rsquo;s position, velocity, and attitude can be determined using numerical integration of IMU measurements.&lt;/p&gt;
&lt;p&gt;Using low-cost Micro Electro Mechanical Systems (MEMS) based Inertial Measurement Unit (IMU) has been grown in the past decade. Due to recent advances in MEMS technology, IMUs became smaller, cheaper, and more accurate, and they are now available for use in mobile robots, smartphones, drones, and autonomous vehicles. This sensors suffers from noise and bias, which affect dirctly the performance attitude estimation alogrithm.
In the past decades, different MSDF techniques and Deep Learning models have been developed to tackle this problem and increase the accuracy and reliability of attitude estimation techniques.&lt;/p&gt;
&lt;p&gt;Attitude can be represented in many different forms. The Tait-Bryan angles (also called Euler angles) are the most familiar form and known as yaw, pitch, and roll (or heading, elevation, and bank). Engineers widely use rotation matrix and quaternions, but the quaternions are less intuitive.&lt;/p&gt;
&lt;h2 id=&#34;related-works&#34;&gt;Related works&lt;/h2&gt;
&lt;p&gt;In the past decade, much research has been conducted on the inertial navigation techniques. These studies could roughly divided in three categories, estimation methods, Multi-Data Sensor Fusion (MSDF) techinques, and evolutionary/AI algorithms. Kalman Filter family (i.e., EKF, UKF, MEKF) and other commonly used algorithms such as Madgwick, and Mahony are based on the dynamic model of the system. Kalman filter first introduced in [], and its vairents such as EKF, UKF, and MEKF have been implemented for attitude estimation applications.&lt;/p&gt;
&lt;p&gt;In [] Carsuo et al. compared different sensor fusion algorithms for inertial attitude estimation. this comparative study showed that Sensor Fusion Algorithms (SFA) performance are highly depended to parameters tuning and fixed parameter values are not suitable for all applications. So, the parameter tuning is one the disadvantages of conventioal attitude estimation method. This problem could be tackeld by using evolutionary algorithms such as fuzzy logic and deep learning. Most of deep learning approches in inertial nvigation has focues on inertial odomotery and just few of them try to solve the inertial attitude estimation problem. Deep learning methods usually used for visual or visual-inertial based navigation. Chen et&lt;/p&gt;
&lt;p&gt;Rochefort et al., proposed a neural networks-based satellite attitude estimation algorithm by using a quaternion neural network. This study presents a new way of integrating the neural network into the state estimator and develops a training procedure which is easy to implement. This algorithm provides the same accuracy as the EKF with significantly lower computational complexity. In [Chang 2011] a Time Varying Complementary Filter (TVCF) has been proped to use fuzzy logic inference system for CF parameters adjustment for the application of attitude estimation. Chen et al. deep recurrent neural networks for estimating the displacement of a user over a specified time window. OriNet [] intrduced by Esfahani et al., to estimate the orientation in quaternion form based on LSTM layers and IMU measuremetns.
[300] developed a sensor fusion method to provide pseudo-GPS position information by using empirical mode decomposition threshold filtering (EMDTF) for IMU noise elimination and a long short-term memory (LSTM) neural network for pseudo-GPS position predication during GPS outages.&lt;/p&gt;
&lt;p&gt;Dhahbane et al. [301] developed a neural network-based complementary filter (NNCF) with ten hidden layers and trained by Bayesian Regularization Backpropagation (BRB) training algorithm to improve the generalization qualities and solve the overfitting problem. In this method output of complementary filter used as the neural network input.&lt;/p&gt;
&lt;p&gt;Li et al., proposed an adaptive Kalman filter with a fuzzy neural network for trajectory estimation system mitigating the measurement noise and the undulation for the implementation of the touch interface.
An Adaptive Unscented Kalman Filter (AUKF) method  intrduced to combine sensor fusion algorithm with deep learning to achieve high precision attitude estimation based on low cost, small size IMU in high dynamic environment.
Deep Learing has been used in [] to denoise the gyroscope measuremetns for an open-loop attitude estimation algorithm.
Weber et al. [] present a real-time-capable neural network for robust IMU-based attitude estimation. In this study, accelerometer, gyrsocope, and IMU sampling rate has been used as input to the neural network and the output is the attitude in the quaternion form. This model only suitable for estimating the roll and pitch angle. Sun et al., intrduced a two-stage deep learning framwork for inertial odometry basd on LSTM and FFNN architcutre. In this study, the first stage is used to estimate the orientation and the second stage is used to estimate the position.
A Neural Network model has been developed by Santos et al. [] for static attitude determination based on PointNet architecture. They used attitude profile matrix as input. This model uses Swish activation function and Adam as its optimizer.&lt;/p&gt;
&lt;p&gt;A deep learning model has been developed to estimate the Multirotor Unmanned Aerial Vehicle (MUAV) based on Kalman filter and Feed Forward Neural Network (FFNN) in []. LSTM framework has been used in [] the Euler angles using acceleromter, gyroscope and magnetometer but the sensor sampling rate has not been considered.&lt;/p&gt;
&lt;p&gt;In the below table, we summarized some of the related works in the field of navigation using deep learning.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Year/Month&lt;/th&gt;
&lt;th&gt;Modality&lt;/th&gt;
&lt;th&gt;Application&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;PoseNet&lt;/td&gt;
&lt;td&gt;2015/12&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Relocalization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VINet&lt;/td&gt;
&lt;td&gt;2017/02&lt;/td&gt;
&lt;td&gt;Vision +Inertial&lt;/td&gt;
&lt;td&gt;Visual Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DeepVO&lt;/td&gt;
&lt;td&gt;2017/05&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Visual Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VidLoc&lt;/td&gt;
&lt;td&gt;2017/07&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Relocalization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PoseNet+&lt;/td&gt;
&lt;td&gt;2017/07&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Relocalization&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SfmLearner&lt;/td&gt;
&lt;td&gt;2017/07&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Visual Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IONet&lt;/td&gt;
&lt;td&gt;2018/02&lt;/td&gt;
&lt;td&gt;Inertial Only&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UnDeepVO&lt;/td&gt;
&lt;td&gt;2018/05&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Visual Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VLocNet&lt;/td&gt;
&lt;td&gt;2018/05&lt;/td&gt;
&lt;td&gt;Vision&lt;/td&gt;
&lt;td&gt;Relocalization, Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RIDI&lt;/td&gt;
&lt;td&gt;2018/09&lt;/td&gt;
&lt;td&gt;Inertial Only&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SIDA&lt;/td&gt;
&lt;td&gt;2019/01&lt;/td&gt;
&lt;td&gt;Inertial Only&lt;/td&gt;
&lt;td&gt;Domain Adaptation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VIOLearner&lt;/td&gt;
&lt;td&gt;2019/04&lt;/td&gt;
&lt;td&gt;Vision + Inertial&lt;/td&gt;
&lt;td&gt;Visual Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brossard et al.&lt;/td&gt;
&lt;td&gt;2019/05&lt;/td&gt;
&lt;td&gt;Inertial Only&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SelectFusion&lt;/td&gt;
&lt;td&gt;2019/06&lt;/td&gt;
&lt;td&gt;Vision + Inertial + LIDAR&lt;/td&gt;
&lt;td&gt;VIO andSensor Fusion&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LO-Net&lt;/td&gt;
&lt;td&gt;2019/06&lt;/td&gt;
&lt;td&gt;LIDAR&lt;/td&gt;
&lt;td&gt;LIDAR Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;L3-Net&lt;/td&gt;
&lt;td&gt;2019/06&lt;/td&gt;
&lt;td&gt;LIDAR&lt;/td&gt;
&lt;td&gt;LIDAR Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Lima et al.&lt;/td&gt;
&lt;td&gt;2019/8&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DeepVIO&lt;/td&gt;
&lt;td&gt;2019/11&lt;/td&gt;
&lt;td&gt;Vision+Inertial&lt;/td&gt;
&lt;td&gt;Visual Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OriNet&lt;/td&gt;
&lt;td&gt;2020/4&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GALNet&lt;/td&gt;
&lt;td&gt;2020/5&lt;/td&gt;
&lt;td&gt;Inertial, Dynamic and Kinematic&lt;/td&gt;
&lt;td&gt;Autonomous Cars&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PDRNet&lt;/td&gt;
&lt;td&gt;2021/3&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Pedestrian Dead Reckoning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kim et al.&lt;/td&gt;
&lt;td&gt;2021/4&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Inertial Odometry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RIANN&lt;/td&gt;
&lt;td&gt;2021/5&lt;/td&gt;
&lt;td&gt;Inertial&lt;/td&gt;
&lt;td&gt;Attitude Estimation&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;problem-definition&#34;&gt;Problem definition&lt;/h2&gt;
&lt;p&gt;This study addressed the real time inertial attitude estimation based on gyroscope and accelerometer measuerments. The IMU sensor considered to rigidly attached to the object of interest. The estimaation is based on the current and pervious measurements of gyroscope and accelerometer which is used to fed into a Neural Network model to estimate the attitude. Despite almost all pervious studies, we do not consider any initial reset period for filter convergence. To aviod any singularites and have the least number of redundant parameters, we use quanternion representation with the componnets $[w, x, y, z]$ instead of Direction Cosine Matrix (DCM) or Euler angles. The error between the estimated attitude and the true attitude is calculated by quaternion multiplicative error and using the following equation:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\mathbf{q}_{err} = \mathbf{q}_{true} \otimes \mathbf{q}_{est}^{-1} 
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;where $\mathbf{q}_{err}$ represnet the shortest rotation between true and estimated orientation. The quaternion multiplication operator is calculated by the following equation:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\mathbf{q} \otimes \mathbf{p} = \begin{bmatrix}
q_0p_0 - q_1p_1 - q_2p_2 - q_3p_3 \\
q_0p_1 + q_1p_0 + q_2p_3 - q_3p_2 \\
q_0p_2 - q_1p_3 + q_2p_0 + q_3p_1 \\
q_0p_3 + q_1p_2 - q_2p_1 + q_3p_0
\end{bmatrix}
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;where $\mathbf{q}$ and $\mathbf{p}$ are the quanternions to be multiplied. The angle between the true and estimated orientation is calculated by the following equation:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{gathered}
\theta = 2 \arccos( scalar( \mathbf{q}_{err}) )
\end{gathered}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $\theta$ is the angle between the true and estimated orientation.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;h3 id=&#34;attitude&#34;&gt;Attitude&lt;/h3&gt;
&lt;p&gt;Attitude is the mathematical representation of the orientation in space related to the reference frames. Attitude parameters (attitude coordinates) refer to sets of parameters (coordinates) that fully describe a rigid body&amp;rsquo;s attitude, which are not unique expressions. There are many ways to represent the attitude of a rigid body. The most common are the Euler angles, the rotation matrix, and the quaternions. The Euler angles are the most familiar form and known as yaw, pitch, and roll (or heading, elevation, and bank). Engineers widely use rotation matrix and quaternions, but the quaternions are less intuitive. The Euler angles are defined as the rotations about the three orthogonal axes of the body frame. But, the Euler angles suffer from the problem of gimbal lock. The rotation matrix is a 3x3 matrix that represents the orientation of the body frame with respect to the inertial frame which leads to have 6 redundant parameters. The quaternions are a 4x1 vector which are more suitable for attitude estimation because they are not subject to the gimbal lock problem and have the least redundant parameters. The quaternions are defined as the following:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{gathered}
\mathbf{q} =
\begin{bmatrix}
q_0 \
q_1 \
q_2 \
q_3
\end{bmatrix}
\end{gathered}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $q_0$ is the scalar part and $q_1$, $q_2$, and $q_3$ are the vector part. And the following equation shows the relationship between the quaternions and the euler angles:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\mathbf{q} =
\begin{bmatrix}
\cos(\phi/2) \cos(\theta/2) \cos(\psi/2) + \sin(\phi/2) \sin(\theta/2) \sin(\psi/2) \\
\sin(\phi/2) \cos(\theta/2) \cos(\psi/2) - \cos(\phi/2) \sin(\theta/2) \sin(\psi/2) \\
\cos(\phi/2) \sin(\theta/2) \cos(\psi/2) + \sin(\phi/2) \cos(\theta/2) \sin(\psi/2) \\
\cos(\phi/2) \cos(\theta/2) \sin(\psi/2) - \sin(\phi/2) \sin(\theta/2) \cos(\psi/2)
\end{bmatrix}
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;where $\phi$, $\theta$, and $\psi$ are the Euler angles.&lt;/p&gt;
&lt;p&gt;Attitude determination and control play a vital role in Aerospace engineering. Most aerial or space vehicles have subsystem(s) that must be pointed to a specific direction, known as pointing modes, e.g., Sun pointing, Earth pointing. For example, communications satellites, keeping satellites antenna pointed to the Earth continuously, is the key to the successful mission. That will be achieved only if we have proper knowledge of the vehicle’s orientation; in other words, the attitude must be determined. Attitude determination methods can be divided in two categories: static and dynamic.&lt;/p&gt;
&lt;p&gt;Static attitude determination is a point-to-point time independent attitude determining method with the memoryless approach is called attitude determination. It is the observations or measurements processing to obtain the information for describing the object&amp;rsquo;s orientation relative to a reference frame. It could be determined by measuring the directions from the vehicle to the known points, i.e., Attitude Knowledge. Due to accuracy limit, measurement noise, model error, and process error, most deterministic approaches are inefficient for accurate prospects; in this situation, using statistical methods will be a good solution&lt;/p&gt;
&lt;p&gt;Dynamic attitude determination methods also known as Attitude estimation refers to using mathematical methods and techniques (e.g., statistical and probabilistic) to predict and estimate the future attitude based on a dynamic model and prior measurements. These techniques fuse data that retain a series of measurements using algorithms such as filtering, Multi-Sensor-Data-Fusion. The most commonly use attitude estimation methods are Extended Kalman Filter, Madgwick, and Mahony.&lt;/p&gt;
&lt;h3 id=&#34;attitude-determination-from-inertial-sensors&#34;&gt;Attitude Determination from Inertial Sensors&lt;/h3&gt;
&lt;p&gt;Attitude could be measured based on accelerometer and gyroscope readings. Gyroscope meaesures the angular velocity in body frame about the three orthogonal axes (i.e., x,y,z) usually denotd by $p$, $q$, and $r$ and relays on the principle of the angular momentum conservation. The gyroscope output, body rates with respect to the inertial frame which expressed in body frame is:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{gathered}
\mathbf{\omega} =
\begin{bmatrix}
\omega_x \
\omega_y \
\omega_z
\end{bmatrix}
\end{gathered}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $\omega_x$, $\omega_y$, and $\omega_z$ are the angular velocity about the x, y, and z axes, respectively. The accelerometer measures the linear acceleration in body frame about the three orthogonal axes (i.e., x,y,z) usually denotd by $a_x$, $a_y$, and $a_z$ and relays on the principle of Newton&amp;rsquo;s second law. The accelerometer output, linear acceleration with respect to the inertial frame which expressed in body frame is:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{gathered}
\mathbf{a} =
\begin{bmatrix}
a_x \
a_y \
a_z
\end{bmatrix}
\end{gathered}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Attitude can be determined from the accelerometer and gyroscope readings using the following equations:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{gathered}
\phi = \arctan\left(\frac{a_y}{a_z}\right) \
\theta = \arctan\left(\frac{-a_x}{\sqrt{a_y^2 + a_z^2}}\right) \
\end{gathered}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;Attitude update using gyroscope readings:&lt;/p&gt;
&lt;div&gt;
$$
\begin{equation}
\begin{gathered}
\dot{\phi} = p + q \sin(\phi) \tan(\theta) + r \cos(\phi) \tan(\theta) \\
\dot{\theta} = q \cos(\phi) - r \sin(\phi) \\
\dot{\psi} = \frac{q \sin(\phi)}{\cos(\theta)} + \frac{r \cos(\phi)}{\cos(\theta)} \\
\end{gathered}
\end{equation}
$$
&lt;/div&gt;
&lt;p&gt;where $\phi$, $\theta$, and $\psi$ are the Euler angles. Or in the quaternion form:&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{gathered}
\mathbf{\dot{q}} = \frac{1}{2} \mathbf{q} \otimes \mathbf{\omega}
\end{gathered}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;where $\mathbf{\dot{q}}$ is the quaternion derivative, $\mathbf{q}$ is the quaternion, and $\mathbf{\omega}$ is the angular velocity. It is necessary to mention that heading angle $\psi$ is not determined from the accelerometer, and gyroscope readings only can be used to measure the rate of change of the heading angle.&lt;/p&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;h3 id=&#34;deep-learning-model&#34;&gt;Deep Learning Model&lt;/h3&gt;
&lt;h3 id=&#34;loss-function&#34;&gt;Loss Function&lt;/h3&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment&lt;/h2&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Attitude Representation - Other</title>
      <link>https://armanasq.github.io/attitude-representations-others/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://armanasq.github.io/attitude-representations-others/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://armanasq.github.io/quaternion/&#34;&gt;Quaternion&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gibbs-vector--rodrigues-parameter-representation&#34;&gt;Gibbs Vector / Rodrigues Parameter Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modified-rodrigues-parameters&#34;&gt;Modified Rodrigues Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cayley-klein&#34;&gt;Cayley-Klein&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gibbs-vector--rodrigues-parameter-representation&#34;&gt;Gibbs Vector / Rodrigues Parameter Representation&lt;/h2&gt;
&lt;p&gt;The Gibbs vector also known as Rodrigues Parameter is a set of three parameters denoted by $ g $ (or $ P $) and can be directly derived from axis-angle $ (e, \theta) $ or quaternion representation as follows:&lt;/p&gt;
&lt;div&gt;
$$ \mathbf{g} = \frac{\mathbf{q}_{v}}{q_0} $$
&lt;/div&gt;
&lt;div&gt;
$$ \mathbf{g} = \frac{e \sin\frac{\theta}{2}}{\cos\frac{\theta}{2}} $$
&lt;/div&gt;
&lt;p&gt;where $ \mathbf{q}_{v} $ is the vector part of the quaternion, $ e $ is the unit vector of the axis of rotation, and $ \theta $ is the angle of rotation.&lt;/p&gt;
&lt;p&gt;The Gibbs vector is a unit vector that represents the axis of rotation and the magnitude of the vector represents the angle of rotation. The Gibbs vector can be used to represent the rotation matrix $C_{\psi\theta\phi}$ as:&lt;/p&gt;
&lt;div&gt;
$$ C_{\psi\theta\phi} = \begin{bmatrix} 1 - 2(g_2^2 + g_3^2) &amp; 2(g_1g_2 - g_3) &amp; 2(g_1g_3 + g_2) \\ 2(g_1g_2 + g_3) &amp; 1 - 2(g_1^2 + g_3^2) &amp; 2(g_2g_3 - g_1) \\ 2(g_1g_3 - g_2) &amp; 2(g_2g_3 + g_1) &amp; 1 - 2(g_1^2 + g_2^2) \end{bmatrix} $$
&lt;/div&gt;
&lt;p&gt;where $ g_1 $, $ g_2 $, and $ g_3 $ are the components of the Gibbs vector.&lt;/p&gt;
&lt;p&gt;The Gibbs vector components expressed in DCM can be calculated using the following:&lt;/p&gt;
&lt;div&gt;
$$ g_1 = \frac{R_{23}-R_{32}}{1+R_{11}+R_{22}+R_{33}} $$
&lt;/div&gt;
&lt;div&gt;
$$ g_2 = \frac{R_{31}-R_{13}}{1+R_{11}+R_{22}+R_{33}} $$
&lt;/div&gt;&lt;div&gt;
$$ g_3 = \frac{R_{12}-R_{21}}{1+R_{11}+R_{22}+R_{33}} $$
&lt;/div&gt;
where $ R_{ij} $ is the element of the rotation matrix.
&lt;p&gt;The Rodrigues Parameter preferred as an attitude error representation because it is a unit vector and it is easy to calculate the attitude error between two quaternions. The attitude error between two quaternions can be calculated using the following:&lt;/p&gt;
&lt;div&gt;
$$ \mathbf{g}_{error} = \frac{2\mathbf{q}_{v}}{q_0} $$
&lt;/div&gt;
The Gibbs vector components expereince a singularity at $ \theta = \pi $, which is the same as the Euler angles. The Gibbs vector is not a good representation for small rotations.
&lt;h2 id=&#34;modified-rodrigues-parameters&#34;&gt;Modified Rodrigues Parameters&lt;/h2&gt;
&lt;p&gt;In attitude filter design Modified Rodrigues Parameters (MRP) is preferred for attitude error representation. The MRP is a set of three parameters denoted by $ \mathbf{m} $ and can be directly derived from axis-angle $ (e, \theta) $ or quaternion representation as follows:&lt;/p&gt;
&lt;div&gt;
$$ \mathbf{m} = \frac{\mathbf{q}_{v}}{1+q_0} $$
&lt;/div&gt;&lt;div&gt;
$$ \mathbf{m} = \frac{e \sin\frac{\theta}{2}}{1+\cos\frac{\theta}{2}} $$
&lt;/div&gt;
where $ \mathbf{q}_{v} $ is the vector part of the quaternion, $ e $ is the unit vector of the axis of rotation, and $ \theta $ is the angle of rotation.
&lt;p&gt;Due to above equation the maximum equivalent rotation to describe is $ \pm 360^{\circ}$ (the singularity occurs in $ \pm 360^{\circ}$).&lt;/p&gt;
&lt;h2 id=&#34;cayley-klein&#34;&gt;Cayley-Klein&lt;/h2&gt;
&lt;p&gt;The Cayley-Klein parameters are consisting of 4 parameters which are closely related to the quaternions and denoted by matrix $ \mathbf{K}_{2\times 2} $.&lt;/p&gt;
&lt;div&gt;
$$ K = \begin{bmatrix} \alpha &amp; \beta \\ \gamma &amp; \sigma \end{bmatrix} $$
&lt;/div&gt;
and satisfy the constraints
&lt;div&gt;
$$ \alpha \bar{\alpha} + \gamma \bar{\gamma} = 1 \\ \alpha \bar{\alpha} + \beta \bar{\beta} = 1 \\ \alpha \bar{\beta} + \gamma \bar{\sigma} = 0 \\ \alpha \sigma + \beta \gamma = 1 \\ \beta = -\bar{\gamma} \\ \sigma = \bar{\alpha} $$
&lt;/div&gt;
where $ \alpha $, $ \beta $, $ \gamma $, and $ \sigma $ are the Cayley-Klein parameters and $ \bar{\alpha} $, $ \bar{\beta} $, $ \bar{\gamma} $, and $ \bar{\sigma} $ are the conjugate of the Cayley-Klein parameters.
&lt;p&gt;The corresponding quaternions are defined_as:&lt;/p&gt;
&lt;div&gt;
$$ \mathbf{q}_K = \begin{bmatrix} \frac{ \alpha + \sigma }{2} \\ \frac{ -i(\beta + \gamma) }{2} \\ \frac{ \beta - \gamma }{2} \\ \frac{ -i(\alpha - \sigma) }{2} \end{bmatrix} $$
&lt;/div&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] Markley, F. Landis, and John L. Crassidis. Fundamentals of spacecraft attitude determination and control. Vol. 1286. New York, NY, USA:: Springer New York, 2014. &lt;br&gt;
[2] Junkins, John L., and Hanspeter Schaub. Analytical mechanics of space systems. American Institute of Aeronautics and Astronautics, 2009. &lt;br&gt;
[3] De Ruiter, Anton H., Christopher Damaren, and James R. Forbes. Spacecraft dynamics and control: an introduction. John Wiley &amp;amp; Sons, 2012. &lt;br&gt;
[4] Wertz, James R., ed. Spacecraft attitude determination and control. Vol. 73. Springer Science &amp;amp; Business Media, 2012. &lt;br&gt;
[5] Vepa, Ranjan. Dynamics and Control of Autonomous Space Vehicles and Robotics. Cambridge University Press, 2019. &lt;br&gt;
[6] Shuster, Malcolm D. &amp;ldquo;A survey of attitude representations.&amp;rdquo; Navigation 8.9 (1993): 439-517. &lt;br&gt;
[7] Markley, F. Landis. &amp;ldquo;Attitude error representations for Kalman filtering.&amp;rdquo; Journal of guidance, control, and dynamics 26.2 (2003): 311-317. &lt;br&gt;
[8] Markley, F. Landis, and Frank H. Bauer. Attitude representations for Kalman filtering. No. AAS-01-309. 2001. &lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attitude Representation - Quaternions</title>
      <link>https://armanasq.github.io/quaternion/</link>
      <pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://armanasq.github.io/quaternion/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://armanasq.github.io/euler-angles/&#34;&gt;Euler Angles&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#euler-parameters-quaternions-representation&#34;&gt;Euler Parameters (Quaternions) representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;euler-parameters-quaternions-representation&#34;&gt;Euler Parameters (Quaternions) representation&lt;/h2&gt;
&lt;p&gt;A four-element vector with three imaginary and one real component is known as Quaternion. These hypercomplex numbers are optimum for numerical stability and memory load. The Euler parameters are a four-dimensional vector that can be used to represent the orientation of a rigid body. The Euler parameters are defined as:&lt;/p&gt;
&lt;div&gt;
$$ q = \begin{bmatrix} q_0 \\ q_1 \\ q_2 \\ q_3 \end{bmatrix} $$
&lt;/div&gt;
&lt;p&gt;where $q_0$ is the scalar part and $q_1$, $q_2$, and $q_3$ are the vector part. It could be written as:&lt;/p&gt;
&lt;div&gt;
$$ q = q_0 + q_1i + q_2j + q_3k $$
&lt;/div&gt;
&lt;p&gt;where $i$, $j$, and $k$ are the imaginary unit vectors and&lt;/p&gt;
&lt;div&gt;
$$ i^2 = j^2 = k^2 = ijk = -1 $$
&lt;/div&gt;
&lt;div&gt;
$$ \mathbb{q} = (q_0 , \mathbf{q}_v) $$
&lt;/div&gt;
&lt;p&gt;where $ \mathbf{q}_v = (q_1 , q_2 , q_3) $ is the vector part of the quaternion. The quaternion is a unit quaternion if $ \mathbb{q} \cdot \mathbb{q}^* = 1 $, where $ \mathbb{q}^* $ is the conjugate of $ \mathbb{q} $.&lt;/p&gt;
&lt;p&gt;It is noticeable that some authors may use left-handed quaternions witch is defined by:&lt;/p&gt;
&lt;div&gt;
$$ \mathbf{q} = iq_1 + jq_2 + kq_3 + q_0 \\ ijk = 1 $$
&lt;/div&gt;
This representation has no fundamental implications but will change the details of formulation.
&lt;p&gt;Quaternions do not have any singularity such as Euler angles. However, due to the lack of independence of components, it may present difficulties in the application of the filter equations. The quaternion is not unique, and the mirror quaternion will result in the same rotation. This is a purely mathematical representation and based upon single rotation theta around vector e with angle. It could not be used for visualization.&lt;/p&gt;
&lt;p&gt;Quaternion also, can be used to describe the axis-angle representation by:&lt;/p&gt;
&lt;div&gt;
$$ \mathbf{q} = \begin{bmatrix}q_w \\ q_x \\ q_y \\ q_z\end{bmatrix} = \begin{bmatrix} \cos\frac{\theta}{2} \\ v_x \sin\frac{\theta}{2} \\ v_y \sin\frac{\theta}{2} \\ v_z \sin\frac{\theta}{2} \end{bmatrix} = \begin{bmatrix} \cos\frac{\theta}{2} \\ \mathbf{v} \sin\frac {\theta}{2} \end{bmatrix} $$
&lt;/div&gt;
&lt;p&gt;Also, the quaternion can be expressed in $4 \times 4$ skew-symmetric matrix form&lt;/p&gt;
&lt;div&gt;
$$ Q = \begin{bmatrix} q_0 &amp; -q_1 &amp; -q_2 &amp; -q_3 \\ q_1 &amp; q_0 &amp; -q_3 &amp; q_2 \\ q_2 &amp; q_3 &amp; q_0 &amp; -q_1 \\ q_3 &amp; -q_2 &amp; q_1 &amp; q_0 \end{bmatrix} $$
&lt;/div&gt;
&lt;p&gt;The quaternion represents the attitude of frame $A$ relative to frame $B$ defined by the following equation:&lt;/p&gt;
&lt;div&gt;
$$ {}^{A}_{B}\mathbf{q}={}^{B}_{A}\mathbf{q}^* $$
&lt;/div&gt;
&lt;p&gt;where $ {}^{A}_{B}\mathbf{q} $  is the quaternion that represents the attitude of frame $ A $ relative to frame $ B $.&lt;/p&gt;
&lt;p&gt;$ {}^{B}_{A}\mathbf{q}^* $ is the conjugate of the quaternion that represents the attitude of frame $ A $ relative to frame $ B $. The $ \mathbf{q}^* $ (conjugate of the quaternion $ \mathbf{q} $) gives the inverse rotation.&lt;/p&gt;
&lt;p&gt;The relationship between quaternions and Euler angles based on $zyx$ sequence can be calculated using the following:&lt;/p&gt;
&lt;div&gt;
$$ \mathbf{q} = \begin{bmatrix} \cos\frac{\theta_x}{2} \cos\frac{\theta_y}{2} \cos\frac{\theta_z}{2} + \sin\frac{\theta_x}{2} \sin\frac{\theta_y}{2} \sin\frac{\theta_z}{2} \\ \sin\frac{\theta_x}{2} \cos\frac{\theta_y}{2} \cos\frac{\theta_z}{2} - \cos\frac{\theta_x}{2} \sin\frac{\theta_y}{2} \sin\frac{\theta_z}{2} \\ \cos\frac{\theta_x}{2} \sin\frac{\theta_y}{2} \cos\frac{\theta_z}{2} + \sin\frac{\theta_x}{2} \cos\frac{\theta_y}{2} \sin\frac{\theta_z}{2} \\ \cos\frac{\theta_x}{2} \cos\frac{\theta_y}{2} \sin\frac{\theta_z}{2} - \sin\frac{\theta_x}{2} \sin\frac{\theta_y}{2} \cos\frac{\theta_z}{2} \end{bmatrix} $$
&lt;/div&gt;
&lt;p&gt;where $ \theta_x $, $ \theta_y $, and $ \theta_z $ are the Euler angles.&lt;/p&gt;
&lt;p&gt;Also, the Euler angles can be calculated using the following:&lt;/p&gt;
&lt;div&gt;
$$ \phi = \arctan\left(\frac{2(q_0q_1 + q_2q_3)}{1 - 2(q_1^2 + q_2^2)}\right) $$
&lt;/div&gt;
&lt;div&gt;
$$ \theta = \arcsin\left(2(q_0q_2 - q_3q_1)\right) $$
&lt;/div&gt;
&lt;div&gt;
$$ \psi = \arctan\left(\frac{2(q_0q_3 + q_1q_2)}{1 - 2(q_2^2 + q_3^2)}\right) $$
&lt;/div&gt;
Since there are 12 different Euler angles sets, there are 12 quaternion to Euler angles conversion equation.
&lt;p&gt;The quaternion can be used to represent the rotation matrix $C_{\psi\theta\phi}$ as:&lt;/p&gt;
&lt;div&gt;
$$ C_{\psi\theta\phi} = \begin{bmatrix} q_0^2 + q_1^2 - q_2^2 - q_3^2 &amp; 2(q_1q_2 - q_0q_3) &amp; 2(q_1q_3 + q_0q_2) \\ 2(q_1q_2 + q_0q_3) &amp; q_0^2 - q_1^2 + q_2^2 - q_3^2 &amp; 2(q_2q_3 - q_0q_1) \\ 2(q_1q_3 - q_0q_2) &amp; 2(q_2q_3 + q_0q_1) &amp; q_0^2 - q_1^2 - q_2^2 + q_3^2 \end{bmatrix} $$
&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://armanasq.github.io/attitude-representations-others/&#34;&gt;Other Attitude Representations&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] Markley, F. Landis, and John L. Crassidis. Fundamentals of spacecraft attitude determination and control. Vol. 1286. New York, NY, USA:: Springer New York, 2014. &lt;br&gt;
[2] Junkins, John L., and Hanspeter Schaub. Analytical mechanics of space systems. American Institute of Aeronautics and Astronautics, 2009. &lt;br&gt;
[3] De Ruiter, Anton H., Christopher Damaren, and James R. Forbes. Spacecraft dynamics and control: an introduction. John Wiley &amp;amp; Sons, 2012. &lt;br&gt;
[4] Wertz, James R., ed. Spacecraft attitude determination and control. Vol. 73. Springer Science &amp;amp; Business Media, 2012. &lt;br&gt;
[5] Vepa, Ranjan. Dynamics and Control of Autonomous Space Vehicles and Robotics. Cambridge University Press, 2019. &lt;br&gt;
[6] Shuster, Malcolm D. &amp;ldquo;A survey of attitude representations.&amp;rdquo; Navigation 8.9 (1993): 439-517. &lt;br&gt;
[7] Markley, F. Landis. &amp;ldquo;Attitude error representations for Kalman filtering.&amp;rdquo; Journal of guidance, control, and dynamics 26.2 (2003): 311-317. &lt;br&gt;
[8] Markley, F. Landis, and Frank H. Bauer. Attitude representations for Kalman filtering. No. AAS-01-309. 2001. &lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attitude Representation - Euler Angles</title>
      <link>https://armanasq.github.io/euler-angles/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://armanasq.github.io/euler-angles/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://armanasq.github.io/attitude-representation/&#34;&gt;Attitude Representation&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#euler-angles-representation&#34;&gt;Euler Angles Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;euler-angles-representation&#34;&gt;Euler Angles Representation&lt;/h2&gt;
&lt;p&gt;A vector of three angles that represent the attitude of the coordinate frame $ i $ with respect to the coordinate frame $ j $ is called Euler angles. Euler angles are the most commonly used attitude representation because it&amp;rsquo;s easy to use and understand. One of Euler angles&amp;rsquo; obvious advantages is their intuitive representation.&lt;/p&gt;
&lt;div&gt;
$$ \text{Euler angles} = \begin{bmatrix} \phi \\ \theta \\ \psi \end{bmatrix} $$
&lt;/div&gt;
&lt;p&gt;where $\phi$, $\theta$, and $\psi$ are the rotation angles about the $x$, $y$, and $z$ axes, respectively. The Euler angles are defined as follows:&lt;/p&gt;
&lt;div&gt;
$$  \phi = \arctan\left(\frac{R_{32}}{R_{33}}\right) \\ \theta = \arcsin\left(-R_{31}\right) \\ \psi = \arctan\left(\frac{R_{21}}{R_{11}}\right)  $$
&lt;/div&gt;
where $R_{ij}$ is the element of the rotation matrix $R$.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Roll&lt;/strong&gt;: Rotation around the x-axis with angle $ \phi $&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;
$$ C_{\phi} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; cos(\phi) &amp; sin(\phi) \\ 0 &amp; -sin(\phi) &amp; cos(\phi) \end{bmatrix} $$
&lt;/div&gt;
* **Pitch**: Rotation around the y-axis with angle $ \theta $
&lt;div&gt;
$$ C_{\theta} = \begin{bmatrix} cos(\theta) &amp; 0 &amp; -sin(\theta) \\ 0 &amp; 1 &amp; 0 \\ sin(\theta) &amp; 0 &amp; cos(\theta) \end{bmatrix} $$
&lt;/div&gt;
* **Yaw**: Rotation around the z-axis with angle $ \psi $
&lt;div&gt;
$$ C_{\psi} = \begin{bmatrix} cos(\psi) &amp; sin(\psi) &amp; 0 \\ -sin(\psi) &amp; cos(\psi) &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} $$
&lt;/div&gt;
Euler angles represent three consecutive rotations, and they could be defined in twelve different orders. The most common order is the yaw-pitch-roll (YPR) order, which is also called the z-y-x order. The rotation matrix can be written as:
&lt;div&gt;
$$ C_{\psi\theta\phi} = C_{\psi}C_{\theta}C_{\phi} $$
&lt;/div&gt;
&lt;div&gt;
$$ C_{\psi\theta\phi} = \begin{bmatrix} cos(psi)cos(\theta) &amp; cos(psi)sin(\theta)sin(\phi)-sin(psi)cos(\phi) &amp; cos(psi)sin(\theta)cos(\phi)+sin(psi)sin(\phi) \\ sin(psi)cos(\theta) &amp; sin(psi)sin(\theta)sin(\phi)+cos(psi)cos(\phi) &amp; sin(psi)sin(\theta)cos(\phi)-cos(psi)sin(\phi) \\ -sin(\theta) &amp; cos(\theta)sin(\phi) &amp; cos(\theta)cos(\phi) \end{bmatrix} $$
&lt;/div&gt;
The Euler angles of the rotation matrix $C_{\phi\theta\psi}$ can be written as:
&lt;div&gt;
$$ \phi = \arctan\left(\frac{C_{32}}{C_{33}}\right) $$
&lt;/div&gt;&lt;div&gt;
$$ \theta = \arctan\left(\frac{C_{32}}{\sqrt{1-C_{32}^2}}\right) $$
&lt;/div&gt;&lt;div&gt;
$$ \psi = \arctan\left(\frac{C_{31}}{C_{33}}\right) $$
&lt;div&gt;
The Euler angles are not unique. For example, the Euler angles $ (0,0,0) $ and $ (2\pi,2\pi,2\pi) $ represent the same rotation. The Euler angles are also not invariant to the order of the rotations. For example, the Euler angles $ R_{x,y,z}(0,0,0) $ and $ R_{z,y,x}(0,0,0) $ represent the same rotation, but the rotation matrix is different.
&lt;p&gt;Three rotation angles $\phi$, $\theta$, and $\psi$ are about the sequential displaced body-fixed axes, and twelve different sequences are possible that can be used for the same rotation. The location of each sequential rotation depends on the preceding rotation, and there are divided into two main categories:&lt;/p&gt;
&lt;ol&gt; 
  &lt;li&gt; &lt;b&gt;Symmetric sequences&lt;/b&gt;: The first and third rotations are performed around the same axis, second rotation is performed around one of the two others:&lt;/li&gt;
$$ R_{i,j,i}(\alpha, \beta, \gamma) = R_i(\alpha)R_j(\beta)R_i(\gamma) $$
&lt;center&gt; Symmetric sequence $ (i,j,i)$, $ i \ne j$, $ \alpha, \beta, \gamma \in \mathbb{R}$ &lt;/center&gt;
  &lt;li&gt; &lt;b&gt;Asymmetric sequences&lt;/b&gt;: All rotations performed around three different axes: &lt;/li&gt;
$$ R_{i,j,k}(\alpha, \beta, \gamma) = R_i(\alpha)R_j(\beta)R_k(\gamma) $$
&lt;center&gt; Asymmetric sequence $ (i,j,k)$, $ i \ne j \ne k \ne i$, $ \alpha, \beta, \gamma \in \mathbb{R}$ &lt;/center&gt;
&lt;/ol&gt;
&lt;p&gt;These angles are not unique, and the mirror angles will result in the same rotations.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt; For &lt;b&gt;&lt;i&gt;Symmetric sequences&lt;/i&gt;&lt;/b&gt;: $ R(\alpha, \beta, \gamma) = R(\alpha + \pi, -\beta,\gamma - \pi) $ &lt;/li&gt;
  &lt;li&gt; For &lt;b&gt;&lt;i&gt;Asymmetric sequences&lt;/i&gt;&lt;/b&gt;: $ R(\alpha, \beta, \gamma) = R(\alpha + \pi, \pi -\beta,\gamma - \pi) $ &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The main disadvantages of Euler angles are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Singularity
&lt;/li&gt;
&lt;li&gt;Non-uniqueness &lt;/li&gt;
&lt;li&gt;Non-invariance &lt;/li&gt;
&lt;li&gt; Less accuracy for integration of attitude incremental changes over time&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At $ \theta = \left(\pm\frac{\pi}{2}\right) $ the singularities will occur and usually known as mathematical gimble lock where to axes are parallel to each other.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://armanasq.github.io/quaternion/&#34;&gt;Quaternion&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] Markley, F. Landis, and John L. Crassidis. Fundamentals of spacecraft attitude determination and control. Vol. 1286. New York, NY, USA:: Springer New York, 2014. &lt;br&gt;
[2] Junkins, John L., and Hanspeter Schaub. Analytical mechanics of space systems. American Institute of Aeronautics and Astronautics, 2009. &lt;br&gt;
[3] De Ruiter, Anton H., Christopher Damaren, and James R. Forbes. Spacecraft dynamics and control: an introduction. John Wiley &amp;amp; Sons, 2012. &lt;br&gt;
[4] Wertz, James R., ed. Spacecraft attitude determination and control. Vol. 73. Springer Science &amp;amp; Business Media, 2012. &lt;br&gt;
[5] Vepa, Ranjan. Dynamics and Control of Autonomous Space Vehicles and Robotics. Cambridge University Press, 2019. &lt;br&gt;
[6] Shuster, Malcolm D. &amp;ldquo;A survey of attitude representations.&amp;rdquo; Navigation 8.9 (1993): 439-517. &lt;br&gt;
[7] Markley, F. Landis. &amp;ldquo;Attitude error representations for Kalman filtering.&amp;rdquo; Journal of guidance, control, and dynamics 26.2 (2003): 311-317. &lt;br&gt;
[8] Markley, F. Landis, and Frank H. Bauer. Attitude representations for Kalman filtering. No. AAS-01-309. 2001. &lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attitude Representation</title>
      <link>https://armanasq.github.io/attitude-representation/</link>
      <pubDate>Mon, 17 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://armanasq.github.io/attitude-representation/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#direction-cosine-matrix-dcm&#34;&gt;Direction Cosine Matrix (DCM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#axis-angle-representation&#34;&gt;Axis-Angle Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://armanasq.github.io/attitude/&#34;&gt;Attitude&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Attitude representation is a set of coordinates that fully describe a rigid body’s orientation with respect to a reference frame. There are an infinite number of attitude representations, each of which has strengths and weaknesses. Choosing the proper attitude representation depends on the estimation algorithm, type of the moving object (e.g. satellite, spacecraft), type of mission, and reference frame selection. Attitude representation impacts mathematical complexity, geometrical singularities, and operational range, so it&amp;rsquo;s crucial to choose the proper representation for the objectives. At least, three coordinates are needed to describe the attitude in a 3D space that has at least one singularity. Singularities can be avoided by using four or more coordinates, but even the use of four coordinates does not guarantee their avoidance.&lt;/p&gt;
&lt;p&gt;There are various attitude representations that are common in the industry, such as Direction Cosine Matrix, Euler angles, Euler Parameters (Quaternions), Gibb&amp;rsquo;s vectors, and so on. We will describe a few of them below.&lt;/p&gt;
&lt;p&gt;To maintain consistency in mathematical notations, two reference frames (as a reference frame) and (as a body frame) have been defined as follows:&lt;/p&gt;
&lt;div&gt;
$$ 
N \equiv  \begin{bmatrix} n_1 \\ n_2 \\ n_3 \end{bmatrix},   B \equiv  \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}  
$$
&lt;/div&gt;
&lt;h2 id=&#34;direction-cosine-matrix-dcm&#34;&gt;Direction Cosine Matrix (DCM)&lt;/h2&gt;
&lt;p&gt;In mathematics, a direction cosine matrix (DCM) is a matrix that transforms coordinate reference frames. Attitude Matrix, also known as DCM, is the most fundamental and redundant method of describing relative attitudes.&lt;/p&gt;
&lt;div&gt;
$$
\mathbf{R} =
\begin{bmatrix}
r_{11} &amp; r_{12} &amp; r_{13} \\
r_{21} &amp; r_{22} &amp; r_{23} \\
r_{31} &amp; r_{32} &amp; r_{33}
\end{bmatrix} \in \mathbb{R}^{3\times 3}
$$
&lt;/div&gt;
&lt;p&gt;There are nine parameters, of which six are redundant due to orthogonality. The DCM elements can be described as the dot product of coordinate system axes, which express the base vector as follows:&lt;/p&gt;
&lt;div&gt;
$$ DCM = \begin{bmatrix} b_1 \cdot n_1 &amp; b_1 \cdot n_2 &amp; b_1 \cdot n_3 \\ b_2 \cdot n_1 &amp; b_2 \cdot n_2 &amp; b_2 \cdot n_3 \\ b_3 \cdot n_1 &amp; b_3 \cdot n_2 &amp; b_3 \cdot n_3\end{bmatrix} $$
&lt;/div&gt;
In the other hand, the cosine of three angles between each body vector $ b_i, (i=1,2,3) $ and three axes $ n_i, (i=1,2,3) $ are called the direction cosine matrix.
&lt;div&gt;
$$ b_i = cos(\alpha_{i1}\mathbf{n}_1) + cos(\alpha_{i2}\mathbf{n}_2) + cos(\alpha_{i3}\mathbf{n}_3) \\ i=1,2,3 $$
&lt;/div&gt;
&lt;p&gt;So, the direction cosine matrix can be rewritten by:&lt;/p&gt;
&lt;div&gt;
$$ DCM = \begin{bmatrix} cos(\alpha_{11}) &amp; cos(\alpha_{12}) &amp; cos(\alpha_{13}) \\ cos(\alpha_{21}) &amp; cos(\alpha_{22}) &amp; cos(\alpha_{23}) \\ cos(\alpha_{31}) &amp; cos(\alpha_{32}) &amp; cos(\alpha_{33})  \end{bmatrix} $$
&lt;/div&gt;
&lt;p&gt;So,&lt;/p&gt;
&lt;div&gt;
$$ \hat{\mathbf{b}} = \text{DCM} \hat{\mathbf{n}} $$
&lt;/div&gt;
&lt;p&gt;where $\hat{\mathbf{b}}$ and $\hat{\mathbf{n}}$ are the unit vectors of the body and reference frames, respectively.&lt;/p&gt;
&lt;h2 id=&#34;axis-angle-representation&#34;&gt;Axis-Angle Representation&lt;/h2&gt;
&lt;p&gt;Euler’s theorem states that all rotations of a solid object can be expressed as single rotation $ \theta $ about a unit length axis $ e $ in the rotation plane. In other words, each orthogonal matrix $ R $ has a specified unit vector rotation axis donated $ e $, known as Euler axis, and a single rotation angle $ \theta $ is called Euler angle. The axis angle representation can be written as:&lt;/p&gt;
&lt;p&gt;$$ \theta \mathbf{e}= \begin{bmatrix} \theta e_1 \ \theta e_2 \ \theta e_3 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;$$ \mathbf{e} = \begin{bmatrix} e_1 \ e_2 \ e_3 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$ |\mathbf{e}| = 1 $$&lt;/p&gt;
&lt;p&gt;Since, $(e,\theta)$ and $(-e,-\theta)$ correspond to the same rotation, it’s not a unique representation. The axis-angle representation is not a good choice for attitude estimation because it has a singularity at $\theta = \pi$. The axis-angle representation is also not a good choice for attitude control because it is not a linear representation. The axis-angle representation is a good choice for attitude visualization. The axis-angle representation is also a good choice for attitude initialization.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://armanasq.github.io/euler-angles/&#34;&gt;Euler Angles&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] Markley, F. Landis, and John L. Crassidis. Fundamentals of spacecraft attitude determination and control. Vol. 1286. New York, NY, USA:: Springer New York, 2014. &lt;br&gt;
[2] Junkins, John L., and Hanspeter Schaub. Analytical mechanics of space systems. American Institute of Aeronautics and Astronautics, 2009. &lt;br&gt;
[3] De Ruiter, Anton H., Christopher Damaren, and James R. Forbes. Spacecraft dynamics and control: an introduction. John Wiley &amp;amp; Sons, 2012. &lt;br&gt;
[4] Wertz, James R., ed. Spacecraft attitude determination and control. Vol. 73. Springer Science &amp;amp; Business Media, 2012. &lt;br&gt;
[5] Vepa, Ranjan. Dynamics and Control of Autonomous Space Vehicles and Robotics. Cambridge University Press, 2019. &lt;br&gt;
[6] Shuster, Malcolm D. &amp;ldquo;A survey of attitude representations.&amp;rdquo; Navigation 8.9 (1993): 439-517. &lt;br&gt;
[7] Markley, F. Landis. &amp;ldquo;Attitude error representations for Kalman filtering.&amp;rdquo; Journal of guidance, control, and dynamics 26.2 (2003): 311-317. &lt;br&gt;
[8] Markley, F. Landis, and Frank H. Bauer. Attitude representations for Kalman filtering. No. AAS-01-309. 2001. &lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attitude</title>
      <link>https://armanasq.github.io/attitude/</link>
      <pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://armanasq.github.io/attitude/</guid>
      <description>&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attitude-and-attitude-terminology&#34;&gt;Attitude and Attitude Terminology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Attitude determination and control play a vital role in Aerospace engineering. Most aerial or space vehicles have subsystem(s) that must be pointed to a specific direction, known as pointing modes, e.g., Sun pointing, Earth pointing. For example, communications satellites, keeping satellites antenna pointed to the Earth continuously, is the key to the successful mission. That will be achieved only if we have proper knowledge of the vehicle’s orientation; in other words, the attitude must be determined. In this post, the fundamental concepts for defining the attitude of an object in the three-dimensional space will be presented. It is necessary to have a clear view of the exact meaning of the attitude or orientation. So, at first, the attitude and attitude terminology will be defined. Then the mathematical relationships between the attitude and the angular velocity will be presented. Finally, the attitude and angular velocity will be used to define the attitude dynamics.&lt;/p&gt;
&lt;h2 id=&#34;attitude-and-attitude-terminology&#34;&gt;Attitude and Attitude Terminology&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attitude&lt;/strong&gt; is the mathematical representation of the orientation in space related to the reference frames. Attitude parameters (attitude coordinates) refer to sets of parameters (coordinates) that fully describe a rigid body&amp;rsquo;s attitude, which is not unique expressions. At least three parameters are required to describe the orientation uniquely. The process of determining these parameters is called attitude determination. Attitude determination methods can be divided in two categories: static and dynamic.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Static Attitude Determination&lt;/strong&gt; is a point-to-point time-independent attitude-determining method with the memoryless approach, also known as attitude determination. It is the observations or measurements processing to obtain the information for describing the object&amp;rsquo;s orientation relative to a reference frame. It could be determined by measuring the directions from the vehicle to the known points, i.e., Attitude Knowledge. Due to accuracy limit, measurement noise, model error, and process error, most deterministic approaches are inefficient for accurate prospects; in this situation, using statistical methods will be a good solution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Attitude Determination&lt;/strong&gt; methods, also known as Attitude estimation, refer to using mathematical methods and techniques (e.g., statistical and probabilistic) to predict and estimate the future attitude based on a dynamic model and prior measurements. These techniques fuse data that retain a series of measurements using algorithms such as filtering, Multi-Sensor-Data-Fusion.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we consider attitude estimation as mathematical methods and attitude determination as instruments and measurements. In that case, we could find that no such works had been done in attitude estimation until the eighteenth or nineteenth century, as M.D. Shuster mentioned in [&lt;a id =&#34;id1&#34; href=&#34;#1&#34; &gt; 1&lt;/a&gt;, &lt;a id =&#34;id2&#34; href=&#34;#2&#34; &gt;2&lt;/a&gt;] attitude estimation is a young and underdeveloped field such that Sputnik 1 (the first artificial satellite) and Echo 1 (the first passive communications satellite experiment) did not have attitude determination and control system (ADCS). Also, the next generation of spacecraft has an attitude control system without any attitude estimation. Those spacecraft used passive attitude control methods such as gravity gradient attitude stabilization.&lt;/p&gt;
&lt;p&gt;At first, two frames must be defined to formulate the attitude, the body frame $B$ and the Observer frame $O$. Then we can define the attitude as the orientation of the $B$ frame with respect to the $O$ frame. Usually, the rigid body orientation is given with respect to an inertial frame called Inertial Fixed References System (IFRS). As mentioned before, attitude is a set of coordinates which defines the orientation. It could be a 3D vector which is represented by a 3D rotation matrix. The basic rotation matrix (also called elemental rotation) is a 3x3 matrix which is used to rotate the coordinate system by an angle $\theta$ about $x$, $y$, or $z$ axis and defined by the following equation:&lt;/p&gt;
&lt;div&gt;
$$ R_x = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; \cos(\theta) &amp; -\sin(\theta) \\ 0 &amp; \sin(\theta) &amp; \cos(\theta)  \end{bmatrix} $$
&lt;/div&gt;
&lt;div&gt;
$$ R_y = \begin{bmatrix} \cos(\theta) &amp; 0 &amp; \sin(\theta)  \\  0 &amp; 1 &amp; 0 \\ -\sin(\theta) &amp; 0 &amp; \cos(\theta) \end{bmatrix} $$
&lt;/div&gt;
&lt;div&gt;
$$ R_z = \begin{bmatrix} \cos(\theta) &amp; -\sin(\theta) &amp; 0 \\ \sin(\theta) &amp; \cos(\theta) &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} $$
&lt;/div&gt;
&lt;p&gt;where $\theta$ is the angle of rotation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://armanasq.github.io/attitude-representation/&#34;&gt;Attitude Representation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[&lt;a id=&#34;1&#34; href=&#34;#id1&#34; &gt; 1 &lt;/a&gt;] M. D. Shuster, &amp;ldquo;In my estimation,&amp;rdquo; The Journal of the Astronautical Sciences, 2006. &lt;br&gt;
[&lt;a id=&#34;2&#34; href=&#34;#id2&#34; &gt; 2 &lt;/a&gt;]	M. D. Shuster, &amp;ldquo;Beyond estimation,&amp;rdquo; Advances in the Astronautical Sciences, 2006. &lt;br&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
