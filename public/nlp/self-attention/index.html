<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: January 4, 2024 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  

  
  
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.d060e36f065b14306ff371728665eb02.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  

  <meta name="google-site-verification" content="Zv4l_ljWZhu4o0Z-kfZwQmuokpt40AvKXA78N8kynpc" />





<script async src="https://www.googletagmanager.com/gtag/js?id=G-55GQYC5GYC"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-55GQYC5GYC', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>




<script>
  (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','G-55GQYC5GYC');
</script>




















  
  
  






  <meta name="author" content="Arman Asgharpoor Golroudbari" />





  

<meta name="description" content="⇐ Natural Language Processing
Understanding Self-Attention - A Step-by-Step Guide Self-attention is a fundamental concept in natural language processing (NLP) and deep learning, especially prominent in transformer-based models. In this post, we will delve into the self-attention mechanism, providing a step-by-step guide from scratch." />



<link rel="alternate" hreflang="en-us" href="https://armanasq.github.io/nlp/self-attention/" />
<link rel="canonical" href="https://armanasq.github.io/nlp/self-attention/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@wowchemy" />
  <meta property="twitter:creator" content="@wowchemy" />
<meta property="twitter:image" content="https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="" />
<meta property="og:url" content="https://armanasq.github.io/nlp/self-attention/" />
<meta property="og:title" content="Understanding Self-Attention - A Step-by-Step Guide | " />
<meta property="og:description" content="⇐ Natural Language Processing
Understanding Self-Attention - A Step-by-Step Guide Self-attention is a fundamental concept in natural language processing (NLP) and deep learning, especially prominent in transformer-based models. In this post, we will delve into the self-attention mechanism, providing a step-by-step guide from scratch." /><meta property="og:image" content="https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2023-09-01T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2023-09-01T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://armanasq.github.io/nlp/self-attention/"
  },
  "headline": "Understanding Self-Attention - A Step-by-Step Guide",
  
  "datePublished": "2023-09-01T00:00:00Z",
  "dateModified": "2023-09-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Arman Asgharpoor Golroudbari"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "⇐ Natural Language Processing\nUnderstanding Self-Attention - A Step-by-Step Guide Self-attention is a fundamental concept in natural language processing (NLP) and deep learning, especially prominent in transformer-based models. In this post, we will delve into the self-attention mechanism, providing a step-by-step guide from scratch."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>Understanding Self-Attention - A Step-by-Step Guide | </title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="8947f1d8d93b21b66bd80e81bf931e1a" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/"></a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/"></a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/robotic"><span>Robotic</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/publication"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/certificates"><span>Certificates</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="mailto:a.asgharpoor1993@gmail.com" data-toggle="tooltip" data-placement="bottom" title="Drop me an email."  aria-label="Drop me an email.">
                <i class="fas fa-envelope" aria-hidden="true"></i>
              </a>
            </li>
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="https://github.com/armanasq" data-toggle="tooltip" data-placement="bottom" title="Follow Me on GitHub." target="_blank" rel="noopener" aria-label="Follow Me on GitHub.">
                <i class="fab fa-github" aria-hidden="true"></i>
              </a>
            </li>
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>Understanding Self-Attention - A Step-by-Step Guide</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Sep 1, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    12 min read
  </span>
  

  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <p><a href="/nlp/">⇐ Natural Language Processing</a></p>
<div style="width:50%;display: block; margin-left: auto; margin-right: auto; margin-top: 0px auto">
  <img src="/transformer/transformer.png" alt="Self-Attention" style="object-fit: cover;;">
</div>
<h1 id="understanding-self-attention---a-step-by-step-guide">Understanding Self-Attention - A Step-by-Step Guide</h1>
<p>Self-attention is a fundamental concept in natural language processing (NLP) and deep learning, especially prominent in transformer-based models. In this post, we will delve into the self-attention mechanism, providing a step-by-step guide from scratch.</p>
<p>Self-attention has gained widespread adoption in various models following the publication of the Transformer paper, &lsquo;Attention Is All You Need,&rsquo; garnering significant attention in the field.</p>
<hr>
<h2 id="1-introductiona-nameintroductiona">1. Introduction<a name="introduction"></a></h2>
<p>Self-attention, also known as scaled dot-product attention, is a fundamental concept in the field of NLP and deep learning. It plays a pivotal role in tasks such as machine translation, text summarization, and sentiment analysis. Self-attention enables models to weigh the importance of different parts of an input sequence when making predictions or capturing dependencies between words.</p>
<div style="width:100%;display: block; margin-left: auto; margin-right: auto; margin-top: 0px auto">
  <img src="/transformer/selfattention.png" alt="Self-Attention" style="object-fit: cover;;">
</div>
<h2 id="2-understanding-attentiona-nameunderstanding-attentiona">2. Understanding Attention<a name="understanding-attention"></a></h2>
<p>Before we dive into self-attention, let&rsquo;s grasp the broader concept of attention. Imagine reading a long document; your focus naturally shifts from one word to another, depending on the context. Attention mechanisms in deep learning mimic this behavior, allowing models to selectively concentrate on specific elements of the input data while ignoring others.</p>
<p>For instance, in the sentence &ldquo;The cat sat on the <strong>mat</strong>,&rdquo; attention helps you recognize that &ldquo;mat&rdquo; is the crucial word for understanding the sentence.</p>
<figure style="text-align: center;">
  <img src="/transformer/head-view.gif" alt="Attention">
  <figcaption>Credit: <a href="https://github.com/jessevig/bertviz" target="_blank" >https://github.com/jessevig/bertviz </a> </figcaption>
</figure>
<h2 id="3-self-attention-overview">3. Self-Attention Overview</h2>
<p>Picture self-attention as the conductor of an orchestra, orchestrating the harmony of information within an input embedding. Its role is to imbue contextual wisdom, allowing the model to discern the significance of individual elements within a sequence and dynamically adjust their influence on the final output. This orchestration proves invaluable in language processing tasks, where the meaning of a word hinges upon its companions in the sentence or document.</p>
<h4 id="the-quartet-q-k-v-and-self-attention">The Quartet: Q, K, V, and Self-Attention</h4>
<p>At the heart of self-attention are the quartet of Query ($Q$), Key ($K$), Value ($V$), and Self-Attention itself. These components work together in a symphony:</p>
<ul>
<li>
<p><strong>Query ($Q$):</strong> Think of the queries as the elements seeking information. For each word in the input sequence, a query vector is calculated. These queries represent what you want to pay attention to within the sequence.</p>
</li>
<li>
<p><strong>Key ($K$):</strong> Keys are like signposts. They help identify and locate important elements in the sequence. Like queries, key vectors are computed for each word.</p>
</li>
<li>
<p><strong>Value ($V$):</strong> Values carry the information. Once again, for each word, a value vector is computed. These vectors hold the content that we want to consider when determining the importance of words in the sequence.</p>
</li>
</ul>
<ol>
<li>
<p><strong>Query, Key, and Value Calculation:</strong> For each word in the input sequence, we calculate query ($Q$), key ($K$), and value ($V$) vectors. These vectors are the foundation upon which the attention mechanism operates.</p>
</li>
<li>
<p><strong>Attention Scores:</strong> With the quartet prepared, attention scores are computed for each pair of words in the sequence. The attention score between a query and a key quantifies their compatibility or relevance.</p>
</li>
<li>
<p><strong>Weighted Aggregation:</strong> Finally, the attention scores are used as weights to perform a weighted aggregation of the value vectors. This aggregation results in the self-attention output, representing an enhanced and contextually informed representation of the input sequence.</p>
</li>
</ol>
<h4 id="the-symphony-of-self-attention">The Symphony of Self-Attention</h4>
<p>Self-attention is not just a mechanism; it&rsquo;s a symphony of operations that elevate the understanding of sequences in deep learning models. Its adaptability and ability to capture intricate relationships are what make modern NLP models, like transformers, so powerful.</p>
<h2 id="4-embedding-an-input-sentence">4. Embedding an Input Sentence</h2>
<p>In natural language processing (NLP), representing words and sentences in a numerical format is essential for machine learning models to understand and process text. This process is known as &ldquo;word embedding&rdquo; or &ldquo;sentence embedding,&rdquo; and it forms the foundation for many NLP tasks. In this section, we&rsquo;ll delve into the concept of word embeddings and demonstrate how to embed a sentence using Python.</p>
<h3 id="word-embeddings">Word Embeddings</h3>
<p>Word embeddings are numerical representations of words, designed to capture semantic relationships between words. The idea is to map each word to a high-dimensional vector, where similar words are closer in the vector space. One of the most popular word embeddings is Word2Vec, which generates word vectors based on the context in which words appear in a large corpus of text.</p>
<p>Let&rsquo;s look at an example using the Gensim library to create Word2Vec embeddings:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Import the Gensim library</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample sentences for training the Word2Vec model</span>
</span></span><span class="line"><span class="cl"><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="s1">&#39;machine&#39;</span><span class="p">,</span> <span class="s1">&#39;learning&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;fascinating&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="s1">&#39;natural&#39;</span><span class="p">,</span> <span class="s1">&#39;language&#39;</span><span class="p">,</span> <span class="s1">&#39;processing&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;important&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="s1">&#39;word&#39;</span><span class="p">,</span> <span class="s1">&#39;embeddings&#39;</span><span class="p">,</span> <span class="s1">&#39;capture&#39;</span><span class="p">,</span> <span class="s1">&#39;semantic&#39;</span><span class="p">,</span> <span class="s1">&#39;relations&#39;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Train the Word2Vec model</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Get the word vector for a specific word</span>
</span></span><span class="line"><span class="cl"><span class="n">vector</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;machine&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="o">[</span>-1.9442164e-03 -5.2675214e-03  9.4471136e-03 -9.2987325e-03
</span></span><span class="line"><span class="cl">  4.5039477e-03  5.4041781e-03 -1.4092624e-03  9.0070926e-03
</span></span><span class="line"><span class="cl">  9.8853596e-03 -5.4750429e-03 -6.0210000e-03 -6.7469729e-03
</span></span><span class="line"><span class="cl"> -7.8948820e-03 -3.0479168e-03 -5.5940272e-03 -8.3446801e-03
</span></span><span class="line"><span class="cl">  7.8290224e-04  2.9946566e-03  6.4147436e-03 -2.6289499e-03
</span></span><span class="line"><span class="cl"> -4.4534765e-03  1.2495709e-03  3.9146186e-04  8.1169987e-03
</span></span><span class="line"><span class="cl">  1.8280029e-04  7.2315861e-03 -8.2645155e-03  8.4335366e-03
</span></span><span class="line"><span class="cl"> -1.8889094e-03  8.7011540e-03 -7.6168370e-03  1.7963862e-03
</span></span><span class="line"><span class="cl">  1.0564864e-03  4.6005251e-05 -5.1032533e-03 -9.2476979e-03
</span></span><span class="line"><span class="cl"> -7.2642174e-03 -7.9511739e-03  1.9137275e-03  4.7846674e-04
</span></span><span class="line"><span class="cl"> -1.8131376e-03  7.1201660e-03 -2.4756920e-03 -1.3473093e-03
</span></span><span class="line"><span class="cl"> -8.9005642e-03 -9.9254129e-03  8.9493981e-03 -5.7539381e-03
</span></span><span class="line"><span class="cl"> -6.3729975e-03  5.1994072e-03  6.6699935e-03 -6.8316413e-03
</span></span><span class="line"><span class="cl">  9.5975993e-04 -6.0084737e-03  1.6473436e-03 -4.2892788e-03
</span></span><span class="line"><span class="cl"> -3.4407973e-03  2.1856665e-03  8.6615775e-03  6.7281104e-03
</span></span><span class="line"><span class="cl"> -9.6770572e-03 -5.6221043e-03  7.8803329e-03  1.9893574e-03
</span></span><span class="line"><span class="cl"> -4.2560520e-03  5.9881213e-04  9.5209610e-03 -1.1027169e-03
</span></span><span class="line"><span class="cl"> -9.4246380e-03  1.6084099e-03  6.2323548e-03  6.2823701e-03
</span></span><span class="line"><span class="cl">  4.0916502e-03 -5.6502391e-03 -3.7069322e-04 -5.5317880e-05
</span></span><span class="line"><span class="cl">  4.5717955e-03 -8.0415895e-03 -8.0183093e-03  2.6475071e-04
</span></span><span class="line"><span class="cl"> -8.6082993e-03  5.8201565e-03 -4.1781188e-04  9.9711772e-03
</span></span><span class="line"><span class="cl"> -5.3439774e-03 -4.8613906e-04  7.7567734e-03 -4.0679323e-03
</span></span><span class="line"><span class="cl"> -5.0159004e-03  1.5900708e-03  2.6506938e-03 -2.5649595e-03
</span></span><span class="line"><span class="cl">  6.4475285e-03 -7.6599526e-03  3.3935606e-03  4.8997044e-04
</span></span><span class="line"><span class="cl">  8.7321829e-03  5.9827138e-03  6.8153618e-03  7.8225443e-03<span class="o">]</span>
</span></span></code></pre></div><p>In this example, we first import the Gensim library, which provides tools for creating and using word embeddings. We then define a list of sentences to train our Word2Vec model. The <code>vector_size</code> parameter specifies the dimensionality of the word vectors, <code>window</code> controls the context window size, <code>min_count</code> sets the minimum frequency for words to be considered, and <code>sg</code> (skip-gram) indicates the training algorithm.</p>
<p>After training, you can access the word vectors using <code>model.wv['word']</code>, where &lsquo;word&rsquo; is the word you want to obtain a vector for.</p>
<h3 id="sentence-embeddings">Sentence Embeddings</h3>
<p>While word embeddings represent individual words, sentence embeddings capture the overall meaning of a sentence. One popular method for obtaining sentence embeddings is by averaging the word vectors in the sentence:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample sentence and its word embeddings</span>
</span></span><span class="line"><span class="cl"><span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;machine&#39;</span><span class="p">,</span> <span class="s1">&#39;learning&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;fascinating&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">word_vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate the sentence embedding by averaging word vectors</span>
</span></span><span class="line"><span class="cl"><span class="n">sentence_embedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">word_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">sentence_embedding</span><span class="p">)</span>
</span></span></code></pre></div><p>In this code, we take a sample sentence and obtain the word embeddings for each word in the sentence using the Word2Vec model we trained earlier. We then calculate the sentence embedding by averaging the word vectors. This gives us a numerical representation of the entire sentence.</p>
<p>Sentence embeddings are useful for various NLP tasks, including text classification, sentiment analysis, and information retrieval.</p>
<h3 id="pre-trained-embeddings">Pre-trained Embeddings</h3>
<p>In many NLP projects, it&rsquo;s common to use pre-trained word embeddings or sentence embeddings. These embeddings are generated from large corpora and capture general language patterns. Popular pre-trained models include Word2Vec, GloVe, and BERT.</p>
<p>Here&rsquo;s how you can load pre-trained word embeddings using Gensim with the GloVe model:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load pre-trained GloVe embeddings</span>
</span></span><span class="line"><span class="cl"><span class="n">glove_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;glove.6B.100d.txt&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Get the word vector for a specific word</span>
</span></span><span class="line"><span class="cl"><span class="n">vector</span> <span class="o">=</span> <span class="n">glove_model</span><span class="p">[</span><span class="s1">&#39;machine&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
</span></span></code></pre></div><p>In this example, we load pre-trained GloVe embeddings from a file (&lsquo;glove.6B.100d.txt&rsquo; in this case) and access word vectors using <code>glove_model['word']</code>.</p>
<p>In summary, word and sentence embeddings play a pivotal role in NLP tasks, allowing us to represent text data numerically. Whether you create your own embeddings or use pre-trained models, embeddings are a fundamental component in building powerful NLP models.</p>
<h2 id="4-the-mathematics-of-self-attentiona-namethe-mathematics-of-self-attentiona">4. The Mathematics of Self-Attention<a name="the-mathematics-of-self-attention"></a></h2>
<p>Mathematically, self-attention can be expressed as:</p>
<p>Given an input sequence of vectors $X = [x_1, x_2, &hellip;, x_n]$, where $x_i$ is a vector representing the i-th element in the sequence, we compute the self-attention output $Y$ as follows:</p>
<p>$$
Y = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>
<ul>
<li>$Q$ (Query), $K$ (Key), and $V$ (Value) are learned linear transformations of the input sequence $X$, parameterized by weight matrices $W_Q$, $W_K$, and $W_V$, respectively.</li>
<li>$d_k$ is the dimension of the key vectors, often chosen to match the dimension of the query and value vectors.</li>
<li>$\text{softmax}$ is the softmax function applied along the rows of the matrix.</li>
<li>$QK^T$ is the matrix of dot products between query and key vectors.</li>
<li>The resulting matrix is used to weight the value vectors.</li>
</ul>
<p>Now, let&rsquo;s break down each step with a detailed example.</p>
<h2 id="5-self-attention-in-transformersa-nameself-attention-in-transformersa">5. Self-Attention in Transformers<a name="self-attention-in-transformers"></a></h2>
<p>Transformers, the backbone of modern NLP models, prominently feature self-attention. In a transformer architecture, self-attention is applied in parallel multiple times, followed by feedforward layers. Here&rsquo;s a detailed view of how it operates in a transformer:</p>
<ul>
<li>
<p><strong>Query, Key, and Value:</strong> Each input vector $x_i$ is linearly transformed into three vectors: query ($q_i$), key ($k_i$), and value ($v_i$). These transformations are achieved through learned weight matrices $W_Q$, $W_K$, and $W_V$. These vectors are used to compute attention scores.</p>
</li>
<li>
<p><strong>Attention Scores:</strong> The attention score between a query vector $q_i$ and a key vector $k_j$ is computed as their dot product:</p>
</li>
</ul>
<p>$$
\text{Attention}(q_i, k_j) = q_i \cdot k_j
$$</p>
<ul>
<li><strong>Scaled Attention:</strong> To stabilize training and control gradient magnitudes, the dot products are scaled down by a factor of $\sqrt{d_k}$, where $d_k$ is the dimension of the key vectors:</li>
</ul>
<p>$$
\text{Scaled Attention}(q_i, k_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
$$</p>
<ul>
<li><strong>Attention Weights:</strong> The scaled attention scores are passed through a softmax function to obtain attention weights that sum to 1:</li>
</ul>
<p>$$
\text{Attention Weight}(q_i, k_j) = \text{softmax}(\text{Scaled Attention}(q_i, k_j))
$$</p>
<ul>
<li><strong>Weighted Sum:</strong> Finally, the attention weights are used to compute a weighted sum of the value vectors:</li>
</ul>
<p>$$
\text{Self-Attention}(X) = \sum_j \text{Attention Weight}(q_i, k_j) \cdot v_j
$$</p>
<p>Now, let&rsquo;s work through a step-by-step example to illustrate how self-attention operates.</p>
<h2 id="6-step-by-step-examplea-namestep-by-step-examplea">6. Step-by-Step Example<a name="step-by-step-example"></a></h2>
<p>Let&rsquo;s consider a simple input sequence $X$ with three vectors, each of dimension 4:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">X = [
</span></span><span class="line"><span class="cl">  [1, 0, 0, 1],
</span></span><span class="line"><span class="cl">  [0, 2, 2, 0],
</span></span><span class="line"><span class="cl">  [1, 1, 0, 0]
</span></span><span class="line"><span class="cl">]
</span></span></code></pre></div><p>We&rsquo;ll perform self-attention on this sequence.</p>
<h3 id="step-1-query-key-and-value-transformation">Step 1: Query, Key, and Value Transformation</h3>
<p>We initiate the process by linearly transforming each vector in $X$ into query ($Q$), key ($K$), and value ($V$) vectors using learned weight matrices $W_Q$, $W_K$, and $W_V$. For this example, let&rsquo;s assume the weight matrices are as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">W_Q = [
</span></span><span class="line"><span class="cl">  [0.5, 0.2, 0.1, 0.3],
</span></span><span class="line"><span class="cl">  [0.1, 0.3, 0.2, 0.5],
</span></span><span class="line"><span class="cl">  [0.3, 0.1, 0.5, 0.2]
</span></span><span class="line"><span class="cl">]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">W_K = [
</span></span><span class="line"><span class="cl">  [0.4, 0.1, 0.3, 0.2],
</span></span><span class="line"><span class="cl">  [0.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">2, 0.4, 0.1, 0.3],
</span></span><span class="line"><span class="cl">  [0.3, 0.2, 0.4, 0.1]
</span></span><span class="line"><span class="cl">]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">W_V = [
</span></span><span class="line"><span class="cl">  [0.2, 0.3, 0.1, 0.4],
</span></span><span class="line"><span class="cl">  [0.4, 0.2, 0.3, 0.1],
</span></span><span class="line"><span class="cl">  [0.1, 0.4, 0.2, 0.3]
</span></span><span class="line"><span class="cl">]
</span></span></code></pre></div><p>Now, we compute the query, key, and value vectors for each element in $X$:</p>
<p>For the first element $x_1$:</p>
<ul>
<li>$q_1 = X[1] \cdot W_Q = [1, 0, 0, 1] \cdot [0.5, 0.2, 0.1, 0.3] = 0.8$</li>
<li>$k_1 = X[1] \cdot W_K = [1, 0, 0, 1] \cdot [0.4, 0.1, 0.3, 0.2] = 0.6$</li>
<li>$v_1 = X[1] \cdot W_V = [1, 0, 0, 1] \cdot [0.2, 0.3, 0.1, 0.4] = 0.6$</li>
</ul>
<p>For the second element $x_2$:</p>
<ul>
<li>$q_2 = X[2] \cdot W_Q = [0, 2, 2, 0] \cdot [0.1, 0.3, 0.2, 0.5] = 1.0$</li>
<li>$k_2 = X[2] \cdot W_K = [0, 2, 2, 0] \cdot [0.2, 0.4, 0.1, 0.3] = 0.6$</li>
<li>$v_2 = X[2] \cdot W_V = [0, 2, 2, 0] \cdot [0.4, 0.2, 0.3, 0.1] = 0.8$</li>
</ul>
<p>For the third element $x_3$:</p>
<ul>
<li>$q_3 = X[3] \cdot W_Q = [1, 1, 0, 0] \cdot [0.3, 0.1, 0.5, 0.2] = 0.6$</li>
<li>$k_3 = X[3] \cdot W_K = [1, 1, 0, 0] \cdot [0.3, 0.2, 0.4, 0.1] = 0.5$</li>
<li>$v_3 = X[3] \cdot W_V = [1, 1, 0, 0] \cdot [0.1, 0.4, 0.2, 0.3] = 0.5$</li>
</ul>
<h3 id="step-2-attention-scores">Step 2: Attention Scores</h3>
<p>Now that we have the query ($q_i$) and key ($k_j$) vectors for each element, we calculate the attention scores between all pairs of elements:</p>
<ul>
<li>Attention between $x_1$ and $x_1$: $\text{Attention}(q_1, k_1) = 0.8 \cdot 0.6 = 0.48$</li>
<li>Attention between $x_1$ and $x_2$: $\text{Attention}(q_1, k_2) = 0.8 \cdot 0.6 = 0.48$</li>
<li>Attention between $x_1$ and $x_3$: $\text{Attention}(q_1, k_3) = 0.8 \cdot 0.5 = 0.40$</li>
</ul>
<p>Continue this process for all pairs of elements.</p>
<h3 id="step-3-scaled-attention">Step 3: Scaled Attention</h3>
<p>To stabilize the attention scores during training and prevent issues related to gradient vanishing or exploding, the dot products are scaled down by a factor of $\sqrt{d_k}$. For this example, let&rsquo;s assume $d_k = 4$:</p>
<ul>
<li>Scaled Attention between $x_1$ and $x_1$: $\text{Scaled Attention}(q_1, k_1) = \frac{0.48}{\sqrt{4}} = 0.24$</li>
<li>Scaled Attention between $x_1$ and $x_2$: $\text{Scaled Attention}(q_1, k_2) = \frac{0.48}{\sqrt{4}} = 0.24$</li>
<li>Scaled Attention between $x_1$ and $x_3$: $\text{Scaled Attention}(q_1, k_3) = \frac{0.40}{\sqrt{4}} = 0.20$</li>
</ul>
<p>Continue this process for all pairs of elements.</p>
<h3 id="step-4-attention-weights">Step 4: Attention Weights</h3>
<p>Apply the softmax function to the scaled attention scores to obtain attention weights:</p>
<ul>
<li>Attention Weight between $x_1$ and $x_1$: $\text{Attention Weight}(q_1, k_1) = \text{softmax}(0.24) \approx 0.5987$</li>
<li>Attention Weight between $x_1$ and $x_2$: $\text{Attention Weight}(q_1, k_2) = \text{softmax}(0.24) \approx 0.5987$</li>
<li>Attention Weight between $x_1$ and $x_3$: $\text{Attention Weight}(q_1, k_3) = \text{softmax}(0.20) \approx 0.5799$</li>
</ul>
<p>Continue this process for all pairs of elements.</p>
<h3 id="step-5-weighted-sum">Step 5: Weighted Sum</h3>
<p>Finally, we compute the weighted sum of the value vectors using the attention weights:</p>
<ul>
<li>Weighted Sum for $x_1$: $\text{Self-Attention}(x_1) = 0.5987 \cdot v_1 + 0.5987 \cdot v_2 + 0.5799 \cdot v_3$</li>
</ul>
<p>This weighted sum represents the self-attention output for $x_1$. Repeat this process for $x_2$ and $x_3$ to get the self-attention outputs for all elements in the sequence.</p>
<h2 id="7-multi-head-attentiona-namemulti-head-attentiona">7. Multi-Head Attention<a name="multi-head-attention"></a></h2>
<p>In practical applications, self-attention is often extended to multi-head attention. Instead of relying on a single set of learned transformations ($W_Q$, $W_K$, $W_V$), multi-head attention uses multiple sets of transformations, or &ldquo;heads.&rdquo; Each head focuses on different aspects or relationships within the input sequence. The outputs of these heads are concatenated and linearly combined to produce the final self-attention output. This mechanism allows models to capture various types of information simultaneously.</p>
<h2 id="8-positional-encodinga-namepositional-encodinga">8. Positional Encoding<a name="positional-encoding"></a></h2>
<p>One critical aspect of self-attention is that it doesn&rsquo;t inherently capture the sequential order of elements in the input sequence, as it computes attention based on content alone. To address this limitation, positional encodings are added to the input embeddings in transformers. These encodings provide the model</p>
<p>with information about the positions of words in the sequence, enabling it to distinguish between words with the same content but different positions.</p>
<h2 id="9-applications-of-self-attentiona-nameapplications-of-self-attentiona">9. Applications of Self-Attention<a name="applications-of-self-attention"></a></h2>
<p>Self-attention has found applications beyond NLP and transformers. It has been used in computer vision for tasks like image segmentation, where capturing long-range dependencies is crucial. Additionally, self-attention mechanisms have been adapted for recommendation systems, speech processing, and even reinforcement learning.</p>
<h2 id="10-conclusiona-nameconclusiona">10. Conclusion<a name="conclusion"></a></h2>
<p>Self-attention is a cornerstone of modern deep learning, playing a vital role in understanding and processing sequential data effectively. This comprehensive guide has explored the theoretical foundations, mathematical expressions, practical applications, and a detailed step-by-step example of self-attention. By mastering self-attention, you gain insight into the inner workings of state-of-the-art models in NLP and other domains, opening the door to creating more intelligent and context-aware AI systems.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/natural-language-processing/">Natural Language Processing</a>
  
  <a class="badge badge-light" href="/tag/nlp/">NLP</a>
  
  <a class="badge badge-light" href="/tag/tutorial/">Tutorial</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Farmanasq.github.io%2Fnlp%2Fself-attention%2F&amp;text=Understanding&#43;Self-Attention&#43;-&#43;A&#43;Step-by-Step&#43;Guide" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Farmanasq.github.io%2Fnlp%2Fself-attention%2F&amp;t=Understanding&#43;Self-Attention&#43;-&#43;A&#43;Step-by-Step&#43;Guide" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=Understanding%20Self-Attention%20-%20A%20Step-by-Step%20Guide&amp;body=https%3A%2F%2Farmanasq.github.io%2Fnlp%2Fself-attention%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Farmanasq.github.io%2Fnlp%2Fself-attention%2F&amp;title=Understanding&#43;Self-Attention&#43;-&#43;A&#43;Step-by-Step&#43;Guide" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=Understanding&#43;Self-Attention&#43;-&#43;A&#43;Step-by-Step&#43;Guide%20https%3A%2F%2Farmanasq.github.io%2Fnlp%2Fself-attention%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Farmanasq.github.io%2Fnlp%2Fself-attention%2F&amp;title=Understanding&#43;Self-Attention&#43;-&#43;A&#43;Step-by-Step&#43;Guide" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://armanasq.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu423262b037e945bf3d00a3d75617f940_247637_270x270_fill_q75_lanczos_center.jpeg" alt="Arman Asgharpoor Golroudbari"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://armanasq.github.io/">Arman Asgharpoor Golroudbari</a></h5>
      <h6 class="card-subtitle">Space-AI Researcher</h6>
      <p class="card-text">My research interests revolve around planetary rovers and spacecraft vision-based navigation.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-comment-alt"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:a.asgharpoor1993@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=IlAgF9UAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/armanasq" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://linkedin.com/in/asgharpoor" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/my-orcid?orcid=0000-0001-6271-4533" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.webofscience.com/wos/author/record/IAN-3152-2023" target="_blank" rel="noopener">
        <i class="ai ai-publons"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://researchgate.net/profile/Arman_Asgharpoor" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/uploads/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  

  

  
  <section id="comments">
    
  
  <script src="https://giscus.app/client.js"
          data-repo="Armanasq/Armanasq.github.io"
          data-repo-id="R_kgDOJi13ZQ"
          data-category="[ENTER CATEGORY NAME HERE]"
          data-category-id="[ENTER CATEGORY ID HERE]"
          data-mapping="pathname"
          data-strict="0"
          data-reactions-enabled="1"
          data-emit-metadata="0"
          data-input-position="top"
          data-theme="preferred_color_scheme"
          data-lang="en"
          data-loading="lazy"
          crossorigin="anonymous"
          async>
  </script>


  </section>
  










  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2024 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.85070d5fe00d43eaedff44310b81dc2c.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>













  
    
      
      <!DOCTYPE html>
<html>
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-55GQYC5GYC"></script>

<style>
  .myImg {
    border-radius: 5px;
    cursor: pointer;
    transition: 0.3s;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  .myImg:hover {
    opacity: 0.7;
    cursor: pointer;
  }

   
  .modal-img {
    display: none;  
    position: fixed;  
    z-index: 1;  
    padding-top: 150px;  
    left: 0;
    top: 0px;
    width: 100%;  
    height: 100%;  
    overflow: visible;  
    background-color: rgb(0, 0, 0);  
    background-color: rgba(0, 0, 0, 0.6);  
    margin-left: auto;
    margin-right: auto;
  }

   
  .modal-content {
    display: block;
    margin-left: auto;
    margin-right: auto;
    max-width: 80%;
    max-height: 80%;

  }

   
  #caption {
    margin-left: auto;
    margin-right: auto;
    width: 80%;
    max-width: 700px;
    text-align: center;
    padding: 10px 0;

  }

   
  .modal-content,
  #caption {
    -webkit-animation-name: zoom;
    -webkit-animation-duration: 0.6s;
    animation-name: zoom;
    animation-duration: 0.6s;
    margin-left: auto;
    margin-right: auto;
  }

  @-webkit-keyframes zoom {
    from {
      -webkit-transform: scale(0);
    }
    to {
      -webkit-transform: scale(1);
    }
  }

  @keyframes zoom {
    from {
      transform: scale(0);
    }
    to {
      transform: scale(1);
    }
  }

   
  
 .modal-close {
    position: absolute;
    top: -55px;
    right: 0;
    font-size: 40px;
    font-weight: bold;
    transition: 0.3s;
    cursor: pointer;
  }

  .modal-close:hover,
  .modal-close:focus {
    color: #bbb;
    text-decoration: none;
  }

   
  @media only screen and (max-width: 900px) {
    .modal-content {
      width: 90%;
    }
  }

  .test:hover {
    scale: 1.2;
  }







  .navbar-nav {
    font-size:20px;
    font-family: Merriweather,sans-serif;
  }

  .robotic-section-container {
    display: flex;
    flex-wrap: wrap;
    justify-content: space-between;
    max-width: 1200px;
    margin: 0 auto;
  }
  
  .robotic-section {
    flex-basis: calc(30.33% - 12px);
    margin: 10px;
    background-color: #fff;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    transition: box-shadow 0.3s ease-in-out;
    overflow: hidden;
  }
  
  .robotic-section:hover {
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
  }
  
  .robotic-section-content {
    text-align: center;
    padding: 20px;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    height: 100%;
  }
  
  .robotic-section-content .image-placeholder {
    width: 300px;
    height: 300px;
    margin: 0 auto;
    display: flex;
    align-items: center;
    justify-content: center;
    background-color: #f1f1f1;
  }
  
  .robotic-section-content .image-placeholder img {
    max-width: 100%;
    max-height: 100%;
    object-fit: contain;
  }
  
  .robotic-section-content-h2 {
    margin-top: 10px;
    font-size: 1.rem;
    font-weight: bold;
    color: #333;
  }
  
  .robotic-section-content-h2 :hover{
    font-size: 10px
  }
  .robotic-section-content-h2 {
    margin-top: 10px;
    color: #777;
    font-size: 1.2rem;
  }
  
  .robotic-section-content .text-placeholder {
    height: 80px;
    background-color: #f1f1f1;
  }
  
  .robotic-section-content a {
    display: inline-block;
    margin-top: 20px;
    padding: 10px 20px;
    background-color: #FF4081;
    color: #fff;
    text-decoration: none;
    border-radius: 4px;
    font-weight: bold;
    transition: background-color 0.3s ease-in-out;
  }
  
  .robotic-section-content a:hover {
    background-color: #E91E63;
  }
  
   
  @media (max-width: 768px) {
    .robotic-section {
      flex-basis: calc(50% - 40px);
    }
  }
  
   
  @media (max-width: 480px) {
    .robotic-section {
      flex-basis: 100%;
    }
  }
</style>
</head>
<body>

<div id="myModal" class="modal-img">
  <div class="modal-content">
    <span class="modal-close">&times;</span>
    <img id="img01" style="margin-left: auto; margin-right: auto;">
    <div id="caption"></div>
  </div>
</div>




<script>
    
    var modal = document.getElementById("myModal");
    
    
    var images = document.querySelectorAll("img.myImg");
    
    
    var modalImg = document.getElementById("img01");
    var captionText = document.getElementById("caption");
    
    
    for (var i = 0; i < images.length; i++) {
      
      images[i].setAttribute("data-src", images[i].src);
      
      images[i].addEventListener("click", function() {
        
        modalImg.src = this.getAttribute("data-src");
        captionText.innerHTML = this.alt;
        
        modal.style.display = "block";
      });
    }
    
    
    var modalClose = document.querySelector(".modal-content .modal-close");
    
    
    modalClose.onclick = function() {
      modal.style.display = "none";
    };
    
















    
    </script>
    
    </body>
    </html>
      
    
  






</body>
</html>
