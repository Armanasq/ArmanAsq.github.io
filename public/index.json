
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Researcher | Space Enthusiast | Deep Learning I‚Äôm Arman Asgharpoor Golroudbari, a researcher at Deep Space Initiatives. I strive to applied machine learning through interdisciplinary research on flight dynamics, control theory, deep learning, and sensor fusion algorithms.\nResearch Collaborations: I am actively seeking opportunities for academic research collaborations, aiming to foster interdisciplinary synergies and engage in innovative knowledge exchange. ü§ùüí°\nResearch My passion for space technology has ignited collaborations with renowned institutions and projects, such as:\nMilky Way Program @ Deep Space Initiative: Contributed to space transportation system research, addressing pressing space-related issues. Oxford Machine Learning Summer School: Achieved top rank in the Health and Medicine OxML competition track, focusing on vision-based breast cancer detection. Fuzzy Logic Lab @ University of Tehran: Developed deep neural networks for visual odometry, enhancing accuracy and performance. Space Lab @ University of Tehran: Pioneered deep learning-based inertial odometry techniques, leveraging state-of-the-art datasets and optimization tools. Department of Aerospace Eng. @ University of Tehran: Explored the intriguing world of Quantum Computing and its applications in space. Experience Beyond Research I believe in sharing knowledge and experiences:\nMentor @ Space Generation Advisory Council: Providing personalized guidance and support to aspiring space enthusiasts. Martial Arts Instructor @ Iran Martial Arts Federation: Cultivating communication skills through teaching diverse students. Manager @ Arman Imen Passargad: Nurturing leadership and management skills in challenging work environments. Teaching Assistant @ University of Tehran: Imparting practical programming skills to graduate-level students. Feel free to reach out for inquiries or to explore exciting collaborations in the realm of space engineering. Let‚Äôs embark on an extraordinary journey together.\n","date":1682899200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1682899200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Researcher | Space Enthusiast | Deep Learning I‚Äôm Arman Asgharpoor Golroudbari, a researcher at Deep Space Initiatives. I strive to applied machine learning through interdisciplinary research on flight dynamics, control theory, deep learning, and sensor fusion algorithms.","tags":null,"title":"Arman Asgharpoor Golroudbari","type":"authors"},{"authors":null,"categories":null,"content":"‚áê Natural Language Processing\nUnderstanding Self-Attention - A Step-by-Step Guide Self-attention is a fundamental concept in natural language processing (NLP) and deep learning, especially prominent in transformer-based models. In this post, we will delve into the self-attention mechanism, providing a step-by-step guide from scratch.\nSelf-attention has gained widespread adoption in various models following the publication of the Transformer paper, ‚ÄòAttention Is All You Need,‚Äô garnering significant attention in the field.\n1. Introduction Self-attention, also known as scaled dot-product attention, is a fundamental concept in the field of NLP and deep learning. It plays a pivotal role in tasks such as machine translation, text summarization, and sentiment analysis. Self-attention enables models to weigh the importance of different parts of an input sequence when making predictions or capturing dependencies between words.\n2. Understanding Attention Before we dive into self-attention, let‚Äôs grasp the broader concept of attention. Imagine reading a long document; your focus naturally shifts from one word to another, depending on the context. Attention mechanisms in deep learning mimic this behavior, allowing models to selectively concentrate on specific elements of the input data while ignoring others.\nFor instance, in the sentence ‚ÄúThe cat sat on the mat,‚Äù attention helps you recognize that ‚Äúmat‚Äù is the crucial word for understanding the sentence.\nCredit: https://github.com/jessevig/bertviz 3. Self-Attention Overview Picture self-attention as the conductor of an orchestra, orchestrating the harmony of information within an input embedding. Its role is to imbue contextual wisdom, allowing the model to discern the significance of individual elements within a sequence and dynamically adjust their influence on the final output. This orchestration proves invaluable in language processing tasks, where the meaning of a word hinges upon its companions in the sentence or document.\nThe Quartet: Q, K, V, and Self-Attention At the heart of self-attention are the quartet of Query ($Q$), Key ($K$), Value ($V$), and Self-Attention itself. These components work together in a symphony:\nQuery ($Q$): Think of the queries as the elements seeking information. For each word in the input sequence, a query vector is calculated. These queries represent what you want to pay attention to within the sequence.\nKey ($K$): Keys are like signposts. They help identify and locate important elements in the sequence. Like queries, key vectors are computed for each word.\nValue ($V$): Values carry the information. Once again, for each word, a value vector is computed. These vectors hold the content that we want to consider when determining the importance of words in the sequence.\nQuery, Key, and Value Calculation: For each word in the input sequence, we calculate query ($Q$), key ($K$), and value ($V$) vectors. These vectors are the foundation upon which the attention mechanism operates.\nAttention Scores: With the quartet prepared, attention scores are computed for each pair of words in the sequence. The attention score between a query and a key quantifies their compatibility or relevance.\nWeighted Aggregation: Finally, the attention scores are used as weights to perform a weighted aggregation of the value vectors. This aggregation results in the self-attention output, representing an enhanced and contextually informed representation of the input sequence.\nThe Symphony of Self-Attention Self-attention is not just a mechanism; it‚Äôs a symphony of operations that elevate the understanding of sequences in deep learning models. Its adaptability and ability to capture intricate relationships are what make modern NLP models, like transformers, so powerful.\n4. Embedding an Input Sentence In natural language processing (NLP), representing words and sentences in a numerical format is essential for machine learning models to understand and process text. This process is known as ‚Äúword embedding‚Äù or ‚Äúsentence embedding,‚Äù and it forms the foundation for many NLP tasks. In this section, we‚Äôll delve into the concept of word embeddings and demonstrate how to embed a sentence using Python.\nWord Embeddings Word embeddings are numerical representations of words, designed to capture semantic relationships between words. The idea is to map each word to a high-dimensional vector, where similar words are closer in the vector space. One of the most popular word embeddings is Word2Vec, which generates word vectors based on the context in which words appear in a large corpus of text.\nLet‚Äôs look at an example using the Gensim library to create Word2Vec embeddings:\n# Import the Gensim library from gensim.models import Word2Vec # Sample sentences for training the Word2Vec model sentences = [ [\u0026#39;machine\u0026#39;, \u0026#39;learning\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;fascinating\u0026#39;], [\u0026#39;natural\u0026#39;, \u0026#39;language\u0026#39;, \u0026#39;processing\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;important\u0026#39;], [\u0026#39;word\u0026#39;, \u0026#39;embeddings\u0026#39;, \u0026#39;capture\u0026#39;, \u0026#39;semantic\u0026#39;, \u0026#39;relations\u0026#39;], ] # Train the Word2Vec ‚Ä¶","date":1693526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693526400,"objectID":"8947f1d8d93b21b66bd80e81bf931e1a","permalink":"https://armanasq.github.io/nlp/self-attention/","publishdate":"2023-09-01T00:00:00Z","relpermalink":"/nlp/self-attention/","section":"post","summary":"‚áê Natural Language Processing\nUnderstanding Self-Attention - A Step-by-Step Guide Self-attention is a fundamental concept in natural language processing (NLP) and deep learning, especially prominent in transformer-based models. In this post, we will delve into the self-attention mechanism, providing a step-by-step guide from scratch.","tags":["Natural Language Processing","NLP","Tutorial"],"title":"Understanding Self-Attention - A Step-by-Step Guide","type":"post"},{"authors":null,"categories":null,"content":"‚áê Natural Language Processing Fine Tune Pre-Trained Models with Hugging Face In the realm of Natural Language Processing (NLP), harnessing the capabilities of pre-trained models is a fundamental endeavor. These models, having already learned from vast amounts of text data, serve as valuable starting points for various NLP tasks. Hugging Face, a library widely embraced by NLP practitioners, simplifies the process of fine-tuning pre-trained models to suit specific applications.\nFine-tuning entails refining a pre-trained model by training it further on domain-specific data. This process allows the model to adapt and excel in tasks like text classification, sentiment analysis, and more. With its user-friendly interfaces and comprehensive functionalities, Hugging Face empowers us to effectively fine-tune these models, bridging the gap between general language understanding and specific tasks.\nThis tutorial delves into the pragmatic intricacies of fine-tuning pre-trained models using Hugging Face. We will explore the nuances of preparing datasets, navigating hyperparameters, visualizing training progress, and employing advanced training techniques. By the end of this tutorial, you‚Äôll be well-equipped to unlock the full potential of pre-trained models for your NLP undertakings.\nSection 1: Prepare Your Dataset Data Exploration Before delving into preprocessing, it‚Äôs essential to understand your dataset‚Äôs structure. For instance, let‚Äôs explore the Yelp Reviews dataset, which consists of reviews and associated ratings. Understanding your data is crucial, so let‚Äôs begin by gaining insights into the dataset‚Äôs dimensions:\nfrom datasets import load_dataset dataset = load_dataset(\u0026#34;yelp_review_full\u0026#34;) # Display the number of training and test examples print(\u0026#34;Number of training examples:\u0026#34;, len(dataset[\u0026#34;train\u0026#34;])) print(\u0026#34;Number of test examples:\u0026#34;, len(dataset[\u0026#34;test\u0026#34;])) # Print a few training examples for i in range(3): print(\u0026#34;Example\u0026#34;, i, \u0026#34;:\u0026#34;, dataset[\u0026#34;train\u0026#34;][i][\u0026#34;text\u0026#34;]) print(\u0026#34;Rating:\u0026#34;, dataset[\u0026#34;train\u0026#34;][i][\u0026#34;label\u0026#34;]) print(\u0026#34;----------\u0026#34;) Number of training examples: 650000 Number of test examples: 50000 Example 0 : dr. goldberg offers everything i look for in a general practitioner. he\u0026#39;s nice and easy to talk to without being patronizing; he\u0026#39;s always on time in seeing his patients; he\u0026#39;s affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first. really, what more do you need? i\u0026#39;m sitting here trying to think of any complaints i have about him, but i\u0026#39;m really drawing a blank. Rating: 4 ---------- Example 1 : Unfortunately, the frustration of being Dr. Goldberg\u0026#39;s patient is a repeat of the experience I\u0026#39;ve had with so many other doctors in NYC -- good doctor, terrible staff. It seems that his staff simply never answers the phone. It usually takes 2 hours of repeated calling to get an answer. Who has time for that or wants to deal with it? I have run into this problem with many other doctors and I just don\u0026#39;t get it. You have office workers, you have patients with medical needs, why isn\u0026#39;t anyone answering the phone? It\u0026#39;s incomprehensible and not work the aggravation. It\u0026#39;s with regret that I feel that I have to give Dr. Goldberg 2 stars. Rating: 1 ---------- Example 2 : Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He\u0026#39;s been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn\u0026#39;t judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life. Rating: 3 ---------- This exploration gives you a bird‚Äôs-eye view of your dataset, aiding in identifying potential imbalances and understanding its distribution.\nTokenization Strategies Tokenization lies at the core of NLP tasks, where text input is transformed into a sequence of tokens understandable by machine learning models. Hugging Face provides a versatile Tokenizer class that simplifies this crucial step. Tokenizers are available for various models and come in two flavors: a full Python implementation and a ‚ÄúFast‚Äù implementation based on the Rust library, which offers improved performance, especially for batched tokenization.\nPreparing Inputs The main classes for tokenization are PreTrainedTokenizer and PreTrainedTokenizerFast, which are base classes for all tokenizers. These classes provide common methods for encoding string inputs into model-ready inputs. They handle tokenizing (splitting text into sub-word token strings), converting token strings to IDs and vice versa, and encoding/decoding (tokenizing and converting to integers). Additionally, they facilitate the management of special tokens like mask, ‚Ä¶","date":1693440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693440000,"objectID":"20d7157ffbc39b33cbd25ed784085409","permalink":"https://armanasq.github.io/nlp/fine-tune-hugging-face/","publishdate":"2023-08-31T00:00:00Z","relpermalink":"/nlp/fine-tune-hugging-face/","section":"post","summary":"‚áê Natural Language Processing Fine Tune Pre-Trained Models with Hugging Face In the realm of Natural Language Processing (NLP), harnessing the capabilities of pre-trained models is a fundamental endeavor. These models, having already learned from vast amounts of text data, serve as valuable starting points for various NLP tasks.","tags":["Natural Language Processing","NLP","Tutorial"],"title":"Fine Tune Pre-Trained Models with Hugging Face","type":"post"},{"authors":null,"categories":null,"content":"‚áê Natural Language Processing Training Your Own BERT Model from Scratch üöÄ Hey there, fellow learner! ü§ì In this post, we‚Äôre going to embark on an exciting journey to train your very own BERT (Bidirectional Encoder Representations from Transformers) model from scratch. BERT is a transformer-based model that has revolutionized the field of natural language processing (NLP). Most of current tutorial only focus on fine-tuning the existing pre-trained model. By the end of this tutorial, you‚Äôll not only understand the code but also the intricate details of the methodologies involved.\nSection 0: Introduction üöÄ BERT has revolutionized the field of NLP by offering pre-trained models that capture rich contextual information from large text corpora. However, training a BERT model tailored to specific tasks or languages requires careful consideration and meticulous steps.\nThe Power of Custom BERT Models Custom BERT models empower researchers, data scientists, and developers to harness the capabilities of BERT while fine-tuning it for unique use cases. Whether you‚Äôre working on a specialized NLP task, dealing with languages with complex structures, or tackling domain-specific challenges, a custom BERT model can be your ally.\nWhat We Will Cover Throughout this tutorial, we will delve into every aspect of creating and training a custom BERT model:\nData Preparation: We‚Äôll start by preparing a diverse and substantial text corpus for training.\nTokenization: We‚Äôll explore tokenization, the process of breaking down text into smaller units for analysis. BERT relies on subword tokenization, a technique that can handle the complexity of various languages and word structures.\nModel Tokenizer Initialization: We‚Äôll initialize the model tokenizer, which is responsible for encoding text into input features that our BERT model can understand.\nPreparing Data for Training: We‚Äôll dive into the crucial steps of preparing our text data for BERT training. This includes masking tokens for masked language modeling (MLM), one of BERT‚Äôs key features.\nCreating a Custom Dataset: We‚Äôll construct a custom PyTorch dataset to efficiently organize and load our training data.\nTraining Configuration: We‚Äôll configure our BERT model, specifying its architecture and parameters. These configurations define the model‚Äôs behavior during training.\nModel Initialization: We‚Äôll initialize the BERT model for MLM, ensuring that it‚Äôs ready to learn from our data. This step includes handling GPU placement for accelerated training.\nTraining Loop and Optimization: We‚Äôll set up the training loop and optimize our model using the AdamW optimizer.\nTesting Your Custom BERT Model: Finally, we‚Äôll put our trained model to the test with real-world examples.\nLet‚Äôs begin our journey by delving into the critical step of data preparation.\nSection 1: Prerequisites and Setup üõ†Ô∏è Before we dive into the BERT training process, let‚Äôs ensure you have the necessary tools and libraries installed. We‚Äôll be using Python, so make sure you have it installed on your system.\nSetting up the Environment # Uninstall any existing TensorFlow versions (if applicable) !pip uninstall -y tensorflow # Install the \u0026#39;transformers\u0026#39; library from the Hugging Face repository !pip install git+https://github.com/huggingface/transformers # Check the installed versions !pip list | grep -E \u0026#39;transformers|tokenizers\u0026#39; We‚Äôll also need your Hugging Face API token (token) for later steps, so make sure you have it ready.\n# Import the relevant libraries for logging in from huggingface_hub import login login(token=`your_huggingface_token`) Section 2: Data Preparation üìä The Significance of Data The richness, diversity, and volume of our data directly impact the model‚Äôs language understanding and generalization capabilities. Our training data must be representative of the language and tasks our model will encounter in the real world which ensures that our model learns relevant patterns and information.\nA diverse dataset exposes the model to various language styles, topics, and domains to enhances the model‚Äôs ability to handle a wide range of inputs.\nThe size of the dataset matters. Larger datasets enable the model to learn more robust language representations, but they also require substantial computational resources.\nData Collection and Sources When collecting data for training a Language model, consider various sources such as books, articles, websites, and domain-specific texts. Open-source datasets and publicly available corpora can be valuable resources. In this tutorial, we obtained data from the OSCAR project (Open Super-large Crawled Aggregated coRpus) which is an Open Source project aiming to provide web-based multilingual resources and datasets. You can find the project page here. Also, you can use any other text data you have access.\nData Preprocessing Data preprocessing involves tasks like text cleaning, sentence tokenization, and ensuring uniform encoding (e.g., UTF-8). Proper preprocessing minimizes noise and ensures ‚Ä¶","date":1693440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693440000,"objectID":"3ddb8c91cfd849838d1c954c6b57de0f","permalink":"https://armanasq.github.io/nlp/train-BERT/","publishdate":"2023-08-31T00:00:00Z","relpermalink":"/nlp/train-BERT/","section":"post","summary":"‚áê Natural Language Processing Training Your Own BERT Model from Scratch üöÄ Hey there, fellow learner! ü§ì In this post, we‚Äôre going to embark on an exciting journey to train your very own BERT (Bidirectional Encoder Representations from Transformers) model from scratch.","tags":["Natural Language Processing","NLP","Tutorial","BERT"],"title":"Training Your Own BERT Model from Scratch ","type":"post"},{"authors":null,"categories":null,"content":"A Comprehensive Guide to the COCO Dataset A Comprehensive Guide to the COCO Dataset Introduction Dataset Characteristics Size and Scale How to Use COCO Dataset in Python PyCOCO COCO Dataset Format and Annotations JSON File Structure Annotation Details The COCO (Common Objects in Context) dataset is one of the most popular and widely used large-scale dataset which is designed for object detection, segmentation, and captioning tasks. In followings, we will explore the properties, characteristics, and significance of the COCO dataset, providing researchers with a detailed understanding of its structure and applications.\nNavigating through the vast expanse of the COCO dataset was an overwhelming experience for me at first. I felt disoriented and daunted by the scattered and insufficient resources available online, as well as the vague tutorials that only added to my confusion. It took numerous trial-and-error attempts and relentless determination to eventually uncover the path towards understanding. Reflecting on this arduous journey, I felt compelled to share my findings, from the very beginning to the triumphant end. My aim is to provide a comprehensive guide, eliminating the need for others to endure the same struggles I encountered. With this humble contribution, I hope to lighten the load for those embarking on their exploration of the COCO dataset, offering valuable insights and saving them from unnecessary hardships.\nHope this post helps you on your journey in computer vision tasks. You can find the Python code in the GitHub repo.\nIntroduction The COCO dataset is a collection of images that depict a wide range of everyday scenes and objects which was created to facilitate research in various computer vision tasks including object recognition, image understanding, and scene understanding. The dataset is notable for its large size, diversity, and rich annotations, making it a valuable resource for advancing computer vision algorithms. You can find it here\nDataset Characteristics Size and Scale The COCO dataset is substantial in size, consisting of over 330,000 images. These images capture a wide variety of scenes, objects, and contexts, making the dataset highly diverse. The images 80 object categories, including people, animals, vehicles, and common objects found in daily life.\nThis mindmap will help to have an overview of the categories in COCO dataset.\n- COCO Dataset\r- Categories\r- person\r- id: 1 - person\r- vehicle\r- id: 2 - bicycle\r- id: 3 - car\r- id: 4 - motorcycle\r- id: 5 - airplane\r- id: 6 - bus\r- id: 7 - train\r- id: 8 - truck\r- id: 9 - boat\r- outdoor\r- id: 10 - traffic light\r- id: 11 - fire hydrant\r- id: 13 - stop sign\r- id: 14 - parking meter\r- id: 15 - bench\r- animal\r- id: 16 - bird\r- id: 17 - cat\r- id: 18 - dog\r- id: 19 - horse\r- id: 20 - sheep\r- id: 21 - cow\r- id: 22 - elephant\r- id: 23 - bear\r- id: 24 - zebra\r- id: 25 - giraffe\r- accessory\r- id: 27 - backpack\r- id: 28 - umbrella\r- id: 31 - handbag\r- id: 32 - tie\r- id: 33 - suitcase\r- sports\r- id: 34 - frisbee\r- id: 35 - skis\r- id: 36 - snowboard\r- id: 37 - sports ball\r- id: 38 - kite\r- id: 39 - baseball bat\r- id: 40 - baseball glove\r- id: 41 - skateboard\r- id: 42 - surfboard\r- id: 43 - tennis racket\r- kitchen\r- id: 44 - bottle\r- id: 46 - wine glass\r- id: 47 - cup\r- id: 48 - fork\r- id: 49 - knife\r- id: 50 - spoon\r- id: 51 - bowl\r- food\r- id: 52 - banana\r- id: 53 - apple\r- id: 54 - sandwich\r- id: 55 - orange\r- id: 56 - broccoli\r- id: 57 - carrot\r- id: 58 - hot dog\r- id: 59 - pizza\r- id: 60 - donut\r- id: 61 - cake\r- furniture\r- id: 62 - chair\r- id: 63 - couch\r- id: 64 - potted plant\r- id: 65 - bed\r- id: 67 - dining table\r- id: 70 - toilet\r- electronic\r- id: 72 - tv\r- id: 73 - laptop\r- id: 74 - mouse\r- id: 75 - remote\r- id: 76 - keyboard\r- id: 77 - cell phone\r- appliance\r- id: 78 - microwave\r- id: 79 - oven\r- id: 80 - toaster\r- id: 81 - sink\r- id: 82 - refrigerator\r- indoor\r- id: 84 - book\r- id: 85 - clock\r- id: 86 - vase\r- id: 87 - scissors\r- id: 88 - teddy bear\r- id: 89 - hair drier\r- id: 90 - toothbrush How to Use COCO Dataset in Python First, you need to download the required library.\npip install pycocotools Import all the required liberaries via\nimport matplotlib.pyplot as plt import matplotlib.patches as patches import matplotlib.colors as colors import seaborn as sns import numpy as np from pycocotools.coco import COCO Let‚Äôs explore the dataset. In this post we used the 2014 version of the COCO dataset. We will define the COCO directory by:\ndataDir=\u0026#39;./\u0026#39; dataType=\u0026#39;val2014\u0026#39; annFile=\u0026#39;{}annotations/instances_{}.json\u0026#39;.format(dataDir,dataType) Then plot the distribution of different categories in the validation dataset (2014) to have a clear overview of the imbalancing in the dataset. Feel free to change the dataType to others inculding train2014, train2017, or val2017.\n# Initialize the COCO api for instance annotations coco=COCO(annFile) # Load the categories in a variable catIDs = coco.getCatIds() cats = coco.loadCats(catIDs) # Get category ‚Ä¶","date":1688256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688256000,"objectID":"90ada948aec7ee5e8b97fda850a6b6e1","permalink":"https://armanasq.github.io/datasets/coco-datset/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/datasets/coco-datset/","section":"post","summary":"A Comprehensive Guide to the COCO Dataset","tags":["Dataset","Computer Vision","COCO"],"title":"COCO Dataset","type":"post"},{"authors":null,"categories":null,"content":"A Comprehensive Guide to the ImageNet Dataset Introduction\nImageNet is a large-scale, diverse database of annotated images designed to aid in visual object recognition software research. This dataset has been instrumental in advancing the field of computer vision and deep learning. In this comprehensive guide, we will explore the structure, organization, characteristics, and significance of the ImageNet dataset.\nStructure and Organization The ImageNet dataset is structured based on the WordNet hierarchy, a lexical database of semantic relations between words in more than 200 languages. Each node in this hierarchy is called a ‚Äúsynset‚Äù and represents a concept that can be described by multiple words or phrases. For example, a synset could be a general category like ‚Äúfurniture‚Äù or a specific object like ‚Äúdesk.‚Äù\nThe ImageNet dataset consists of over 14 million images, each associated with a specific synset. The dataset aims to provide approximately 1000 images for each synset, offering a rich variety of examples for different object categories. The images are annotated with metadata, including the URL of the image, the bounding box coordinates for the object, and the synset ID.\nTo provide a more detailed understanding of the structure and organization, let‚Äôs examine a subset of object categories in ImageNet along with their corresponding synset IDs:\nObject Category Synset ID Cat n02123045 Dog n02084071 Car n02958343 Chair n02791124 Bird n01503061 Each synset represents a specific concept or object category, and it is associated with a unique synset ID. These synset IDs are used to link the images in the dataset to their corresponding object categories.\nDataset Characteristics Size and Scale The ImageNet dataset is exceptionally large, containing over 14 million images. This vast collection allows researchers to train models on a massive scale, capturing a wide range of visual concepts and object categories. The size of the dataset contributes to its representative nature and provides a rich resource for visual recognition tasks.\nImage Quality and Diversity The images in the ImageNet dataset are sourced from various channels, ensuring a diverse range of image quality and visual characteristics. The dataset includes images captured in different settings, under varying lighting conditions, and using various cameras. This diversity enhances the robustness of models trained on the dataset, enabling them to handle real-world scenarios more effectively.\nAnnotation and Labels Each image in the ImageNet dataset is carefully annotated and labeled by human workers using Amazon‚Äôs Mechanical Turk crowdsourcing tool. The annotations provide essential information such as precise bounding box coordinates, segmentations, and class labels for the objects present in the images. The manual curation of annotations ensures high-quality and accurate labeling, making ImageNet suitable for various computer vision tasks, including object recognition, detection, and segmentation.\nTo illustrate the annotations and labels, let‚Äôs consider an example image from the dataset:\nThis image, annotated with a bounding box and class label, showcases the level of detail provided in ImageNet annotations.\nHierarchical Organization ImageNet follows the hierarchical organization of object categories based on the WordNet hierarchy. This hierarchy provides a structured taxonomy for classifying objects into fine-grained categories. The WordNet hierarchy consists of thousands of synsets, representing specific concepts or objects. The hierarchical organization enables researchers to explore different levels of object recognition, from general object classification to fine-grained classification.\nTo visualize the hierarchical structure, let‚Äôs take a look at a subset of categories in the WordNet hierarchy:\nObject Category Parent Category Child Categories Animal Living Thing | Mammal, Reptile, Bird, Fish, Insect, Amphibian | | Furniture | Object | Chair, Table, Bed, Desk, Shelf | | Vehicle | Object | Car, Bicycle, Train, Bus, Motorcycle |\nThe hierarchical organization of the categories enables researchers to explore relationships between object classes and develop models capable of capturing fine-grained distinctions.\nTraining and Validation Splits To facilitate fair evaluation and comparison of different algorithms and models, ImageNet provides predefined training and validation splits. The training set contains a vast number of images used for training deep learning models, while the validation set is used for evaluating the performance of these models. The training and validation splits are designed to maintain a balanced representation of object categories, ensuring unbiased evaluation across different classes.\nThe predefined splits allow researchers to assess the generalization and performance of their models on unseen data, providing a standardized benchmark for evaluating object recognition algorithms.\nImageNet Large Scale Visual Recognition Challenge ‚Ä¶","date":1688256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688256000,"objectID":"e284f31f277f66a8ecc014175e32e0c4","permalink":"https://armanasq.github.io/datasets/image-net-datset/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/datasets/image-net-datset/","section":"post","summary":"A Comprehensive Guide to the ImageNet Dataset Introduction\nImageNet is a large-scale, diverse database of annotated images designed to aid in visual object recognition software research. This dataset has been instrumental in advancing the field of computer vision and deep learning.","tags":["Dataset","Computer Vision","IMAGE NET"],"title":"IMAGE NET Dataset","type":"post"},{"authors":null,"categories":null,"content":"‚áê Computer Vision\nImage Segmentation: A Tutorial Image Segmentation: A Tutorial Introduction: Unraveling the Art of Image Segmentation Understanding Image Segmentation Types of Image Segmentation 1. Semantic Segmentation 2. Instance Segmentation 3. Panoptic Segmentation 4. Boundary-based Segmentation 5. Interactive Segmentation Techniques for Image Segmentation 1. Traditional Methods 1.1 Region Growing Algorithm: Unveiling the Seeds of Segmentation 1.2 Sequential Labeling Algorithm: Unraveling the Sequential Order of Image Segmentation 1.3 Thresholding-Based Algorithm: Unveiling Segmentation through Intensity 1.4 Active Contours Algorithm: Achieving Deformable Image Segmentation 2. Deep Learning-based Methods 3. Attention Mechanisms 4. Transformers in Segmentation 5. Semi-Supervised and Weakly-Supervised Segmentation Evaluation Metrics Challenges and Future Directions 1. Handling Small and Thin Objects 2. Dealing with Class Imbalance 3. Real-time Segmentation 4. Interpretability and Explainability 5. Few-shot and Zero-shot Segmentation 6. Incorporating Domain Knowledge Conclusion Introduction: Unraveling the Art of Image Segmentation Welcome to this tutorial on image segmentation, a captivating journey into the heart of computer vision. In this in-depth guide, we will delve into the fascinating world of image segmentation, a fundamental task that lies at the core of visual understanding and analysis. Image segmentation empowers us to dissect an image into semantically meaningful regions, enabling precise object localization and providing a pixel-level comprehension of visual data. As a pivotal aspect of computer vision, image segmentation finds diverse applications across numerous domains, including object recognition, scene understanding, medical image analysis, robotics, autonomous vehicles, and more. It is a key component of visual understanding systems, computer vision tasks and image processing techniques.\nAt the intersection of art and science, image segmentation challenges us to bridge the gap between pixels and semantics, unlocking the potential for machines to perceive the visual world with human-like acuity. By accurately delineating objects and regions of interest, segmentation algorithms lay the foundation for various high-level vision tasks, such as instance recognition, tracking, 3D reconstruction, and augmented reality. It involves partitioning visual data into multiple segments or regions with similar visual characteristics.\nThis tutorial will serve as your gateway to an advanced understanding of image segmentation. We will explore a wide spectrum of segmentation techniques, ranging from traditional methods rooted in handcrafted features to state-of-the-art deep learning-based models driven by neural networks. We will also discover state-of-the-art techniques including attention mechanisms and transformer architectures, which have breathed new life into the field, revolutionizing the way we perceive and process visual data.\nMoreover, this tutorial will equip you with the knowledge to evaluate segmentation models using various metrics, enabling you to quantify their performance and guide your research towards more impactful results. Alongside evaluation, we will also unravel the challenges that continue to inspire researchers in the quest for enhanced segmentation techniques. From handling class imbalance to addressing real-time constraints and achieving interpretability, we will uncover the cutting-edge advancements that are shaping the future of image segmentation.\nWhether you are an aspiring computer vision researcher or a seasoned practitioner seeking to stay at the forefront of the field, this tutorial will be your beacon of knowledge. We invite you to immerse yourself in the intricate world of image segmentation and embark on a journey of discovery, innovation, and transformative contributions to the fascinating realm of computer vision. Let us unlock the secrets of image segmentation, paving the way for groundbreaking advancements in artificial intelligence and beyond.\nUnderstanding Image Segmentation Image segmentation is the process of partitioning an image into multiple non-overlapping segments or regions, each representing a distinct object, area, or component in the scene. Unlike image classification, which assigns a single label to the entire image, image segmentation provides a fine-grained understanding at the pixel level. Image segmentation could be considered as a pixel-wise clustering task in which each pixel label as a particular class. This pixel-wise labeling enables various downstream tasks, such as object localization and tracking, instance counting, and 3D reconstruction.\nTypes of Image Segmentation 1. Semantic Segmentation Semantic segmentation aims to assign a semantic label to each pixel in the image. The labels correspond to predefined categories, such as ‚Äúcar,‚Äù ‚Äútree,‚Äù ‚Äúroad,‚Äù etc. This type of segmentation enables a holistic understanding of the scene, but it ‚Ä¶","date":1688256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688256000,"objectID":"395f107f4477ff0f85754b85f6c4a547","permalink":"https://armanasq.github.io/computer-vision/image-segementation/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/computer-vision/image-segementation/","section":"post","summary":"‚áê Computer Vision\nImage Segmentation: A Tutorial Image Segmentation: A Tutorial Introduction: Unraveling the Art of Image Segmentation Understanding Image Segmentation Types of Image Segmentation 1. Semantic Segmentation 2. Instance Segmentation 3.","tags":["Image Segmentation","Tutorial"],"title":"Image Segmentation","type":"post"},{"authors":null,"categories":null,"content":"‚áê Computer Vision Image Segmentation Tutorial using COCO Dataset and Deep Learning Image Segmentation Tutorial using COCO Dataset and Deep Learning COCO Dataset Overview 1. Large-Scale Image Collection 2. Object Categories 3. Instance-Level Annotations 4. Captions for Images 5. Training, Validation, and Test Splits 6. Evaluation Metrics Prerequisites Steps Step 1: Set up the Environment Step 2: Install Required Libraries Step 3: Download and Preprocess the COCO Dataset Step 4: Prepare the Data for Training Step 5: Implement the Model Step 6: Train the Model Step 7: Perform Image Segmentation In this tutorial, we will delve into how to perform image segmentation using the COCO dataset and deep learning. Image segmentation is the process of partitioning an image into multiple segments to identify objects and their boundaries. The COCO dataset is a popular benchmark dataset for object detection, instance segmentation, and image captioning tasks. We will use deep learning techniques to train a model on the COCO dataset and perform image segmentation. You can find a comprehensive tutorial on using COCO dataset here.\nCOCO Dataset Overview The COCO (Common Objects in Context) dataset is a widely used large-scale benchmark dataset for computer vision tasks, including object detection, instance segmentation, and image captioning. It provides a large-scale collection of high-quality images, along with pixel-level annotations for multiple object categories.\nCOCO was created to address the limitations of existing datasets, such as Pascal VOC and ImageNet, which primarily focus on object classification or bounding box annotations. COCO extends the scope by providing rich annotations for both object detection and instance segmentation.\nThe key features of the COCO dataset include:\n1. Large-Scale Image Collection The COCO dataset contains over 200,000 images, making it one of the largest publicly available datasets for computer vision tasks. The images are sourced from a wide range of contexts, including everyday scenes, street scenes, and more. The large-scale collection ensures diversity and represents real-world scenarios.\n2. Object Categories COCO covers a wide range of object categories, including common everyday objects, animals, vehicles, and more. It consists of 80 distinct object categories, such as person, car, dog, and chair. The variety of object categories enables comprehensive evaluation and training of computer vision models.\n3. Instance-Level Annotations One of the distinguishing features of the COCO dataset is its detailed instance-level annotations. Each object instance in an image is labeled with a bounding box and a pixel-level segmentation mask. This fine-grained annotation allows models to understand the boundaries and shapes of objects, making it suitable for tasks like instance segmentation.\n4. Captions for Images In addition to object annotations, the COCO dataset includes five English captions for each image. This aspect of the dataset makes it valuable for natural language processing tasks, such as image captioning and multimodal learning.\n5. Training, Validation, and Test Splits The COCO dataset is divided into three main subsets: training, validation, and test. The training set consists of a large number of images (around 118,000), which are commonly used for training deep learning models. The validation set (around 5,000 images) is used for hyperparameter tuning and model evaluation during development. The test set (around 40,000 images) is not publicly available, and its annotations are withheld for objective evaluation in benchmark challenges.\n6. Evaluation Metrics COCO introduces evaluation metrics tailored for different tasks. For object detection, the widely used mean Average Precision (mAP) metric is employed, which considers precision-recall curves for different object categories. For instance segmentation, the COCO dataset uses the COCO mAP metric, which considers both bounding box accuracy and segmentation quality.\nOverall, the COCO dataset has become a standard benchmark for evaluating and advancing state-of-the-art computer vision models. Its large-scale image collection, detailed annotations, and diverse object categories make it a valuable resource for developing and evaluating models for various computer vision tasks.\nPrerequisites Before getting started, make sure you have the following:\nPython 3.6 or above: Python is the programming language we‚Äôll use for the tutorial. TensorFlow 2.x or PyTorch: We‚Äôll use one of these deep learning frameworks for building and training the segmentation model. COCO dataset: Download the COCO dataset from the official website. Choose the desired version (e.g.,2014, 2017) and download the following files: Train images: train2014.zip Train annotations: annotations_trainval2014.zip After downloading, extract the contents of both ZIP files into a directory of your choice.\nSteps Step 1: Set up the Environment Create a new directory for your project ‚Ä¶","date":1688256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688256000,"objectID":"002ac64b31a7692450b0790c8b32f981","permalink":"https://armanasq.github.io/computer-vision/image-segementation-coco/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/computer-vision/image-segementation-coco/","section":"post","summary":"‚áê Computer Vision Image Segmentation Tutorial using COCO Dataset and Deep Learning Image Segmentation Tutorial using COCO Dataset and Deep Learning COCO Dataset Overview 1. Large-Scale Image Collection 2. Object Categories 3.","tags":["Image Segmentation","Tutorial"],"title":"Image Segmentation Using COCO Dataset","type":"post"},{"authors":null,"categories":null,"content":"","date":1688256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688256000,"objectID":"c052a1ce3e680dcd2fde919001ab4a15","permalink":"https://armanasq.github.io/datasets/open-image-datset/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/datasets/open-image-datset/","section":"post","summary":"","tags":["Dataset","Computer Vision","Open Images Dataset"],"title":"Open Images Dataset","type":"post"},{"authors":[],"categories":null,"content":"Advancing Cancer Diagnosis: Improving Classification of Histopathological Slices As participants of the Oxford Machine Learning Summer School 2023, we are thrilled to announce our achievement in the implementation of transfer learning techniques for improving the classification of cancer cells in H\u0026amp;E stained histopathological slices. We are proud to have secured the 1st and 3rd positions in the Health Cases Competition.\nToday, we present our project, which aimed for cancer diagnosis by identifying the presence of carcinoma cells and determining their benign or malignant nature.\nObjective: Our primary goal was to develop a robust classification system capable of accurately distinguishing H\u0026amp;E stained histopathological slices as either containing carcinoma cells or not, while also providing insights into the nature of the carcinoma, whether it is benign or malignant.\nDataset: We utilized a dataset comprising 186 histopathological slides from breast biopsies, with 62 of them annotated with labels for training and evaluation.\nChallenge: Throughout our project, we faced two main challenges. First, we had limited annotated training data, necessitating innovative approaches to ensure reliable results. Second, the dataset had an uneven distribution of classes, which required careful handling to address class imbalances.\nTo address these challenges, we employed transfer learning, a technique that utilizes pre-trained models on large-scale datasets to enhance performance on specific tasks. By leveraging the knowledge gained from prior tasks, we aimed to improve the accuracy of cancer cell classification in histopathological slices.\nOur research involved several key stages. We meticulously preprocessed the data to optimize the input images for accurate analysis. We then selected suitable models and fine-tuned them to optimize their performance for our specific task.\nAdditionally, we employed techniques such as data augmentation and class balancing to mitigate the impact of limited training data and class imbalances. These approaches played a crucial role in enhancing the model‚Äôs ability to generalize and make accurate predictions.\nThroughout our project, we conducted thorough evaluations using established performance metrics such as accuracy, precision, recall, and F1-score. We also compared our results with state-of-the-art methods and performed extensive cross-validation to ensure the reliability and generalizability of our findings.\n","date":1688223600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688223600,"objectID":"3a769d4aef3a6e193da67e131cdfd004","permalink":"https://armanasq.github.io/talk/deep-learning-based-carcinoma-classification-oxml-2023/","publishdate":"2023-07-01T15:00:00Z","relpermalink":"/talk/deep-learning-based-carcinoma-classification-oxml-2023/","section":"event","summary":"Advancing Cancer Diagnosis: Improving Classification of Histopathological Slices As participants of the Oxford Machine Learning Summer School 2023, we are thrilled to announce our achievement in the implementation of transfer learning techniques for improving the classification of cancer cells in H\u0026E stained histopathological slices.","tags":[],"title":"Deep Learning-based Carcinoma Classification - OxML 2023","type":"event"},{"authors":null,"categories":null,"content":"In this tutorial, we will explore Physics Informed Neural Networks (PINNs), which are neural networks trained to solve supervised learning tasks while respecting given laws of physics described by general nonlinear partial differential equations. PINNs are a class of data-efficient universal function approximators that encode underlying physical laws as prior information. We will cover two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. We will provide examples and Python code snippets using TensorFlow to illustrate the concepts.\nIntroduction to PINN Formulation of PINN Data-Driven Loss Term Physics-Based Loss Term Total Loss Function Training PINN Advantages of PINN Limitations and Challenges Conclusion PINN is a powerful and innovative framework that combines the strengths of both physics-based modeling and deep learning. This approach aims to solve partial differential equations (PDEs) and other physical problems by leveraging the expressiveness of neural networks while incorporating prior knowledge of the underlying physics. PINN has gained significant attention in recent years due to its ability to handle complex, multi-physics problems and provide accurate predictions even with limited data.\nIntroduction to PINN Traditional methods for solving PDEs, such as finite difference or finite element methods, rely on discretizing the domain and solving a system of equations. These approaches often require fine-grained meshes, which can be computationally expensive and challenging to implement for complex geometries. Additionally, these methods may struggle with noisy or incomplete data.\nPINN offers an alternative solution by combining physics-based models with neural networks, which are known for their ability to learn complex patterns and generalize well to unseen data. By parameterizing the solution using a neural network, PINN can approximate the unknown solution to a PDE using a set of training data and enforce the governing equations at the same time.\nFormulation of PINN The key idea behind PINN is to train a neural network to approximate the solution to a PDE while respecting the underlying physics. This is achieved by minimizing a loss function that consists of two components: a data-driven loss term and a physics-based loss term.\nData-Driven Loss Term The data-driven loss term ensures that the neural network accurately predicts the known data points. Suppose we have a set of N data points, denoted as {(x_i, t_i, y_i)} for i = 1, 2, ‚Ä¶, N, where (x_i, t_i) represents the spatial and temporal coordinates, and y_i represents the corresponding observed value. The data-driven loss term is typically defined as the mean squared error between the predicted solution and the observed data:\nHere, u(x_i, t_i) denotes the predicted solution at (x_i, t_i), and Œ© represents the training domain.\nPhysics-Based Loss Term The physics-based loss term ensures that the neural network satisfies the governing equations of the PDE. Suppose we have a set of K governing equations, denoted as {F_k}, where k = 1, 2, ‚Ä¶, K. These equations represent the physical laws or conservation principles governing the system. The physics-based loss term is typically defined as the mean squared error between the residuals of the governing equations:\nHere, R_k(u) denotes the residual of the kth governing equation, which is obtained by substituting the predicted solution u(x, t) into the kth equation.\nTotal Loss Function The total loss function is the sum of the data-driven loss term and the physics-based loss term:\nHere, Œ± and Œ≤ are hyperparameters that control the relative importance of the data-driven and physics-based terms, respectively.\nTraining PINN To train a PINN, we typically use an optimization algorithm, such as stochastic gradient descent (SGD), to minimize the total loss function. The weights and biases of the neural network are updated iteratively to find the optimal solution. During the training process, the network learns to approximate the unknown solution to the PDE by minimizing the data-driven loss term while satisfying the physics-based loss term.\nAdvantages of PINN PINN offers several advantages over traditional methods for solving PDEs:\nFlexibility: PINN can handle complex geometries and boundary conditions without the need for explicit meshing or grid generation. This flexibility allows for easier integration with real-world applications and reduces the computational cost associated with mesh generation.\nGeneralizability: Neural networks have the ability to generalize well to unseen data. Once trained, a PINN can accurately predict the solution at any point within the domain, even in regions where no data points are available. This is particularly useful when dealing with sparse or noisy data.\nMulti-physics Applications: PINN is capable of solving multi-physics problems by incorporating multiple sets of governing equations. This makes it suitable for problems ‚Ä¶","date":1687046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687046400,"objectID":"3f9e90700677d674286fde35fad3c584","permalink":"https://armanasq.github.io/Deep-Learning/PINN/","publishdate":"2023-06-18T00:00:00Z","relpermalink":"/Deep-Learning/PINN/","section":"post","summary":"In this tutorial, we will explore Physics Informed Neural Networks (PINNs), which are neural networks trained to solve supervised learning tasks while respecting given laws of physics described by general nonlinear partial differential equations.","tags":["Deep Learning","Physics-Informed Neural Networks","PINN","Neural Networks"],"title":"Physics-Informed Neural Networks (PINN)","type":"post"},{"authors":null,"categories":null,"content":"\rComprehensive PyTorch Tutorial: Implementing a Neural Network Class üéâ Welcome to this tutorial on implementing a Neural Network class using PyTorch! üéâ\nIn this tutorial, we will focus on building a Neural Network class in PyTorch and go through each step in detail, explaining every part of the Neural Network class, so you can understand the underlying concepts thoroughly.\nComprehensive PyTorch Tutorial: Implementing a Neural Network Class 1. Introduction to Neural Networks üß† 1.1 The Neurons - Building Blocks of a Neural Network 1.2 Learning Superpowers üí™ 1.3 Building a Neural Network üõ†Ô∏è 1.4 The Amazing Forward Pass üèÉ‚Äç‚ôÇÔ∏è 1.5 Learning from Mistakes üß†‚ö° 2. Building Blocks of a Neural Network Input Layer Hidden Layers Output Layer 3. Defining the Neural Network Model Class 3.1 Parts of the Neural Network Model Class 3.1.1 The Constructor Method 3.1.2 The Forward Method 3.1.3 The Input Layer 3.1.4 The Hidden Layers 3.1.5 The Output Layer 3.2 Defining the Neural Network Model Class in Code Explanation: 3.3 Summary 4. Implementing Forward Propagation 5. Backpropagation and Training 6. Adding Activation Functions 7. Customizing Loss Functions 8. Optimizers for Training 9. Putting It All Together: Example Usage 10. Conclusion Let‚Äôs get started!\n1. Introduction to Neural Networks üß† [Credit: Alex Lenail].\nNeural Networks are fascinating and powerful models inspired by the intricate structure of the human brain. They have revolutionized the field of artificial intelligence and are at the core of many modern machine learning advancements. In this introduction, we will delve into the inner workings of Neural Networks, exploring their architecture, learning process, and applications.\n1.1 The Neurons - Building Blocks of a Neural Network At the heart of a Neural Network are artificial neurons, also known as nodes. These neurons are analogous to the neurons in the human brain, and they are responsible for processing and transmitting information. Each neuron takes input from other neurons or external data, processes it using learnable parameters (weights and biases), and then produces an output. These outputs serve as inputs for the neurons in the next layer, creating a cascade of interconnected information flow.\n1.2 Learning Superpowers üí™ The learning process in Neural Networks is a marvel of computational intelligence. During training, the network learns from labeled examples to adjust its learnable parameters (weights and biases). This optimization process aims to minimize a predefined loss function that measures the difference between the predicted outputs and the true labels.\nTo achieve this, the network utilizes a powerful algorithm called backpropagation. During backpropagation, the network‚Äôs performance on the training data is evaluated, and the gradients of the loss function with respect to the learnable parameters are calculated. These gradients indicate the direction and magnitude of adjustments needed to minimize the loss. The network then uses optimization algorithms, like stochastic gradient descent (SGD) or Adam, to update the parameters accordingly.\n1.3 Building a Neural Network üõ†Ô∏è To construct a Neural Network, we first decide on its architecture. This includes determining the number of layers, the number of neurons in each layer, and the connections between them. The first layer is the input layer, which receives the raw input data. The last layer is the output layer, responsible for producing the final predictions or outcomes of the network.\nIn between the input and output layers, we have one or more hidden layers. These layers are the powerhouse of the network, as they perform the complex computations and feature extraction required for learning intricate patterns in the data. The number of hidden layers and the number of neurons in each layer are hyperparameters that depend on the problem‚Äôs complexity and the size of the dataset.\n1.4 The Amazing Forward Pass üèÉ‚Äç‚ôÇÔ∏è Once we have our Neural Network architecture set up, it‚Äôs time for the forward pass! During the forward pass, data flows through the network from the input layer to the output layer. At each neuron, the input data is multiplied by the corresponding weights, and biases are added to create a weighted sum. This sum is then passed through an activation function, like the popular Rectified Linear Unit (ReLU) or Sigmoid, to introduce non-linearity to the model.\nThe result of each neuron‚Äôs computation becomes the input for the neurons in the next layer. This sequential computation through the layers enables the network to learn increasingly abstract and higher-level representations of the input data.\n1.5 Learning from Mistakes üß†‚ö° After the forward pass, the network compares its predictions with the true labels of the training data. The discrepancies between the predicted outputs and the true labels are quantified using a loss function, such as mean squared error or cross-entropy loss. The goal during training is to minimize this loss, effectively ‚Ä¶","date":1685923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685923200,"objectID":"e90f46f6871bf7c88842f460b781afba","permalink":"https://armanasq.github.io/Deep-Learning/PyTorch-NN-Class/","publishdate":"2023-06-05T00:00:00Z","relpermalink":"/Deep-Learning/PyTorch-NN-Class/","section":"post","summary":"A tutorial on mastering Neural Network Class in PyTorch! .","tags":["PyTorch"],"title":"PyTorch Tutorial: Implementing a Neural Network Class","type":"post"},{"authors":null,"categories":null,"content":"\rDeep Neural Network Implementation Using PyTorch - Implementing all the layers In this tutorial, we will explore the various layers available in the torch.nn module. These layers are the building blocks of neural networks and allow us to create complex architectures for different tasks. We will cover a wide range of layers, including containers, convolution layers, pooling layers, padding layers, non-linear activations, normalization layers, recurrent layers, transformer layers, linear layers, dropout layers, sparse layers, distance functions, loss functions, vision layers, shuffle layers, data parallel layers, utilities, quantized functions, and lazy module initialization.\nContainers Containers are modules that serve as organizational structures for other neural network modules. They allow us to combine multiple layers or modules together to form a more complex neural network architecture. In PyTorch, there are several container classes available in the torch.nn module.\nModule Module is the base class for all neural network modules in PyTorch. It provides the fundamental functionalities and attributes required for building neural networks. When creating a custom neural network module, we typically inherit from the Module class.\nSequential Sequential is a container that allows us to stack layers or modules in a sequential manner. It provides a convenient way to define and organize the sequence of operations in a neural network. Each layer/module added to the Sequential container is applied to the output of the previous layer/module in the order they are passed.\nExample code:\nimport torch import torch.nn as nn model = nn.Sequential( nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10), nn.Softmax(dim=1) ) ModuleList ModuleList is a container that holds submodules in a list. It allows us to create a list of layers or modules and access them as if they were attributes of the container. ModuleList is useful when we have a varying number of layers or modules, and we want to iterate over them or access them dynamically.\nExample code:\nimport torch import torch.nn as nn class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.layers = nn.ModuleList([ nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10), nn.Softmax(dim=1) ]) def forward(self, x): for layer in self.layers: x = layer(x) return x ModuleDict ModuleDict is a container that holds submodules in a dictionary. Similar to ModuleList, it allows us to create a dictionary of layers or modules and access them by their specified keys. This is useful when we have a collection of layers or modules with specific names or purposes.\nExample code:\nimport torch import torch.nn as nn class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.layers = nn.ModuleDict({ \u0026#39;fc1\u0026#39;: nn.Linear(784, 128), \u0026#39;relu\u0026#39;: nn.ReLU(), \u0026#39;fc2\u0026#39;: nn.Linear(128, 10), \u0026#39;softmax\u0026#39;: nn.Softmax(dim=1) }) def forward(self, x): x = self.layers[\u0026#39;fc1\u0026#39;](x) x = self.layers[\u0026#39;relu\u0026#39;](x) x = self.layers[\u0026#39;fc2\u0026#39;](x) x = self.layers[\u0026#39;softmax\u0026#39;](x) return x ParameterList and ParameterDict ParameterList and ParameterDict are containers specifically designed for holding parameters (e.g., weights and biases) in a list or dictionary, respectively. They are useful when we want to manage and manipulate parameters in a structured manner, especially in cases where we have a varying number of parameters.\nExample code:\nimport torch import torch.nn as nn class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.weights = nn.ParameterList([ nn.Parameter(torch.randn(784, 128)), nn.Parameter(torch.randn(128, 10)) ]) self.biases = nn.ParameterDict({ \u0026#39;bias1\u0026#39;: nn.Parameter(torch.zeros(128)), \u0026#39;bias2\u0026#39;: nn.Parameter(torch.zeros(10)) }) def forward(self, x): x = torch.matmul(x, self.weights[0]) + self.biases[\u0026#39;bias1\u0026#39;] x = torch.relu(x) x = torch.matmul(x, self.weights[1]) + self.biases[\u0026#39;bias2\u0026#39;] return x Exercise: Create a custom neural network using any combination of ModuleList and ModuleDict containers, and define your forward pass logic.\nBy using these container classes, we can easily organize and manage the layers or modules within our deep neural network, enabling us to build complex architectures with ease.\nConvolution Layers Convolution layers are widely used in computer vision tasks for extracting features from input data. They apply a set of learnable filters to input data and produce feature maps. The nn.Conv2d layer in PyTorch is commonly used for 2D convolution operations.\nExample code:\nimport torch import torch.nn as nn class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) def forward(self, x): x = self.conv1(x) x = torch.relu(x) x = self.conv2(x) x = torch.relu(x) return x Exercise: Create a convolutional neural network with multiple convolution layers. ‚Ä¶","date":1685836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685836800,"objectID":"d13c12fe854442ee62b972e035afb72a","permalink":"https://armanasq.github.io/Deep-Learning/PyTorch-DNN-layers/","publishdate":"2023-06-04T00:00:00Z","relpermalink":"/Deep-Learning/PyTorch-DNN-layers/","section":"post","summary":"Deep Neural Network Implementation Using PyTorch - Implementing all the layers In this tutorial, we will explore the various layers available in the torch.nn module. These layers are the building blocks of neural networks and allow us to create complex architectures for different tasks.","tags":["Deep Learning","Neural Networks","PyTorch"],"title":"DNN Implementation Using PyTorch - Exploring Layers","type":"post"},{"authors":null,"categories":null,"content":"\rA Profound Exploration of Derivatives in PyTorch: An Advanced Comprehensive Guide 1. Introduction to Derivatives 1.1. Understanding Derivatives Derivatives are fundamental mathematical tools that measure how a function changes with respect to its inputs. In the context of deep learning, derivatives play a crucial role in optimization algorithms like gradient descent, enabling neural networks to learn from data and improve their performance over time.\n1.2. Why Derivatives in PyTorch? PyTorch‚Äôs automatic differentiation engine, known as autograd, sets it apart as a leading deep learning framework. Automatic differentiation allows PyTorch to calculate derivatives effortlessly during both forward and backward passes through the neural network. This powerful capability frees researchers from the burden of manually computing gradients, enabling them to focus on model design and experimentation.\n2. Calculating Derivatives 2.1. Gradients with Autograd The heart of PyTorch‚Äôs automatic differentiation lies in the computation of gradients. In this section, we‚Äôll dive into the mechanics of autograd, which automatically tracks operations performed on tensors during the forward pass and computes gradients during the backward pass. Let‚Äôs see an example of gradient computation:\nimport torch # Define a tensor with requires_grad=True to track its operations x = torch.tensor([3.0], requires_grad=True) # Define a function (e.g., y = 2*x^2 + 3*x + 1) y = 2 * x**2 + 3 * x + 1 # Compute gradients with respect to x y.backward() # Access gradients using the grad attribute of the tensor print(x.grad) # Output: tensor([15.]) 2.2. Computing Partial Derivatives Deep learning models often have multiple parameters, requiring the computation of partial derivatives. In this section, we‚Äôll explore techniques to calculate partial derivatives and gradients for complex functions involving multiple variables.\nimport torch # Define multiple tensors with requires_grad=True to track their operations x = torch.tensor([1.0], requires_grad=True) y = torch.tensor([2.0], requires_grad=True) # Define a function (e.g., z = 3*x^2 + 4*y + 2) z = 3 * x**2 + 4 * y + 2 # Compute gradients with respect to both x and y z.backward() # Access gradients using the grad attribute of the tensors print(x.grad) # Output: tensor([6.]) print(y.grad) # Output: tensor([4.]) 2.3. Higher-Order Derivatives Beyond first-order derivatives, we can delve into the realm of higher-order derivatives, such as second-order derivatives. Second derivatives provide valuable insights into the curvature of functions and the optimization landscape. We can compute Hessian matrices, which encapsulate all second partial derivatives, and leverage them for optimization and advanced model training.\nimport torch # Define a tensor with requires_grad=True to track its operations x = torch.tensor([2.0], requires_grad=True) # Define a function (e.g., y = x^3 + 2x^2 + 3x + 4) y = x**3 + 2 * x**2 + 3 * x + 4 # Compute first-order and second-order derivatives with respect to x first_derivative = torch.autograd.grad(y, x, create_graph=True)[0] second_derivative = torch.autograd.grad(first_derivative, x)[0] print(first_derivative) # Output: tensor([17.]) print(second_derivative) # Output: tensor([10.]) 3. Custom Derivatives 3.1. Defining Custom Functions PyTorch allows researchers to define custom functions using standard Python operations. By using PyTorch‚Äôs tensor operations, researchers can create complex functions that involve tensors and still obtain their gradients effortlessly. This flexibility is crucial for advanced deep learning tasks that require custom loss functions, activation functions, or other components.\nimport torch # Define a custom function using PyTorch tensor operations def custom_function(x): return torch.sin(x) + torch.cos(x) # Define a tensor with requires_grad=True to track its operations x = torch.tensor([1.0], requires_grad=True) # Apply the custom function to the tensor y = custom_function(x) # Compute gradients with respect to x y.backward() # Access gradients using the grad attribute of the tensor print(x.grad) # Output: tensor([-0.3012]) 3.2. Creating Custom Derivatives In some cases, PyTorch‚Äôs autograd may not automatically handle the derivatives of certain operations or functions. However, PyTorch allows researchers to extend autograd and define custom derivatives for non-standard operations. By creating custom gradients, researchers can ensure accurate and precise gradient calculations for their specific use cases.\nimport torch # Define a custom function with non-standard derivative def custom_function(x): return torch.log(x) # Define a tensor with requires_grad=True to track its operations x = torch.tensor([2.0], requires_grad=True) # Apply the custom function to the tensor y = custom_function(x) # Manually define the custom derivative for the function def custom_derivative(x): return 1 / x # Compute gradients with respect to x using the custom derivative ‚Ä¶","date":1685664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685664000,"objectID":"9ef88c920d18a8237913448a660aeb21","permalink":"https://armanasq.github.io/Deep-Learning/PyTorch-Derivatives/","publishdate":"2023-06-02T00:00:00Z","relpermalink":"/Deep-Learning/PyTorch-Derivatives/","section":"post","summary":"The comprehensive guide on derivatives in PyTorch covers custom gradients, optimization, control flow, and more, empowering researchers in advanced deep learning.","tags":["PyTorch"],"title":"A Profound Exploration of Derivatives in PyTorch: An Advanced Comprehensive Guide","type":"post"},{"authors":null,"categories":null,"content":"\rA Profound Journey into PyTorch Tensors: A Tutorial A Profound Journey into PyTorch Tensors: A Tutorial 1. Introduction to Tensors 1.1. What are Tensors? 1.2. Why Use Tensors in PyTorch? 1.2.1. GPU Acceleration 1.2.2. Automatic Differentiation 1.2.3. Compatibility with Neural Network Libraries 1.3. Tensor Notation and Nomenclature 2. Creating Tensors 2.1. Initialization from Lists and Arrays 2.2. Creating Tensors with Default Values 2.3. Creating Tensors from Existing Tensors 2.4. Data Types and Precision 2.5. Tensor Attributes and Metadata 2.6. Tensor Serialization and I/O 3. Tensor Operations: Indexing and Slicing 3.1. Basic Indexing and Indexing Tricks 3.2. Advanced Indexing (Integer and Boolean) 3.1. Basic Indexing and Indexing Tricks 3.1.1. Basic Indexing 3.1.2. Advanced Indexing with Integer Arrays 3.1.3. Boolean Array Indexing 3.3. Modifying Values Using Indexing 3.4. Slicing and Striding Explained 3.4.1. Slicing to Extract Sub-tensors 3.4.2. Striding to Skip Elements during Slicing 3.5. In-place vs. Out-of-place Operations 3.5.1. In-place Operations 3.5.2. Out-of-place Operations 4. Element-wise Tensor Operations 4.1. Arithmetic Operations 4.2. Element-wise Mathematical Functions 4.3. Comparison Operations 4.4. Clipping Tensors 4.5. Handling NaN and Inf 5. Tensor Broadcasting 5.1. Broadcasting Rules and Broadcasting Dimensions 5.2. Broadcasting Examples and Common Pitfalls 5.3. Broadcasting vs. Tile and Expand 6. Working with Devices (CPU and GPU) 6.1. Device Configuration and Availability 6.2. Moving Tensors Between Devices 6.3. Using Mixed Precision (Half and Single) 7. Tensor Creation Methods 7.1. Zeros and Ones Tensors 7.2. Identity and Diagonal Tensors 7.3. Range and Linspace Tensors 7.4. Logspace and Exponential Tensors 7.5. Random Tensors (Uniform, Normal, and more) 7.6. Loading Data from NumPy Arrays 8. Tensor Reshaping and Dimensionality 8.1. Reshaping Tensors 8.2. Transposing and Permuting Dimensions 8.3. Squeezing and Unsqueezing 8.4. Flattening and Raveling Tensors 8.5. Concatenating and Stacking Tensors 9. Tensor Reduction Operations 9.1. Summation and Mean 9.2. Minimum and Maximum 9.3. Argmin and Argmax 9.4. Reductions Along Specific Axes 9.5. Logical Reductions (All, Any) 10. Gradient Computation and Autograd 10.1. Automatic Differentiation in PyTorch 10.2. Computing Gradients with Autograd 10.3. Detaching Tensors from Autograd 10.4. Working with require_grad and volatile 11. Tensor Operations in Advanced Topics 11.1. Advanced Broadcasting and Einsum 11.2. Tensor Concatenation and Splitting 11.3. Masked Operations and Scatter-Gather 11.4. Advanced Element-wise Operations Conclusion Welcome to this comprehensive and scholarly tutorial on the intriguing domain of PyTorch tensors. In this academic exposition, we will delve deep into the intricate intricacies of tensors, encompassing their creation, mathematical operations, and advanced functionalities, all within the context of deep learning. This tutorial aims to provide a rigorous and technical understanding of PyTorch tensors, enabling you to effectively harness their power for various machine learning endeavors.\nAs we embark on this scholarly journey, we shall navigate through the foundational concepts and theoretical underpinnings of tensors. Emphasis will be placed on their rigorous mathematical representations, properties, and significance in the context of PyTorch.\nThroughout this academic exploration, we shall familiarize ourselves with the celestial realm of PyTorch, an esteemed open-source machine learning library renowned for its capabilities in neural network research. We will delve into the realm of automatic differentiation, a powerful tool that empowers us to efficiently compute gradients and optimize complex models with utmost precision.\nAdditionally, we shall appreciate the ethereal elegance of GPU acceleration, a pivotal technology that accelerates tensor operations, thereby facilitating expedited and more efficient computations for deep learning tasks. This integration of hardware acceleration elevates the performance of neural networks to unparalleled heights, enabling cutting-edge research and applications.\nOur scholarly pursuit will extend to explore the vast landscape of neural network libraries, harmonizing with PyTorch‚Äôs interoperability and compatibility. Understanding these integrated ecosystems will expand our repertoire, equipping us to develop sophisticated and adaptive models that are seamlessly integrated with state-of-the-art architectures.\nThroughout this intellectual odyssey, we shall engage with the nomenclature and notations that embellish tensors, offering profound insights into their inherent structure and characteristics. The meticulous examination of scalar, vector, and matrix tensors will serve as foundational stepping stones to unravel the complexities of higher-dimensional tensors, where abstract structures unfold, enriching our understanding of the mathematical abstractions at play.\nEach ‚Ä¶","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"cfd9458e0a27ecadd1e7f34caa594949","permalink":"https://armanasq.github.io/Deep-Learning/PyTorch-Tensors/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/Deep-Learning/PyTorch-Tensors/","section":"post","summary":"A journey into PyTorch tensors: creation, operations, gradient computation, and advanced functionalities for deep learning.","tags":["PyTorch"],"title":"A Profound Journey into PyTorch Tensors: A Comprehensive Tutorial","type":"post"},{"authors":null,"categories":null,"content":"Carcinoma Classification - OxML 2023 Cases Advanced Cancer Classification Repository The-Health-and-Medicine-OxML-competition-track This repository contains code for a sophisticated and advanced cancer classification model. The model utilizes state-of-the-art deep learning architectures, including ResNet-50, EfficientNet-V2, Inception-V3, and GoogLeNet, to classify images of skin lesions into three classes: benign, malignant, and unknown.\nTable of Contents Carcinoma Classification - OxML 2023 Cases Advanced Cancer Classification Repository The-Health-and-Medicine-OxML-competition-track Table of Contents Introduction Dataset Approach Preprocessing Methodology Equations and Formulas Implementation Details Dataset and Data Augmentation Model Selection and Training Cross-Validation Model Evaluation and Predictions Data Preprocessing Model Architecture Training Loss Function: Optimization Algorithm: Model Ensemble: Evaluation Metrics: Evaluation Conclusion Introduction Cancer classification is a challenging task that plays a crucial role in early detection and diagnosis. The proposed model in this repository aims to accurately classify skin lesion images into three classes: benign, malignant, and unknown. To achieve this, the model utilizes a combination of pre-trained deep learning models, including ResNet-50, EfficientNet-V2, Inception-V3, and GoogLeNet, each with their specific strengths and features. By leveraging the power of ensemble learning, the model can make robust and accurate predictions.\nThis code is part of the Carcinoma Classification competition, specifically focusing on classifying HES stained histopathological slices as containing or not containing carcinoma cells. The goal is to determine if a carcinoma is present and, if so, whether it is benign or malignant. The competition provides a dataset of 186 images, with labels available for only 62 of them. Due to the limited training data, participants are encouraged to leverage pre-trained models and apply various techniques to improve classification performance.\nDataset The dataset consists of HES stained histopathological slices. Each image may contain carcinoma cells, and the corresponding labels indicate whether the carcinoma is benign (0), malignant (1), or not present (-1). It is important to note that the training data is highly imbalanced, and a naive classification approach labeling all samples as healthy would yield high accuracy but an unbalanced precision/recall trade-off. The evaluation metric for this competition is the Mean F1-Score, which provides a good trade-off between sensitivity and specificity.\nApproach To tackle this task, several approaches can be employed:\nRelying on a pre-trained model and using zero/few-shot learning techniques. Fine-tuning the last layer of a pre-trained model for a new classification task. Leveraging a pre-trained model and applying a different classifier, such as Gaussian Process, SVMs, or XGBoost. Preprocessing Several preprocessing considerations should be taken into account when working with this dataset:\nImage Size: The images in the dataset do not have the same size, and cropping them may result in missing the target cells. Resizing the images may alter their features and make them less readable, so careful handling is necessary. Methodology The code provided implements a deep learning pipeline for image classification using pre-trained models. The pipeline consists of the following steps:\nSetting seeds: The set_seeds function sets the random seeds to ensure reproducibility of the results.\nCustomDataset class: This class is used to create a custom dataset for loading and preprocessing the image data. It takes the image directory, labels file, and optional transformations as inputs. The class provides methods to retrieve the length of the dataset and individual data items.\nDevice setup: The code checks if a GPU is available and sets the device accordingly.\nData preparation:\nLoading labeled dataset: The labeled dataset is loaded from a CSV file containing image labels. Finding maximum size: The maximum width and height of the images in the dataset are determined. Data transformation: Two main data transformation pipelines are defined: main_transform: Resizes the images to the maximum width and height, converts them to tensors, and applies normalization. augmentation_transform: Includes resizing, random flips, rotation, color jitter, and normalization for data augmentation. Dataset creation: The main dataset and augmented dataset are created using the CustomDataset class and the respective transformation pipelines. Combining datasets: The main dataset and augmented dataset are combined using the ConcatDataset class. Stratified k-fold cross-validation: The combined dataset is split into train and validation sets using stratified k-fold cross-validation. Model setup:\nPre-trained model loading: Several pre-trained models from the torchvision library are loaded, including ResNet50, EfficientNetV2, ‚Ä¶","date":1685577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685577600,"objectID":"214c3de1b00b9c375723abe8c63d6be5","permalink":"https://armanasq.github.io/project/OxML2023/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/project/OxML2023/","section":"project","summary":"Advanced Cancer Classification Repository. The-Health-and-Medicine-OxML-competition-track","tags":["Deep Learning","OxML"],"title":"Carcinoma Classification - OxML 2023 Cases","type":"project"},{"authors":null,"categories":null,"content":"\rIntroduction üöÄ Dive into the Exciting World of Deep Neural Networks with PyTorch! ü§ñüî•\nHey there, fellow tech enthusiast! ü§ì Ever felt like PyTorch is a bit of a puzzle, unlike its more user-friendly counterparts? Well, fret not, because we‚Äôre here to spice up your deep learning journey! üéâ\n‚ö° Introducing the ‚ÄúPyTorch Deep Dive‚Äù Series ‚ö°\nBuckle up, because we‚Äôre taking you on a whirlwind adventure through the realm of implementing Deep Neural Networks using PyTorch. üåü Whether you‚Äôre a fresh-faced researcher venturing into the deep learning domain or a pro switching up your frameworks, this series has your back!\nüìö It‚Äôs Not Your Typical Tutorial üìö\nBefore we get all technical, let‚Äôs set the stage. We‚Äôre not going to baby you through PyTorch basics. This isn‚Äôt your run-of-the-mill PyTorch tutorial; this is where we show you how to wield the power of PyTorch to craft magnificent Deep Neural Networks. So, if you‚Äôve got your basics down with libraries like NumPy and Matplotlib, you‚Äôre golden!\nüß† Deep Dive, Literally üß†\nHold up! This isn‚Äôt Deep Learning 101 either. We‚Äôre diving straight into the meaty part ‚Äì building the real deal! So if you‚Äôre nodding your head to terms like Neural Networks and the magic of backpropagation, you‚Äôre all set to sail.\nüåà Practical Deep Learning FTW üåà\nPicture this: in our first tutorial, we‚Äôre going hands-on with a super simple deep neural network example using PyTorch. üöÄ Trust us, learning through examples is the secret sauce! üçîüçü After all, who doesn‚Äôt love breaking down complex problems with relatable examples?\nüéÆ Tutorial Level 1: PyTorch Essentials üéÆ\nReady, steady, go! üèÅ We‚Äôre kicking things off by giving you the lowdown on PyTorch ‚Äì why you should use it, what makes it tick, and why it‚Äôs your trusty sidekick for the deep learning journey. Then, brace yourself for some action! We‚Äôre jumping into a fun classification task. Think of it as a sneak peek into the PyTorch universe: how models are born, trained, and put to the test.\nFeeling a bit overwhelmed? No worries at all! Everyone starts somewhere, and this is your starting line. With a sprinkle of patience, a dash of trial and error, and a dollop of dedication, you‚Äôll be whizzing through this in no time.\nüöÄ Elevate Your Neural Network Game üöÄ\nHere‚Äôs the deal: we‚Äôre covering the nitty-gritty, from foundational concepts to crafting your network architecture. By the end of this tutorial, you‚Äôll be waving your PyTorch wand to create powerful deep learning models. ü™Ñ‚ú®\nReady to embark on this journey? We thought so! Head over to the Google Colab notebook at the link below and let‚Äôs dive into the world of PyTorch-powered Deep Neural Networks together:\nüîó Notebook Link\nLet‚Äôs make those neurons dance and those models shine! üï∫üíÉüíª\nTable of Contents Introduction 0. What‚Äôs the Hype about PyTorch? 0.1 The ‚ÄúWhy-Is-PyTorch-Awesome‚Äù Showdown 0.2 The ‚ÄúCode It Like It‚Äôs Hot‚Äù Vibe 0.3 PyTorch vs. the Universe 1. Prerequisites 2. Getting Started with PyTorch Installation Importing Required Libraries Setting up the GPU (Optional) 3. Dataset Preparation Data Loading Data Preprocessing Train-Validation-Test Split 4. Model Architecture Design Neural Network Layers Activation Functions Loss Functions Optimizers Hyperparameters 5. Building the Deep Neural Network Model Defining the Model Class Initializing the Model 6. Training the Model Setting up Training Parameters Defining the Loss Function Selecting the Optimizer Define the Number of Epochs Define the Batch Size Creat Dataset Loader Training the Model Monitoring Training Progress 7. Evaluating the Model Testing the Model Model Evaluation Metrics 8. Improving Model Performance Regularization Techniques Hyperparameter Tuning Data Augmentation Transfer Learning 9. Saving and Loading Models Saving the Model Loading the Model 10. Conclusion 11. References 0. What‚Äôs the Hype about PyTorch? PyTorch, the brainchild of the whizzes at Facebook‚Äôs AI Research lab (FAIR), is THE open-source framework empowering deep learning daredevils like you. üé©‚ú® Whether you‚Äôre a research maestro or a coding ninja, PyTorch is your trusty sidekick for crafting and taming deep neural networks that conquer complexity like champs.\n0.1 The ‚ÄúWhy-Is-PyTorch-Awesome‚Äù Showdown Hold onto your code hats, because PyTorch packs a punch like no other! Here are the dazzling stars that set PyTorch on the red carpet of deep learning fame:\nPython Power-Up: PyTorch speaks fluent Python and cozies up with its libraries, making it the ultimate wingman for your Python-powered projects. üêçüìö\nFacebook‚Äôs Fav: You know it‚Äôs a superstar when even Facebook themselves use it for their deep learning endeavors. üì∏üëë\nUser-Friendly Vibes: PyTorch brings the party with an API so intuitive, even your pet parrot could grasp it. ü¶úüíÉ\nGraphs on the Fly: Ever seen a graph build itself while the code dances? PyTorch‚Äôs dynamic computational graphs are the coolest party trick in town. üï∫üìä\nSpeed Racer: PyTorch zips through computations faster than a caffeinated cheetah, giving you a ‚Ä¶","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"a9f37b0d8a4fc07b7fd18e177567dc8f","permalink":"https://armanasq.github.io/Deep-Learning/PyTorch-DNN/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/Deep-Learning/PyTorch-DNN/","section":"post","summary":"Introduction üöÄ Dive into the Exciting World of Deep Neural Networks with PyTorch! ü§ñüî•\nHey there, fellow tech enthusiast! ü§ì Ever felt like PyTorch is a bit of a puzzle, unlike its more user-friendly counterparts?","tags":["Deep Learning","Neural Networks","PyTorch"],"title":"Deep Neural Network Implementation Using PyTorch","type":"post"},{"authors":["Arman Asgharpoor Golroudbari","Mohammad H. Sabour"],"categories":null,"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://armanasq.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"This comprehensive review article surveys recent advancements in deep learning applications and methods for autonomous navigation. We provide a detailed overview of state-of-the-art deep learning frameworks for key functions in autonomous navigation, including obstacle detection, scene perception, path planning, and control. We analyze recent research studies to evaluate the implementation and testing of these methods, and provide a critical assessment of their strengths, limitations, and potential areas of growth. We also highlight interdisciplinary work related to this field, and discuss the challenges posed by environmental complexity, uncertainty, obstacles, and dynamic environments. Our review emphasizes the importance of navigation for mobile robots, autonomous cars, unmanned aerial vehicles, and space vehicles, and identifies key trends in recent research. By synthesizing findings from multiple studies, we provide a valuable resource for researchers and practitioners working in this field.","tags":["Source Themes","Deep Learning","Navigation","Inertial Sensors","Intelligent Filter","Sensor Fusion","Long-Short Term Memory","Convolutional Neural Network"],"title":"Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review","type":"publication"},{"authors":null,"categories":null,"content":"This project aimed for training and evaluating a deep learning model for predicting quaternion-based orientations using inertial measurement unit (IMU) sensor data. The model is trained using a combination of accelerometer, gyroscope, and magnetometer readings from the IMU sensors.\nEnd-to-End-Deep-Learning-Framework-for-Real-Time-Inertial-Attitude-Estimation-using-6DoF-IMU Code repo of paper Generalizable end-to-end deep learning frameworks for real-time attitude estimation using 6DoF inertial measurement units\nScienceDirect: Link\nArxiv: Link\nIMU Quaternion Prediction This repository contains code for training and evaluating a deep learning model for predicting quaternion orientations using inertial measurement unit (IMU) sensor data. The model is trained using a combination of accelerometer, gyroscope, and magnetometer readings from the IMU sensors.\nTable of Contents End-to-End-Deep-Learning-Framework-for-Real-Time-Inertial-Attitude-Estimation-using-6DoF-IMU IMU Quaternion Prediction Table of Contents Introduction Requirements Installation Usage Data Preprocessing Model Training Model Evaluation Results Contributing Citation License Introduction Inertial measurement units (IMUs) are commonly used in various applications, such as robotics, virtual reality, and motion tracking. They consist of sensors, including accelerometers, gyroscopes, and magnetometers, that provide measurements of linear acceleration, angular velocity, and magnetic field orientation, respectively. Quaternion representations are often used to represent the orientation of an IMU sensor due to their advantages over other representations.\nThe goal of this project is to develop a deep learning model that can accurately predict the orientation of an IMU sensor using the sensor‚Äôs raw data. The model takes as input the accelerometer, gyroscope, and magnetometer readings and outputs a quaternion representing the sensor‚Äôs orientation in 3D space. Accurate quaternion prediction is crucial for applications that rely on precise orientation estimation.\nRequirements To run the code in this repository, you need the following dependencies:\nPython 3.x TensorFlow 2.x Keras NumPy Pandas Matplotlib scikit-learn These dependencies can be easily installed using pip or conda.\nInstallation Clone this repository to your local machine using the following command: git clone https://github.com/your-username/imu-quaternion-prediction.git Change into the project directory: cd imu-quaternion-prediction Install the required dependencies using pip: pip install -r requirements.txt Usage The code in this repository is organized into several modules, each responsible for a specific task. Here is an overview of the main modules and their functionality:\nmodel.py: Contains the definition of the deep learning model used for quaternion prediction. It utilizes convolutional and recurrent neural network layers to learn spatial and temporal patterns from the sensor data. dataset_loader.py: Implements functions for loading and preprocessing the IMU sensor data. It handles reading data from different sources, merging the data into a single dataset, and splitting it into training and testing sets. It also performs windowing and normalization of the sensor data. learning.py: Includes functions for training and evaluating the model. It compiles the model with an appropriate optimizer and loss function, sets up callbacks for early stopping, learning rate scheduling, model checkpointing, and tensorboard logging. It also trains the model using the training dataset and evaluates its performance on the testing dataset. util.py: Provides utility functions used throughout the project, such as functions for computing quaternion error angles and visualizing sensor data. To run the code, you can use the train.py script. This script loads the IMU sensor data, preprocesses it, trains the model, and evaluates its performance. You can customize the hyperparameters and settings in the script to suit your needs.\nTo train the model, run the following command:\npython train.py Data Preprocessing The dataset_loader.py module provides functions for loading and preprocessing the IMU sensor data. The data preprocessing\nsteps include reading the sensor data from CSV files, merging the data from multiple sensors into a single dataset, splitting the data into training and testing sets, windowing the data, and normalizing the sensor readings. These preprocessing steps are essential to prepare the data for training the model.\nModel Training The model.py module contains the definition of the deep learning model used for quaternion prediction. The model architecture consists of convolutional and recurrent neural network layers, which enable the model to learn spatial and temporal patterns from the sensor data. The model is compiled with an appropriate optimizer and loss function for training.\nDuring training, the model is fed with batches of preprocessed sensor data. The model learns to map the input sensor ‚Ä¶","date":1682553600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682553600,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://armanasq.github.io/project/deep-attitude/","publishdate":"2023-04-27T00:00:00Z","relpermalink":"/project/deep-attitude/","section":"project","summary":"Generalizable end-to-end deep learning frameworks for real-time attitude estimation using 6DoF inertial measurement units","tags":["Deep Learning"],"title":"Deep Learning based Inertial Attitude Estimation","type":"project"},{"authors":["Arman Asgharpoor Golroudbari","Mohammad H. Sabour"],"categories":null,"content":" Click the Cite button above to import publication metadata. Create your slides in Markdown - click the Slides button to check out the example. Model A\n","date":1680307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680307200,"objectID":"6e69cad57e33dd7f16a97d645f90e734","permalink":"https://armanasq.github.io/publication/attitude/","publishdate":"2023-04-01T00:00:00Z","relpermalink":"/publication/attitude/","section":"publication","summary":"‚Ä¢ End-to-end learning framework for real-time inertial attitude estimation. Generalized across various sampling rates. RNN-CNN networks employed to learn motion characteristics, noise, and bias. Proposed approach outperforms traditional algorithms and other deepOutperforms traditional algorithms in terms of accuracy up to 40 Evaluated using seven datasets, totaling 120 h and 200 kilometers of IMU measurements.","tags":["Deep Learning","Attitude Estimation","IMU","Navigation"],"title":"Generalizable end-to-end deep learning frameworks for real-time attitude estimation using 6DoF inertial measurement units","type":"publication"},{"authors":null,"categories":null,"content":"\n‚áê Datasets\nIntroduction Data Format Downloading the Dataset Using the KITTI Dataset in Python Prerequisites Install the Required Libraries Load the Dataset Understanding Calibration and Timestamp Data in 3D Vision Applications Intrinsic Matrix Extrinsic Matrix Calibration Data (calib.txt): Timestamp Data (times.txt): Processing Image Data with Timestamps Depth Maps and Visual Odometry Calculating Baseline and Focal Length in Stereo Vision Baseline Calculation Focal Length Calculation Calculating Depth from Stereo Pair of Images Rectification Correspondence Matching Disparity Computation Depth Estimation Depth Refinement Math behind Stereo Depth StereoBM Vs. StereoSGBM Feature Extraction 1. Harris Corner Detection 2. Shi-Tomasi Corner Detector \u0026amp; Good Features to Track 3. Scale-Invariant Feature Transform (SIFT) 4. Speeded-Up Robust Features (SURF) 5. FAST Algorithm for Corner Detection 6. BRIEF (Binary Robust Independent Elementary Features) ORB (Oriented FAST and Rotated BRIEF) Feature Matching Descriptor Distance Measures Matching Techniques Code Example: Feature Matching with Brute-Force Matching and Euclidean Distance Point Cloud to Image Projection Visualizing LiDAR Pointcloud Estimate Camera Motion Feature Point Correspondence Camera Motion Estimation Camera Motion Representation Visual Odometry Pipeline Overview Feature-Based Visual Odometry Visual Odometry Formulation Code Example References Introduction KITTI is a popular computer vision dataset designed for autonomous driving research. It contains a diverse set of challenges for researchers, including object detection, tracking, and scene understanding. The dataset is derived from the autonomous driving platform developed by the Karlsruhe Institute of Technology and the Toyota Technological Institute at Chicago.\nThe KITTI dataset includes a collection of different sensors and modalities, such as stereo cameras, LiDAR, and GPS/INS sensors, which provides a comprehensive view of the environment around the vehicle. The data was collected over several days in the urban areas of Karlsruhe and nearby towns in Germany. The dataset includes more than 200,000 stereo images and their corresponding point clouds, as well as data from the GPS/INS sensors, which provide accurate location and pose information.\nThe dataset is divided into several different categories, each with its own set of challenges. These categories include object detection, tracking, scene understanding, visual odometry, and road/lane detection. Each category contains a set of challenges that researchers can use to evaluate their algorithms and compare their results with others in the field.\nOne of the strengths of the KITTI dataset is its accuracy and precision. The sensors used to collect the data provide a high level of detail and accuracy, making it possible to detect and track objects with high precision. Additionally, the dataset includes a large number of real-world scenarios, which makes it more representative of real-world driving conditions.\nAnother strength of the KITTI dataset is its large size. The dataset includes over 50 GB of data, which includes stereo images, point clouds, and GPS/INS data. This large amount of data makes it possible to train deep neural networks, which are known to perform well on large datasets.\nDespite its strengths, the KITTI dataset also has some limitations. For example, the dataset only covers urban driving scenarios, which may not be representative of driving conditions in other environments. Additionally, the dataset is relatively small compared to other computer vision datasets, which may limit its applicability in certain domains.\nIn summary, the KITTI dataset is a valuable resource for researchers in the field of autonomous driving. Its accuracy, precision, and large size make it an ideal dataset for evaluating and comparing algorithms for object detection, tracking, and scene understanding. While it has some limitations, its strengths make it a popular and widely used dataset in the field.\nData Format The KITTI dataset is available in two formats: raw data and preprocessed data. The raw data contains a large amount of sensor data, including images, LiDAR point clouds, and GPS/IMU measurements, and can be used for various research purposes. The preprocessed data provides more structured data, including object labels, and can be used directly for tasks such as object detection and tracking.\nDownloading the Dataset The KITTI dataset can be downloaded from the official website (http://www.cvlibs.net/datasets/kitti/) after registering with a valid email address. The website provides instructions on how to download and use the data, including the required software tools and file formats.\nUsing the KITTI Dataset in Python Prerequisites Before getting started, make sure you have the following prerequisites:\nPython 3.x installed\rNumPy and Matplotlib libraries installed\rKITTI dataset downloaded and extracted\rOpenCV: for image and video processing ‚Ä¶","date":1680307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680307200,"objectID":"4abe7ef5678df1e3f51ff93392b0b278","permalink":"https://armanasq.github.io/datsets/kitti/","publishdate":"2023-04-01T00:00:00Z","relpermalink":"/datsets/kitti/","section":"post","summary":"‚áê Datasets\nIntroduction Data Format Downloading the Dataset Using the KITTI Dataset in Python Prerequisites Install the Required Libraries Load the Dataset Understanding Calibration and Timestamp Data in 3D Vision Applications Intrinsic Matrix Extrinsic Matrix Calibration Data (calib.","tags":null,"title":"KITTI Dataset","type":"post"},{"authors":null,"categories":null,"content":"‚áê ROS Tutorial: Getting Started with ROS (Robot Operating System) Introduction What is ROS? Why ROS? Step 1: Installing ROS Step 2: Creating a ROS Workspace Step 3: Creating a ROS Package Step 4: Writing a ROS Program Conclusion Tutorial: Getting Started with ROS (Robot Operating System) Introduction Welcome to our step-by-step tutorial on getting started with ROS (Robot Operating System). ROS is an open-source framework for building robotic systems. It provides a collection of libraries, tools, and conventions to help developers create robust and modular robot applications. In this tutorial, we will guide you through the process of setting up ROS, creating a ROS workspace, and running your first ROS program. By the end of this tutorial, you will have a solid foundation for working with ROS and developing your own robotic applications.\nWhat is ROS? ROS, short for Robot Operating System, is an open-source framework designed for building robotic systems. It provides a flexible and modular architecture that enables developers to create complex robot applications by leveraging a wide range of libraries, tools, and community-contributed packages.\nROS was initially developed at Stanford University in 2007 and has since gained significant popularity in the robotics community. It has a large and active user base, which has contributed to the extensive development and refinement of its features and capabilities.\nDespite its name, ROS is not an operating system in the traditional sense. Rather, it serves as a middleware layer that runs on top of a conventional operating system (such as Linux) and provides a set of abstractions and functionalities specifically tailored for robotics.\nOne of the key strengths of ROS is its focus on collaboration and reusability. It encourages the development and sharing of reusable software components called ‚Äúpackages.‚Äù These packages encapsulate specific functionalities or algorithms, making it easier for developers to build upon existing work and leverage the collective knowledge and expertise of the ROS community.\nROS follows a distributed architecture, where different processes, called ‚Äúnodes,‚Äù communicate with each other by passing messages. This messaging system allows nodes to exchange data, commands, and sensor information in a standardized and interoperable manner. It promotes modularity and scalability, enabling developers to break down complex systems into smaller, manageable components that can be developed and tested independently.\nFurthermore, ROS provides a wide range of tools for visualization, simulation, debugging, and analysis, which greatly simplify the development and debugging process. These tools include visualization tools like RViz for visualizing robot models and sensor data, simulation environments like Gazebo for testing and evaluating robot behavior, and debugging tools like rqt_console for monitoring and analyzing the system‚Äôs log messages.\nOverall, ROS has become the de facto standard in the field of robotics due to its versatility, modularity, and active community. It has been widely adopted in both academic and industrial settings for a variety of applications, ranging from autonomous vehicles and industrial robots to medical robotics and research platforms.\nIn the next sections of this tutorial, we will guide you through the process of setting up ROS, creating a workspace, and running your first ROS program. This will provide you with a solid foundation to start developing your own robotic applications using the powerful capabilities of ROS.\nWhy ROS? ROS, or Robot Operating System, has gained significant popularity in the robotics community for several compelling reasons. Let‚Äôs explore why ROS has become the framework of choice for many roboticists and researchers:\nModularity and Reusability: ROS encourages a modular approach to software development, where functionalities are encapsulated into reusable components called packages. This modularity makes it easier to develop, test, and maintain individual components, and enables seamless integration of different software modules into a larger robotic system. The extensive package ecosystem of ROS allows developers to leverage existing solutions and build upon the work of others, saving time and effort.\nInteroperability: ROS promotes interoperability by providing a standardized messaging system for communication between different nodes. Nodes can exchange data, commands, and sensor information using ROS messages, services, and topics. This standardized communication protocol allows for the seamless integration of various hardware and software components, making it easier to build complex robotic systems with heterogeneous components.\nCommunity and Collaboration: ROS has a large and active community of developers and researchers. This vibrant community contributes to the continuous improvement and evolution of ROS through the development of new packages, bug fixes, and documentation. The ROS community also ‚Ä¶","date":1672617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672617600,"objectID":"d782ce4ff890e1892dac926dde423f0a","permalink":"https://armanasq.github.io/ros/tutorial-01/","publishdate":"2023-01-02T00:00:00Z","relpermalink":"/ros/tutorial-01/","section":"post","summary":"‚áê ROS Tutorial: Getting Started with ROS (Robot Operating System) Introduction What is ROS? Why ROS? Step 1: Installing ROS Step 2: Creating a ROS Workspace Step 3: Creating a ROS Package Step 4: Writing a ROS Program Conclusion Tutorial: Getting Started with ROS (Robot Operating System) Introduction Welcome to our step-by-step tutorial on getting started with ROS (Robot Operating System).","tags":["ROS","Tutorial"],"title":"Getting Started with ROS (Robot Operating System)","type":"post"},{"authors":null,"categories":null,"content":"‚áê ROS ROS Tutorial 1: Basic Concepts In this tutorial, we‚Äôll introduce you to the fundamental concepts of the Robot Operating System (ROS) without diving into complex jargon. We‚Äôll start from scratch and gradually build our understanding of ROS.\nWhat is ROS? ROS, the Robot Operating System, is a powerful framework for building robot software. It simplifies the development of robotic applications by providing a structured way to create, manage, and connect software components. ROS is widely adopted in the robotics community due to its open-source nature and rich ecosystem.\nKey ROS Concepts Nodes ROS is designed as a distributed system of nodes. Think of nodes as small software modules that perform specific tasks, such as reading sensor data, processing information, or controlling motors. Each node communicates with others by sending and receiving messages through topics.\nTopics Topics are named channels through which nodes exchange data. A node can publish data to a topic, and other nodes can subscribe to that topic to receive the data. Topics enable modular and decoupled communication between nodes.\nPublishers and Subscribers Publisher: A node that sends data (messages) on a topic is a publisher. Publishers broadcast information to anyone interested in that topic.\nSubscriber: A node that receives data (messages) from a topic is a subscriber. Subscribers listen to topics and react to the data they receive.\nSetting up the ROS Environment Let‚Äôs get started with ROS by setting up a basic development environment.\nInstallation: If you haven‚Äôt already installed ROS, follow the installation instructions for your specific platform on the official ROS website.\nInitialize ROS: After installation, initialize ROS in your current terminal session:\n$ source /opt/ros/your-ros-version/setup.bash Replace your-ros-version with the ROS version you‚Äôve installed.\nCreate a Workspace: Create a workspace to organize your ROS projects:\n$ mkdir -p ~/ros_workspace/src $ cd ~/ros_workspace/src $ catkin_init_workspace $ cd ~/ros_workspace $ catkin_make Creating Your First ROS Package ROS organizes code into packages. Let‚Äôs create a simple package named my_first_package:\n$ cd ~/ros_workspace/src $ catkin_create_pkg my_first_package rospy Here, we‚Äôre creating a Python-based ROS package (rospy) called my_first_package. This package will use Python for programming.\nWriting a Simple ROS Node Now, let‚Äôs create a basic ROS node that publishes a message to a topic. Create a Python script, e.g., simple_publisher.py, inside the my_first_package folder:\n#!/usr/bin/env python import rospy from std_msgs.msg import String def main(): rospy.init_node(\u0026#39;simple_publisher\u0026#39;) pub = rospy.Publisher(\u0026#39;my_topic\u0026#39;, String, queue_size=10) rate = rospy.Rate(1) while not rospy.is_shutdown(): message = \u0026#34;Hello, ROS!\u0026#34; pub.publish(message) rate.sleep() if __name__ == \u0026#39;__main__\u0026#39;: try: main() except rospy.ROSInterruptException: pass In this script, we:\nImport necessary libraries. Initialize the ROS node. Create a publisher on the topic my_topic. Continuously publish the message ‚ÄúHello, ROS!‚Äù at a rate of 1 Hz. Running the ROS Node To run the ROS node, open a terminal and navigate to your workspace:\n$ cd ~/ros_workspace Build your workspace:\n$ catkin_make Now, you can run the ROS node:\n$ rosrun my_first_package simple_publisher.py Checking Published Data To verify that your node is publishing messages correctly, open another terminal and use the rostopic echo command:\n$ rostopic echo my_topic You should see the ‚ÄúHello, ROS!‚Äù message being displayed.\nConclusion Congratulations! You‚Äôve created your first ROS package and node while gaining a better understanding of ROS basics. In the next tutorial, we‚Äôll explore more advanced topics like creating custom messages and building more complex robot behaviors. Stay tuned for more ROS adventures!\n","date":1672617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672617600,"objectID":"1606f0680b2d28cb1387e4eddc64411e","permalink":"https://armanasq.github.io/ros/tutorial-01/","publishdate":"2023-01-02T00:00:00Z","relpermalink":"/ros/tutorial-01/","section":"post","summary":"‚áê ROS ROS Tutorial 1: Basic Concepts In this tutorial, we‚Äôll introduce you to the fundamental concepts of the Robot Operating System (ROS) without diving into complex jargon. We‚Äôll start from scratch and gradually build our understanding of ROS.","tags":["ROS","Tutorial"],"title":"ROS Tutorial 1: Basic Concepts ","type":"post"},{"authors":null,"categories":null,"content":"‚áê Research ROS Tutorial 1: Basic Concepts In this tutorial, we‚Äôll introduce you to the fundamental concepts of the Robot Operating System (ROS) without diving into complex jargon. We‚Äôll start from scratch and gradually build our understanding of ROS.\nWhat is ROS? ROS, the Robot Operating System, is a powerful framework for building robot software. It simplifies the development of robotic applications by providing a structured way to create, manage, and connect software components. ROS is widely adopted in the robotics community due to its open-source nature and rich ecosystem.\nKey ROS Concepts Nodes ROS is designed as a distributed system of nodes. Think of nodes as small software modules that perform specific tasks, such as reading sensor data, processing information, or controlling motors. Each node communicates with others by sending and receiving messages through topics.\nTopics Topics are named channels through which nodes exchange data. A node can publish data to a topic, and other nodes can subscribe to that topic to receive the data. Topics enable modular and decoupled communication between nodes.\nPublishers and Subscribers Publisher: A node that sends data (messages) on a topic is a publisher. Publishers broadcast information to anyone interested in that topic.\nSubscriber: A node that receives data (messages) from a topic is a subscriber. Subscribers listen to topics and react to the data they receive.\nSetting up the ROS Environment Let‚Äôs get started with ROS by setting up a basic development environment.\nInstallation: If you haven‚Äôt already installed ROS, follow the installation instructions for your specific platform on the official ROS website.\nInitialize ROS: After installation, initialize ROS in your current terminal session:\n$ source /opt/ros/your-ros-version/setup.bash Replace your-ros-version with the ROS version you‚Äôve installed.\nCreate a Workspace: Create a workspace to organize your ROS projects:\n$ mkdir -p ~/ros_workspace/src $ cd ~/ros_workspace/src $ catkin_init_workspace $ cd ~/ros_workspace $ catkin_make Creating Your First ROS Package ROS organizes code into packages. Let‚Äôs create a simple package named my_first_package:\n$ cd ~/ros_workspace/src $ catkin_create_pkg my_first_package rospy Here, we‚Äôre creating a Python-based ROS package (rospy) called my_first_package. This package will use Python for programming.\nWriting a Simple ROS Node Now, let‚Äôs create a basic ROS node that publishes a message to a topic. Create a Python script, e.g., simple_publisher.py, inside the my_first_package folder:\n#!/usr/bin/env python import rospy from std_msgs.msg import String def main(): rospy.init_node(\u0026#39;simple_publisher\u0026#39;) pub = rospy.Publisher(\u0026#39;my_topic\u0026#39;, String, queue_size=10) rate = rospy.Rate(1) while not rospy.is_shutdown(): message = \u0026#34;Hello, ROS!\u0026#34; pub.publish(message) rate.sleep() if __name__ == \u0026#39;__main__\u0026#39;: try: main() except rospy.ROSInterruptException: pass In this script, we:\nImport necessary libraries. Initialize the ROS node. Create a publisher on the topic my_topic. Continuously publish the message ‚ÄúHello, ROS!‚Äù at a rate of 1 Hz. Running the ROS Node To run the ROS node, open a terminal and navigate to your workspace:\n$ cd ~/ros_workspace Build your workspace:\n$ catkin_make Now, you can run the ROS node:\n$ rosrun my_first_package simple_publisher.py Checking Published Data To verify that your node is publishing messages correctly, open another terminal and use the rostopic echo command:\n$ rostopic echo my_topic You should see the ‚ÄúHello, ROS!‚Äù message being displayed.\nConclusion Congratulations! You‚Äôve created your first ROS package and node while gaining a better understanding of ROS basics. In the next tutorial, we‚Äôll explore more advanced topics like creating custom messages and building more complex robot behaviors. Stay tuned for more ROS adventures!\n","date":1672617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672617600,"objectID":"98fabfe51caa86d7f8653f8c1388280f","permalink":"https://armanasq.github.io/research/SysRev-PRISMA/","publishdate":"2023-01-02T00:00:00Z","relpermalink":"/research/SysRev-PRISMA/","section":"research","summary":"Systematic Review based on the PRISMA Guideline","tags":["Systematic Review","Meta Analysis","PRISMA"],"title":"Systematic Reviews: A Comprehensive Guide with PRISMA Guideline","type":"research"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 11: Kaggle Competitions: Model Stacking and Ensemble Techniques Introduction Step 1: Building Base Models Step 2: Building the Stacked Model Step 3: Building Ensemble Models Step 4: Fine-tuning and Validation Step 5: Model Blending Conclusion Tutorial 11: Kaggle Competitions: Model Stacking and Ensemble Techniques Introduction Welcome to Tutorial 11 of our Kaggle series! In this tutorial, we will explore advanced techniques for improving your performance in Kaggle competitions. Specifically, we will focus on model stacking and ensemble techniques, which involve combining the predictions of multiple models to create a more robust and accurate final prediction. Model stacking and ensembling are widely used strategies in data science competitions to achieve higher accuracy and better generalization. In this tutorial, we will walk through the process of building stacked models and ensembles, including the necessary code and techniques to implement them effectively. Let‚Äôs get started!\nStep 1: Building Base Models The first step in model stacking and ensembling is to build a set of diverse base models. These base models can be different machine learning algorithms or variations of the same algorithm with different hyperparameters. Follow these steps to build your base models:\nSelect Algorithms: Choose a variety of machine learning algorithms that complement each other. For example, you can include algorithms like Random Forest, Gradient Boosting, Support Vector Machines, and Neural Networks. Train Base Models: Train each base model on your training dataset using cross-validation or any other appropriate technique. Optimize the hyperparameters for each model to achieve the best performance. Generate Predictions: Use the trained base models to generate predictions for the validation dataset. These predictions will be used as input for the next step of model stacking. Step 2: Building the Stacked Model The next step is to build the stacked model using the predictions generated by the base models. Follow these steps to create your stacked model:\nPrepare Stacking Data: Create a new dataset using the predictions from the base models as features. Each prediction from the base models will be a new feature in the stacking dataset. Split Stacking Data: Split the stacking dataset into a training set and a holdout set. The training set will be used to train the stacked model, while the holdout set will be used for evaluation. Train Stacked Model: Train a meta-model (e.g., a simple linear regression or a neural network) on the training set of the stacking dataset. This meta-model will learn to combine the predictions from the base models to make the final prediction. Evaluate Stacked Model: Use the holdout set of the stacking dataset to evaluate the performance of the stacked model. Calculate appropriate evaluation metrics to assess its accuracy and generalization. Step 3: Building Ensemble Models In addition to stacked models, ensembling is another powerful technique to improve model performance. Follow these steps to build ensemble models:\nSelect Ensemble Algorithms: Choose ensemble algorithms such as Bagging, Boosting, or Voting. These algorithms combine the predictions of multiple models using different aggregation techniques. Train Ensemble Models: Train each ensemble model using the training dataset. Each ensemble model will incorporate the predictions from different base models or stacked models. Generate Ensemble Predictions: Use the trained ensemble models to generate predictions for the validation dataset or test dataset. Combine the predictions using the appropriate ensemble aggregation technique (e.g., averaging, weighted averaging, or majority voting). Evaluate Ensemble Models: Evaluate the performance of the ensemble models using appropriate evaluation metrics. Compare the results with the individual base models or stacked models to assess the improvement achieved through ensembling. Step 4: Fine-tuning and Validation After building the stacked models and ensemble models, it‚Äôs essential to fine-tune them and validate their performance. Follow these steps to fine-tune and validate your models:\nHyperparameter Tuning: Experiment with different hyperparameters for the base models, stacked models , and ensemble models. Use techniques like grid search or random search to find the optimal hyperparameters that maximize performance. 2. Cross-Validation: Validate the performance of your models using cross-validation on the training dataset. This helps estimate the generalization performance of your models and provides insights into their stability and variance. 3. Model Selection: Based on the cross-validation results, select the best-performing models for each category (base models, stacked models, and ensemble models). Consider both accuracy and computational efficiency when making your selection.\nStep 5: Model Blending Model blending is another technique used to improve model performance. Follow these ‚Ä¶","date":1671926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671926400,"objectID":"b9f804eb45f9133bb2d66c5cd5968a58","permalink":"https://armanasq.github.io/kaggle/tutorial-11/","publishdate":"2022-12-25T00:00:00Z","relpermalink":"/kaggle/tutorial-11/","section":"post","summary":"‚áê Kaggle\nTutorial 11: Kaggle Competitions: Model Stacking and Ensemble Techniques Introduction Step 1: Building Base Models Step 2: Building the Stacked Model Step 3: Building Ensemble Models Step 4: Fine-tuning and Validation Step 5: Model Blending Conclusion Tutorial 11: Kaggle Competitions: Model Stacking and Ensemble Techniques Introduction Welcome to Tutorial 11 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 11","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 10: Deploying Machine Learning Models on Kaggle Introduction Step 1: Preparing the Model Step 2: Creating a Web Application Step 3: Sharing the Web Application Conclusion Tutorial 10: Deploying Machine Learning Models on Kaggle Introduction Welcome to Tutorial 10 of our Kaggle series! In this tutorial, we will explore the process of deploying machine learning models on Kaggle. Deploying a model involves making it accessible and usable for others to interact with and obtain predictions. Kaggle provides a platform that allows you to deploy your models and create web applications that can be accessed by users. In this tutorial, we will cover the steps to deploy a machine learning model on Kaggle, including preparing the model, creating a web application, and sharing it with others. Let‚Äôs get started and learn how to deploy your models on Kaggle!\nStep 1: Preparing the Model Before deploying a machine learning model on Kaggle, you need to ensure that your model is trained, saved, and ready to be used for making predictions. Follow these steps to prepare your model:\nTrain and Evaluate the Model: Train your machine learning model using the appropriate dataset. Evaluate its performance and ensure that it meets your desired criteria. Save the Model: Once your model is trained and evaluated, save it in a format that can be easily loaded and used for making predictions. Common formats include serialized models (e.g., pickle, joblib) or model files (e.g., .h5 for TensorFlow models). Prepare Dependencies: Take note of any external dependencies or libraries that your model requires to run. Make sure to include these dependencies in the deployment process to ensure the smooth functioning of the model. Step 2: Creating a Web Application Kaggle provides a platform called ‚ÄúKaggle Kernels‚Äù that allows you to create and deploy web applications for your machine learning models. Follow these steps to create a web application using Kaggle Kernels:\nCreate a New Kernel: Log in to Kaggle and navigate to the ‚ÄúKernels‚Äù section. Click on the ‚ÄúNew Notebook‚Äù button to create a new kernel. Choose a Template: Select a kernel template that suits your needs. For a web application, you can choose a template that supports web frameworks like Flask or Django. Import Dependencies: Import the necessary libraries and dependencies required for your web application. This may include frameworks like Flask or Django, as well as any libraries specific to your model. Load the Model: Load the saved machine learning model into your kernel. This typically involves loading the serialized model file or using the appropriate functions to restore the model. Define Web Routes: Define the routes and endpoints for your web application. This includes specifying the URL paths and the corresponding functions that handle the requests. Create HTML Templates: Create HTML templates that define the structure and layout of your web application. These templates can be used to display the input forms and the prediction results. Implement Prediction Logic: Write the code that uses the loaded model to make predictions based on the user input. This may involve processing the user input, performing any necessary data transformations, and feeding the input to the model. Run the Web Application: Once you have implemented the necessary code, run the web application within the kernel to ensure that it functions as expected. Step 3: Sharing the Web Application After creating and testing your web application, you can share it with others on Kaggle. Follow these steps to share your deployed machine learning model:\nPublish the Kernel: Once your web application is ready to be shared, publish the kernel by clicking on the ‚ÄúPublish‚Äù button. This makes your kernel accessible to others on Kaggle. Provide Instructions: In the kernel description or as comments within the code, provide clear instructions on how to use your web application. Explain the expected input format, any constraints or limitations, and how to interpret the prediction results. 3. Include Example Input: Consider including example input data in the kernel to demonstrate how the web application works. This helps users understand the expected input format and facilitates testing. 4. Engage with Users: Be active in the comments section of your kernel. Answer any questions, provide clarifications, and gather feedback from users. This interaction helps improve your web application and fosters a sense of community.\nConclusion Congratulations on completing Tutorial 10: Deploying Machine Learning Models on Kaggle! You have learned how to prepare your machine learning model for deployment, create a web application using Kaggle Kernels, and share your deployed model with others. Deploying models on Kaggle allows you to showcase your work, receive feedback, and collaborate with the data science community. Use this knowledge to make your machine learning models accessible and interactable, and continue to explore the ‚Ä¶","date":1671667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671667200,"objectID":"8d11638be12026c23315a6a865cbbfa2","permalink":"https://armanasq.github.io/kaggle/tutorial-10/","publishdate":"2022-12-22T00:00:00Z","relpermalink":"/kaggle/tutorial-10/","section":"post","summary":"‚áê Kaggle\nTutorial 10: Deploying Machine Learning Models on Kaggle Introduction Step 1: Preparing the Model Step 2: Creating a Web Application Step 3: Sharing the Web Application Conclusion Tutorial 10: Deploying Machine Learning Models on Kaggle Introduction Welcome to Tutorial 10 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 10","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 9: Kaggle for Data Science Learning Introduction Step 1: Kaggle Learn Step 2: Kaggle Notebooks Step 3: Kaggle Competitions Step 4: Kaggle Datasets Step 5: Kaggle Discussion Forums Conclusion Tutorial 9: Kaggle for Data Science Learning Introduction Welcome to Tutorial 9 of our Kaggle series! In this tutorial, we will explore how to leverage Kaggle as a powerful platform for data science learning. Kaggle provides a vast array of resources, competitions, datasets, and community interactions that can help you enhance your data science skills, gain practical experience, and stay updated with the latest trends in the field. Whether you are a beginner looking to learn the basics or an experienced data scientist seeking to expand your knowledge, Kaggle has something to offer. In this tutorial, we will cover various learning opportunities on Kaggle, including courses, tutorials, Kaggle Learn, Kaggle Notebooks, and more. Let‚Äôs dive in and unlock the learning potential of Kaggle!\nStep 1: Kaggle Learn Kaggle Learn is a collection of interactive courses that cover various data science topics and skills. It offers a structured learning path with hands-on exercises and real-world applications. Follow these steps to make the most of Kaggle Learn:\nExplore the Course Catalog: Visit the Kaggle Learn website and explore the course catalog. Each course is designed to provide in-depth knowledge on a specific data science topic, such as Python, SQL, machine learning, deep learning, and more. Choose a course that aligns with your learning goals and interests. Enroll in a Course: Enroll in the course of your choice by clicking on its title. This allows you to track your progress, save your work, and earn completion certificates. Complete the Lessons and Exercises: Work through the lessons and exercises in each course. The interactive coding environment allows you to practice the concepts directly and receive immediate feedback. Apply Your Knowledge: Apply the concepts you learn in Kaggle Learn to real-world projects and competitions. This reinforces your understanding and helps you build practical data science skills. Step 2: Kaggle Notebooks Kaggle Notebooks are a valuable resource for learning and sharing data science knowledge. Notebooks provide a collaborative environment where you can write, run, and share code, visualizations, and explanations. Follow these steps to benefit from Kaggle Notebooks:\nExplore Notebooks: Visit the Kaggle Notebooks section and browse through a wide range of notebooks contributed by the community. You can filter notebooks by topics, tags, popularity, and more. Read and Analyze Notebooks: Open notebooks that interest you and study the code, visualizations, and explanations. Understand the techniques used, the data preprocessing steps, and the insights derived from the analysis. Clone and Modify Notebooks: Clone notebooks that you find useful and modify them to solve similar problems or explore different datasets. This allows you to practice and adapt the techniques learned to your own projects. Share Your Notebooks: Once you feel comfortable, share your own notebooks with the community. Provide clear explanations, document your code, and showcase your data science skills. This not only helps others learn but also establishes you as a knowledgeable contributor in the field. Step 3: Kaggle Competitions Participating in Kaggle competitions is an excellent way to enhance your data science skills and learn from real-world problems. Follow these steps to make the most of Kaggle competitions:\nExplore Competitions: Visit the Kaggle Competitions page and explore the ongoing and past competitions. Read the competition descriptions, data descriptions, and evaluation metrics to understand the problem and the dataset. Join Competitions: Join competitions that align with your interests and skill level. Start with beginner-friendly competitions and gradually challenge yourself with more advanced ones. Learn from Kernels: Study the kernels shared by other participants in the competition. Kernels are code notebooks that showcase different approaches, algorithms, and techniques used by participants. Analyze the top-performing kernels to gain insights into winning strategies. 4. Experiment and Iterate: Develop your own models and techniques based on what you learn from the competition and the kernels. Experiment with different algorithms, feature engineering methods, and model architectures. Continuously iterate and improve your solution based on the feedback you receive from the competition leaderboard. 5. Engage with the Community: Participate in competition forums and discussions. Ask questions, seek advice, and contribute your insights. Engaging with the community not only helps you learn but also expands your professional network.\nStep 4: Kaggle Datasets Kaggle Datasets provide a wealth of resources for learning and exploring different datasets. Follow these steps to leverage Kaggle Datasets for ‚Ä¶","date":1670630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670630400,"objectID":"405bef09b3fec69617ec35cb3f335b04","permalink":"https://armanasq.github.io/kaggle/tutorial-09/","publishdate":"2022-12-10T00:00:00Z","relpermalink":"/kaggle/tutorial-09/","section":"post","summary":"‚áê Kaggle\nTutorial 9: Kaggle for Data Science Learning Introduction Step 1: Kaggle Learn Step 2: Kaggle Notebooks Step 3: Kaggle Competitions Step 4: Kaggle Datasets Step 5: Kaggle Discussion Forums Conclusion Tutorial 9: Kaggle for Data Science Learning Introduction Welcome to Tutorial 9 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 9","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 8: Kaggle Career and Networking Introduction Step 1: Optimizing Your Kaggle Profile Step 2: Collaborating and Networking on Kaggle Step 3: Sharing Knowledge and Contributing to the Community Step 4: Showcasing Your Work and Achievements Step 5: Leveraging Kaggle for Career Opportunities Conclusion Tutorial 8: Kaggle Career and Networking Introduction Welcome to Tutorial 8 of our Kaggle series! In this tutorial, we will explore the career and networking aspects of Kaggle. Kaggle is not only a platform for data science competitions and projects but also a vibrant community of data enthusiasts and professionals. In this tutorial, we will discuss how you can leverage Kaggle to boost your data science career, expand your professional network, and create valuable connections in the industry. We will cover profile optimization, collaboration opportunities, knowledge sharing, and more. By the end of this tutorial, you will have a solid understanding of how to navigate Kaggle for career growth and networking success. Let‚Äôs dive in!\nStep 1: Optimizing Your Kaggle Profile Your Kaggle profile is your professional identity within the Kaggle community. It‚Äôs essential to optimize your profile to showcase your skills, achievements, and expertise. Consider the following steps:\nComplete Your Profile: Fill out all the relevant sections of your Kaggle profile, including your bio, profile picture, location, and social media links. This helps others learn more about you and connect with you. Highlight Your Skills and Expertise: Clearly list your data science skills, programming languages, tools, and techniques in your profile. This allows others to understand your areas of expertise and potentially collaborate with you. Showcase Kaggle Competitions and Projects: Highlight the Kaggle competitions you have participated in and any notable achievements or rankings. Display your Kaggle competition medals and provide brief descriptions of your projects. Link to Your External Work: If you have a personal website, blog, or GitHub repository, include links to them in your profile. This demonstrates your commitment to learning and sharing in the data science community. Step 2: Collaborating and Networking on Kaggle Kaggle provides ample opportunities for collaboration and networking with fellow data enthusiasts and professionals. Consider the following strategies:\nJoin Kaggle Discussions: Participate in Kaggle Discussions by asking questions, providing answers, and engaging in conversations. This helps you connect with like-minded individuals, learn from others, and build your reputation in the community. Join Kaggle Teams: Kaggle allows you to form or join teams for competitions. Collaborating with team members not only enhances your chances of success but also exposes you to different perspectives and techniques. Participate in Kaggle Notebooks: Explore and contribute to the Kaggle Notebooks section. Share your data analyses, models, and visualizations with the community. Provide insights, explanations, and helpful code comments to showcase your knowledge and expertise. Attend Kaggle Meetups and Events: Keep an eye out for Kaggle meetups, webinars, and virtual events. Participate in these events to network with professionals, learn from experts, and gain insights into the latest trends in data science. Follow and Connect with Influencers: Identify influential Kaggle users, data scientists, and industry experts. Follow their profiles, read their work, and engage with their content. This can lead to valuable connections and opportunities. Step 3: Sharing Knowledge and Contributing to the Community Sharing your knowledge and contributing to the Kaggle community is a great way to establish yourself as an authority and create meaningful connections. Consider the following approaches:\nPublish Kaggle Notebooks: Publish high-quality Kaggle Notebooks that showcase your data analysis, modeling techniques, and insights. Use Markdown cells to provide clear explanations and share your thought process. Write Kaggle Blog Posts: Kaggle allows you to write blog posts on the platform. Share your experiences, lessons learned, and insights gained from competitions or data science projects. Write informative and engaging content to attract readers and initiate discussions. 3. Contribute to Open Source Projects: Kaggle hosts various open source projects related to data science. Contribute to these projects by submitting code improvements, bug fixes, or documentation updates. This demonstrates your commitment to collaborative work and helps you connect with other contributors. 4. Participate in Kaggle Datasets: Kaggle provides datasets for the community to explore and analyze. Contribute by sharing your own datasets or by improving existing ones. This fosters collaboration and knowledge sharing among data enthusiasts.\nStep 4: Showcasing Your Work and Achievements It‚Äôs essential to showcase your work and achievements on Kaggle and beyond. ‚Ä¶","date":1668729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668729600,"objectID":"9a824951ba365e7fb06df9626ce7f9d0","permalink":"https://armanasq.github.io/kaggle/tutorial-08/","publishdate":"2022-11-18T00:00:00Z","relpermalink":"/kaggle/tutorial-08/","section":"post","summary":"‚áê Kaggle\nTutorial 8: Kaggle Career and Networking Introduction Step 1: Optimizing Your Kaggle Profile Step 2: Collaborating and Networking on Kaggle Step 3: Sharing Knowledge and Contributing to the Community Step 4: Showcasing Your Work and Achievements Step 5: Leveraging Kaggle for Career Opportunities Conclusion Tutorial 8: Kaggle Career and Networking Introduction Welcome to Tutorial 8 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 8","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 7: Kaggle Competitions: Winning Strategies Introduction Step 1: Understand the Problem and Metrics Step 2: Exploratory Data Analysis (EDA) Step 3: Data Preprocessing and Feature Engineering Step 4: Model Selection and Training Step 5: Validation and Evaluation Step 6: Hyperparameter Tuning and Model Refinement Step 7: Advanced Techniques and Strategies Conclusion Tutorial 7: Kaggle Competitions: Winning Strategies Introduction Welcome to Tutorial 7 of our Kaggle series! In this tutorial, we will explore winning strategies for Kaggle competitions. Kaggle competitions are a great way to test your data science skills and learn from the best. In this tutorial, we will delve into the techniques and strategies used by top Kaggle competitors to achieve high rankings. We will cover data preprocessing, feature engineering, model selection, ensemble methods, and more. By the end of this tutorial, you will have a solid understanding of winning strategies and be ready to tackle Kaggle competitions with confidence. Let‚Äôs get started!\nStep 1: Understand the Problem and Metrics Before diving into the competition, it‚Äôs crucial to thoroughly understand the problem statement and the evaluation metric. Read the competition‚Äôs overview, data description, and evaluation page carefully. Make sure you understand the task, the input features, the target variable, and how the submissions are evaluated. Familiarize yourself with the evaluation metric and consider its implications when designing your models.\nStep 2: Exploratory Data Analysis (EDA) Performing exploratory data analysis helps you gain insights into the data and identify patterns or relationships. Here are some key steps to follow during EDA:\nLoad the Data: Load the competition data into your preferred data analysis tool, such as Python‚Äôs pandas library. Explore the Data: Analyze the data‚Äôs structure, summary statistics, and distributions. Identify missing values, outliers, or inconsistencies. Visualize the Data: Create visualizations to understand the data better. Use histograms, scatter plots, box plots, and correlation matrices to identify relationships and potential feature engineering opportunities. Step 3: Data Preprocessing and Feature Engineering Data preprocessing and feature engineering play a vital role in improving model performance. Consider the following techniques:\nHandling Missing Values: Decide on an appropriate strategy for handling missing values, such as imputation, deletion, or treating missing values as a separate category. Dealing with Outliers: Identify and handle outliers in your data. Depending on the problem, you can remove outliers, cap or floor extreme values, or transform skewed distributions. Feature Scaling: Normalize or standardize numerical features to ensure they have a similar scale and distribution. Common techniques include min-max scaling and z-score normalization. Feature Encoding: Encode categorical variables using techniques such as one-hot encoding, label encoding, or target encoding. Feature Creation: Create new features from existing ones using techniques like polynomial features, interaction terms, or domain-specific transformations. Dimensionality Reduction: If your dataset has a high number of features, consider applying dimensionality reduction techniques such as principal component analysis (PCA) or feature selection methods to reduce the number of variables. Step 4: Model Selection and Training Selecting the right model or ensemble of models is crucial for competition success. Consider the following steps:\nChoose a Baseline Model: Start with a simple and interpretable model as your baseline, such as logistic regression or decision trees. This helps establish a benchmark for model performance. Explore Different Algorithms: Experiment with various algorithms suitable for the problem, such as random forests, gradient boosting, support vector machines, or neural networks. Tune hyperparameters to optimize model performance. Ensemble Methods: Combine predictions from multiple models using ensemble methods like stacking, bagging, or boosting. Ensemble methods can often improve performance by capturing diverse perspectives. Cross-Validation: Use cross-validation techniques to estimate your model‚Äôs performance on unseen data. This helps identify potential issues like overfitting and guides model selection. Optimize and Fine-Tune: Continuously iterate and improve your models by fine-tuning hyperparameters, applying regularization techniques, and exploring advanced optimization algorithms.\nStep 5: Validation and Evaluation Validate your models using appropriate techniques and evaluate their performance. Consider the following steps:\nSplit the Data: Split your training data into training and validation sets. The validation set helps you evaluate model performance and make adjustments. Validate with Cross-Validation: Implement cross-validation to get a more reliable estimate of your model‚Äôs performance. Choose an ‚Ä¶","date":1668470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668470400,"objectID":"0d0123f1eb8e8439cbc69117d3f37606","permalink":"https://armanasq.github.io/kaggle/tutorial-07/","publishdate":"2022-11-15T00:00:00Z","relpermalink":"/kaggle/tutorial-07/","section":"post","summary":"‚áê Kaggle\nTutorial 7: Kaggle Competitions: Winning Strategies Introduction Step 1: Understand the Problem and Metrics Step 2: Exploratory Data Analysis (EDA) Step 3: Data Preprocessing and Feature Engineering Step 4: Model Selection and Training Step 5: Validation and Evaluation Step 6: Hyperparameter Tuning and Model Refinement Step 7: Advanced Techniques and Strategies Conclusion Tutorial 7: Kaggle Competitions: Winning Strategies Introduction Welcome to Tutorial 7 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 7","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 6: Kaggle API and Automation Introduction Step 1: Installing the Kaggle API Step 2: Authenticating the Kaggle API Step 3: Using the Kaggle API Downloading a Dataset Submitting to a Competition Creating a New Competition Listing Competitions Step 4: Automating Tasks with Kaggle API Conclusion Tutorial 6: Kaggle API and Automation Introduction Welcome to Tutorial 6 of our Kaggle series! In this tutorial, we will explore the Kaggle API and how to automate various tasks on Kaggle. The Kaggle API allows you to interact with Kaggle programmatically, enabling you to automate repetitive tasks, access datasets, submit competition entries, and more. In this tutorial, we will cover the basics of the Kaggle API, its installation, authentication, and demonstrate how to use it to automate common tasks. Let‚Äôs get started!\nStep 1: Installing the Kaggle API Before using the Kaggle API, you need to install it on your machine. Follow these steps to install the Kaggle API:\nInstall Python: Ensure that Python is installed on your machine. You can download Python from the official website (https://www.python.org/downloads/). Install the Kaggle Package: Open your terminal or command prompt and run the following command: pip install kaggle This will install the Kaggle package on your system. Step 2: Authenticating the Kaggle API To access Kaggle datasets and competitions, you need to authenticate the Kaggle API using your Kaggle account credentials. Follow these steps to authenticate the Kaggle API:\nDownload Kaggle API Credentials: Log in to your Kaggle account and navigate to your account settings. Scroll down to the API section and click on the ‚ÄúCreate New API Token‚Äù button. This will download a file named kaggle.json. Place the Credentials File: Move the downloaded kaggle.json file to the appropriate location based on your operating system: Windows: C:\\Users\\{username}\\.kaggle\\kaggle.json macOS/Linux: /Users/{username}/.kaggle/kaggle.json Set Environment Variables: Open your terminal or command prompt and set the KAGGLE_USERNAME and KAGGLE_KEY environment variables using the following commands: Windows: set KAGGLE_USERNAME=your_kaggle_username set KAGGLE_KEY=your_kaggle_key macOS/Linux: export KAGGLE_USERNAME=your_kaggle_username export KAGGLE_KEY=your_kaggle_key Step 3: Using the Kaggle API Once you have installed and authenticated the Kaggle API, you can start using it to automate various tasks on Kaggle. Here are some examples:\nDownloading a Dataset To download a dataset from Kaggle, you can use the following command:\nimport kaggle # Download a dataset kaggle.api.dataset_download_files(\u0026#39;username/dataset-name\u0026#39;, path=\u0026#39;destination-folder\u0026#39;, unzip=True) Submitting to a Competition To submit your predictions to a Kaggle competition, you can use the following command:\nimport kaggle # Submit to a competition kaggle.api.competition_submit(file_name=\u0026#39;submission.csv\u0026#39;, message=\u0026#39;My submission message\u0026#39;, competition=\u0026#39;competition-name\u0026#39;) Creating a New Competition To create a new Kaggle competition, you can use the following command:\nimport kaggle # Create a new competition kaggle.api.competition_create(file_name=\u0026#39;competition-dataset.zip\u0026#39;, title=\u0026#39;Competition Title\u0026#39;, category=\u0026#39;category-name\u0026#39;, description=\u0026#39;Competition description\u0026#39;, enable_gpu=False, team_count=1) Listing Competitions To retrieve a list of Kaggle competitions, you can use the following command:\nimport kag gle # List competitions competitions = kaggle.api.competitions_list() for competition in competitions: print(competition[\u0026#39;title\u0026#39;]) Step 4: Automating Tasks with Kaggle API With the Kaggle API, you can automate repetitive tasks and schedule them to run at specific intervals. Here‚Äôs an example of how to automate the download of a dataset using a Python script and the cron job scheduler:\nCreate a Python Script: Create a Python script that downloads the dataset using the Kaggle API. Save the script with a descriptive name, such as download_dataset.py. Add Kaggle API Code: In your Python script, add the necessary code to download the dataset using the Kaggle API, as shown in the previous section. Schedule the Script: Use the cron job scheduler (on Unix-like systems) or the Task Scheduler (on Windows) to schedule the execution of the Python script at the desired interval. Conclusion Congratulations on completing Tutorial 6: Kaggle API and Automation! You have learned how to install and authenticate the Kaggle API, use it to automate tasks such as downloading datasets and submitting competition entries, and even create a new competition. Automation can save you time and effort, allowing you to focus on more critical aspects of your data science projects. Use the Kaggle API to streamline your workflows and explore the vast opportunities it offers for automation. In the next tutorial, we will dive into advanced data visualization techniques to enhance your data analysis and storytelling. Stay tuned and keep up the great work!\n","date":1668038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668038400,"objectID":"d9c1ac20099010e5cf3fff771a48a633","permalink":"https://armanasq.github.io/kaggle/tutorial-06/","publishdate":"2022-11-10T00:00:00Z","relpermalink":"/kaggle/tutorial-06/","section":"post","summary":"‚áê Kaggle\nTutorial 6: Kaggle API and Automation Introduction Step 1: Installing the Kaggle API Step 2: Authenticating the Kaggle API Step 3: Using the Kaggle API Downloading a Dataset Submitting to a Competition Creating a New Competition Listing Competitions Step 4: Automating Tasks with Kaggle API Conclusion Tutorial 6: Kaggle API and Automation Introduction Welcome to Tutorial 6 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 6","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 5: Collaborating and Sharing on Kaggle Introduction Step 1: Joining Kaggle Competitions Step 2: Collaborating on Kaggle Notebooks Step 3: Participating in Discussions and Forums Step 4: Sharing Datasets on Kaggle Conclusion Tutorial 5: Collaborating and Sharing on Kaggle Introduction Welcome to Tutorial 5 of our Kaggle series! In this tutorial, we will explore the collaborative and sharing aspects of Kaggle. Kaggle provides a vibrant community of data scientists and machine learning enthusiasts where you can collaborate, share your work, and learn from others. In this tutorial, we will cover various features and functionalities that enable collaboration and sharing on Kaggle. Let‚Äôs dive in!\nStep 1: Joining Kaggle Competitions Kaggle competitions are a great way to collaborate and learn from other participants. Here‚Äôs how you can join a Kaggle competition:\nBrowse Competitions: Visit the Kaggle competitions page to explore ongoing competitions. Select a Competition: Choose a competition that interests you and aligns with your goals. Read the Rules: Make sure to carefully read and understand the competition rules, eligibility criteria, and dataset details. Join the Competition: Click on the ‚ÄúJoin Competition‚Äù button to officially join the competition and gain access to the competition forums, datasets, and evaluation metrics. # Joining a Kaggle competition competition_id = \u0026#39;titanic\u0026#39; kaggle.api.competition_join(competition_id) Step 2: Collaborating on Kaggle Notebooks Kaggle Notebooks provide an interactive environment to write, run, and share code, analysis, and visualizations. Here‚Äôs how you can collaborate on Kaggle Notebooks:\nCreate a Notebook: Click on the ‚ÄúNew Notebook‚Äù button to create a new notebook. Choose a Template: Select a programming language (Python or R) and choose a notebook template. Add Code and Explanations: Write your code in code cells and add explanations, markdown cells, and visualizations to document your analysis. Share the Notebook: Share your notebook with others by clicking on the ‚ÄúShare‚Äù button and providing the appropriate permissions. # Creating a Kaggle Notebook import pandas as pd # Load the dataset data = pd.read_csv(\u0026#39;train.csv\u0026#39;) # Perform data analysis # ... # Share the Notebook notebook_id = \u0026#39;your-notebook-id\u0026#39; kaggle.api.kernel_push(\u0026#39;your-username/notebook-title\u0026#39;, notebook_id) Step 3: Participating in Discussions and Forums Kaggle provides discussion forums where you can interact with other data scientists, ask questions, seek help, and share insights. Here‚Äôs how you can participate in discussions:\nJoin the Competition Forum: Access the competition forum to engage with other participants, discuss approaches, and seek guidance. Ask Questions: If you have any doubts or need help, create a new forum thread and ask your questions. Be sure to provide relevant details and context. Share Insights and Tips: If you discover interesting findings or have useful tips, share them with the community by creating new forum threads or commenting on existing ones. # Participating in Discussions discussion_id = \u0026#39;your-discussion-id\u0026#39; message = \u0026#39;Hello, I have a question about the feature engineering approach. Can anyone provide some guidance?\u0026#39; kaggle.api.competition_submit(discussion_id, message) Step 4: Sharing Datasets on Kaggle Kaggle allows you to share datasets with the community, enabling others to explore and utilize your data. Here‚Äôs how you can share datasets on Kaggle:\nPrepare the Dataset: Ensure that your dataset is properly formatted and documented. Create a Dataset: Click on the ‚ÄúNew Dataset‚Äù button and provide the necessary details, such as the dataset name, description , and file uploads. 3. Add Metadata: Include relevant metadata, such as tags, licenses, and data sources, to provide additional context. 4. Make it Public: Choose whether to make the dataset public or limit access to specific users or teams.\n# Sharing a Dataset dataset_name = \u0026#39;your-dataset-name\u0026#39; dataset_description = \u0026#39;This dataset contains information about housing prices.\u0026#39; files = [\u0026#39;data.csv\u0026#39;, \u0026#39;metadata.txt\u0026#39;] kaggle.api.dataset_create_new(dataset_name, files, dataset_description) Conclusion Congratulations on completing Tutorial 5: Collaborating and Sharing on Kaggle! You have learned how to join Kaggle competitions, collaborate on Kaggle Notebooks, participate in discussions and forums, and share datasets with the Kaggle community. These collaborative features are invaluable for learning, receiving feedback, and gaining exposure to different perspectives. Make the most of these functionalities, engage with the community, and continue to enhance your data science skills. In the next tutorial, we will explore advanced visualization techniques to enhance your data analysis and storytelling. Keep up the great work and happy collaborating!\n","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"065e09ce19c152bcd91c0cc10c144d6f","permalink":"https://armanasq.github.io/kaggle/tutorial-05/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/kaggle/tutorial-05/","section":"post","summary":"‚áê Kaggle\nTutorial 5: Collaborating and Sharing on Kaggle Introduction Step 1: Joining Kaggle Competitions Step 2: Collaborating on Kaggle Notebooks Step 3: Participating in Discussions and Forums Step 4: Sharing Datasets on Kaggle Conclusion Tutorial 5: Collaborating and Sharing on Kaggle Introduction Welcome to Tutorial 5 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 5","type":"post"},{"authors":null,"categories":null,"content":"Dataset\nOxIOD Dataset How to use OxIOD Dataset 24 Handheld Sequences 11 Pocket Sequences 8 Handbag Sequences 13 Trolley Sequences 8 Slow Walking Sequences 7 Running Sequences 26 Multi Devices Sequences 35 Multi Users Sequences 26 Large Scale Sequences Use OxIOD Dataset in Python References OxIOD Dataset Oxford Inertial Odometry Dataset [1] is a large set of inertial data for inertial odometry which is recorded by smartphones at 100 Hz in indoor environment. The suite consists of 158 tests and covers a distance of over 42 km, with OMC ground track available for 132 tests. Therefore, it does not include pure rotational movements and pure translational movements, which are helpful for systematically evaluating the model‚Äôs performance under different conditions; however, it covers a wide range of everyday movements.\nDue to the different focus, some information (for example, the alignment of the coordinate frames) is not accurately described. In addition, the orientation of the ground trace contains frequent irregularities (e.g., jumps in orientation that are not accompanied by similar jumps in the IMU data). The dataset is available at Link.\nHow to use OxIOD Dataset The dataset can be download from here. The Dataset Contains:\n24 Handheld Sequences Total 8821 seconds for 7193 meters.\ndata1 time (s) distance (m) seq1 376 301 seq2 234 177 seq3 188 147 seq4 216 166 seq5 322 264 seq6 325 274 seq7 141 118 total 1802 1447 data2 time (s) dis (m) seq1 326 281 seq2 312 264 seq3 301 249 total 939 794 data3 time dis seq1 308 251 seq2 379 324 seq3 609 533 seq4 538 467 seq5 383 319 total 2217 1894 data4 time dis seq1 317 242 seq2 322 243 seq3 606 476 seq4 438 359 seq5 350 284 total 2033 1604 data5 time dis seq1 310 237 seq2 594 466 seq3 560 445 seq4 366 306 total 1830 1454 11 Pocket Sequences Total 5622 seconds for 4231 meters.\ndata1 time dis seq1 330 284 seq2 456 379 seq3 506 405 seq4 491 387 seq5 240 182 total 2023 1637 data2 time dis seq1 651 492 seq2 559 414 seq3 628 429 seq4 668 494 seq5 470 371 seq6 623 494 total 3599 2694 8 Handbag Sequences Total 4100 seconds for 3431 meters.\ndata1 time dis seq1 575 437 seq2 570 467 seq3 580 466 seq4 445 366 total 2170 1736 data2 time dis seq1 575 487 seq2 560 499 seq3 425 381 seq4 370 328 total 1930 1695 13 Trolley Sequences Total 4262 seconds for 2685 meters.\ndata1 time dis seq1 447 251 seq2 309 169 seq3 359 209 seq4 599 362 seq5 612 374 seq6 586 380 seq7 274 174 total 3186 1919 data2 time dis seq1 156 106 seq2 168 118 seq3 161 113 seq4 163 113 seq5 217 158 seq6 211 158 total 1076 766 8 Slow Walking Sequences Total 4150 seconds for 2421 meters.\ndata1 time dis seq1 612 382 seq2 603 353 seq3 617 341 seq4 594 323 seq5 606 352 seq6 503 331 seq7 311 172 seq8 304 167 total 4150 2421 7 Running Sequences Total 3732 seconds for 4356 meters.\ndata1 time dis seq1 691 761 seq2 623 719 seq3 590 665 seq4 603 679 seq5 619 766 seq6 303 373 seq7 303 393 total 3732 4356 26 Multi Devices Sequences Total 7144 seconds for 5350 meters.\niPhone 5 time dis seq1 178 150 seq2 163 133 seq3 160 126 seq4 124 100 seq5 174 139 seq6 167 136 seq7 197 150 seq8 184 141 seq9 184 142 total 1531 1217 iPhone 6 time dis seq1 180 165 seq2 184 171 seq3 182 168 seq4 150 140 seq5 183 162 seq6 171 155 seq7 184 139 seq8 185 148 seq9 173 133 total 1592 1381 nexus 5 time dis seq1 604 452 seq2 609 438 seq3 605 414 seq4 609 403 seq5 607 388 seq6 607 401 seq7 186 130 seq8 194 127 total 4021 2752 35 Multi Users Sequences Total 8821 seconds for 9465 meters.\nuser 2 time dis seq1 311 284 seq2 358 313 seq3 390 328 seq4 217 172 seq5 311 240 seq6 256 193 seq7 371 296 seq8 450 375 seq9 264 221 total 2928 2422 user 3 time dis seq1 382 301 seq2 318 272 seq3 340 295 seq4 232 198 seq5 214 185 seq6 356 289 seq7 258 203 total 2100 1743 user 4 time dis seq1 387 367 seq2 329 307 seq3 305 288 seq4 248 229 seq5 356 314 seq6 293 272 seq7 297 260 seq8 468 411 seq9 435 364 total 3118 2812 user 5 time dis seq1 294 237 seq2 305 264 seq3 253 211 seq4 390 337 seq5 300 226 seq6 338 284 seq7 168 154 seq8 410 395 seq9 274 250 seq10 152 130 total 2884 2488 26 Large Scale Sequences Total 4161 seconds for 3465 meters.\nfloor1 time dis seq1 153 142 seq2 165 143 seq3 158 142 seq4 157 145 seq5 156 142 seq6 156 142 seq7 161 144 seq8 155 143 seq9 160 126 seq10 158 143 total 1579 1412 floor4 time dis seq1 160 170 seq2 157 153 seq3 162 153 seq4 118 106 seq5 164 153 seq6 163 143 seq7 169 141 seq8 166 153 seq9 172 135 seq10 169 154 seq11 166 152 seq12 165 154 seq13 165 133 seq14 164 153 seq15 163 153 seq16 159 133 total 2582 2053 In each folder, there is a raw data subfolder and a syn data subfolder, which represent the raw data collection without synchronisation but with high precise timestep, and the synchronised data but without high precise timestep.\nThe header of files is\nvicon (vi.csv)*\nTime Header translation.x translation.y translation.z rotation.x rotation.y rotation.z rotation.w Sensors (imu.csv)*\nTime attitude_roll(radians) attitude_pitch(radians) ‚Ä¶","date":1667001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667001600,"objectID":"7e1173a647a796069286da4e358a8453","permalink":"https://armanasq.github.io/datsets/OxIOD-Dataset/","publishdate":"2022-10-29T00:00:00Z","relpermalink":"/datsets/OxIOD-Dataset/","section":"post","summary":"Dataset\nOxIOD Dataset How to use OxIOD Dataset 24 Handheld Sequences 11 Pocket Sequences 8 Handbag Sequences 13 Trolley Sequences 8 Slow Walking Sequences 7 Running Sequences 26 Multi Devices Sequences 35 Multi Users Sequences 26 Large Scale Sequences Use OxIOD Dataset in Python References OxIOD Dataset Oxford Inertial Odometry Dataset [1] is a large set of inertial data for inertial odometry which is recorded by smartphones at 100 Hz in indoor environment.","tags":["IMU Dataset","OxIOD Dataset","Inertial Navigation Dataset","Inertial Odometry Dataset"],"title":"OxIOD IMU Dataset","type":"post"},{"authors":null,"categories":null,"content":"Dataset\nIntroduction RoNIN How to use RoNIN dataset HDF5 data format Use RoNIN Dataset in Python References Introduction RoNIN The RoNIN dataset [1] contains over 40 hours of IMU sensor data from 100 human subjects with 3D ground-truth trajectories under natural human movements. This data set provides measurements of the accelerometer, gyroscope, magnetometer, and ground track, including direction and location in 327 sequences and at a frequency of 200 Hz. A two-device data collection protocol was developed. A harness was used to attach one phone to the body for 3D tracking, allowing subjects to control the other phone to collect IMU data freely. It should be noted that the ground track can only be obtained using the 3D tracker phone attached to the harness. In addition, the body trajectory is estimated instead of the IMU. RoNIN datset contians 42.7 hours of IMU-motion data over 276 sequences in 3 buildings, and collected from 100 human subjects with three Android devices. The dataset is available at Link.\nHow to use RoNIN dataset The dataset can be downloaded from Here. The dataset contains following:\nData 13.81 GB\rseen_subjects_test_set.zip 3.15 GB train_dataset_1.zip 4.49 GB train_dataset_2.zip 3.18 GB unseen_subjects_test_set.zip 2.99 GB Pretrained_Models 57.05 MB ronin_body_heading.zip 579.52 KB ronin_lstm.zip 2.29 MB ronin_resnet.zip 48.49 MB ronin_tcn.zip 5.71 MB All HDF5 files are organized as follows\nHDF5 data format raw:\rtango:\rgyro, gyro_uncalib, acce, magnet, game_rv, gravity, linacce, step, tango_pose, tango_adf_pose, rv, pressure, (optional) [wifi, gps, magnetic_rv, magnet_uncalib]\rimu:\rgyro, gyro_uncalib, acce, magnet, game_rv, gravity, linacce, step. rv, pressure, (optional) [wifi, gps, magnetic_rv, magnet_uncalib]\rsynced:\rtime, gyro, gyro_uncalib, acce, magnet, game_rv, rv, gravity, linacce, step\rpose:\rtango_pos, tango_ori, (optional) ekf_ori\rUse RoNIN Dataset in Python First, we need to import libraries\nimport os import h5py import numpy as np import pandas as pd import matplotlib.pyplot as plt To access the data in each folder, a list of all folders in the directory must be created.\nfolder_name = prefixed = [filename for filename in os.listdir(\u0026#39;.\u0026#39;) if filename.startswith(\u0026#34;a\u0026#34;)] Create variables to store the data\nacc = [0, 0, 0] gyro = [0, 0, 0] mag = [0, 0, 0] ori = [0, 0, 0, 0] pose = [0, 0, 0] Import all the data files in to python\nfor i in range(len(folder_name)): load_file = folder_name[i] + \u0026#39;/data.hdf5\u0026#39; df = h5py.File(load_file, \u0026#39;r\u0026#39;) header = np.array(df.get(\u0026#39;synced\u0026#39;)) for i in range(len(np.array(df.get(\u0026#39;synced\u0026#39;)))): # np.array(df.get(\u0026#39;synced\u0026#39;))[header[i]] if header[i] == \u0026#39;acce\u0026#39;: temp = np.array(df.get(\u0026#39;synced\u0026#39;)[header[i]]) acc = np.vstack((acc, temp)) if header[i] == \u0026#39;gyro\u0026#39;: temp = np.array(df.get(\u0026#39;synced\u0026#39;)[header[i]]) gyro = np.vstack((gyro, temp)) if header[i] == \u0026#39;magnet\u0026#39;: temp = np.array(df.get(\u0026#39;synced\u0026#39;)[header[i]]) mag = np.vstack((mag, temp)) header = np.array(df.get(\u0026#39;pose\u0026#39;)) for i in range(len(np.array(df.get(\u0026#39;pose\u0026#39;)))): if header[i] == \u0026#39;ekf_ori\u0026#39;: temp = np.array(df.get(\u0026#39;pose\u0026#39;)[header[i]]) ori = np.vstack((ori, temp)) if header[i] == \u0026#39;tango_pos\u0026#39;: temp = np.array(df.get(\u0026#39;pose\u0026#39;)[header[i]]) pose = np.vstack((pose, temp)) At last, each variable could use to train or evaluate a deep learning model or just save in a CSV format similar to follows\ndf_imu = pd.DataFrame({\u0026#39;Acc x\u0026#39;: acc[1:, 0], \u0026#39;Acc y\u0026#39;: acc[1:, 1], \u0026#39;Acc z\u0026#39;: acc[1:, 2], \u0026#39;Gyro x\u0026#39;: gyro[1:, 0], \u0026#39;Gyro y\u0026#39;: gyro[1:, 1], \u0026#39;Gyro z\u0026#39;: gyro[1:, 2], \u0026#39;Mag x\u0026#39;: mag[1:, 0], \u0026#39;Mag y\u0026#39;: mag[1:, 1], \u0026#39;Mag z\u0026#39;: mag[1:, 2]}) df_imu.to_csv(\u0026#39;RoNIN_IMU.csv\u0026#39;, index=False) df_gt = pd.DataFrame({\u0026#39;Ori w\u0026#39;: ori[1:, 0], \u0026#39;Ori x\u0026#39;: ori[1:, 1], \u0026#39;Ori y\u0026#39;: ori[1:, 2], \u0026#39;Ori z\u0026#39;: ori[1:, 3], \u0026#39;Pose x\u0026#39;: pose[1:, 0], \u0026#39;Pose y\u0026#39;: pose[1:, 1], \u0026#39;Pose z\u0026#39;: pose[1:, 2]}) df_gt.to_csv(\u0026#39;RoNIN_GT.csv\u0026#39;, index=False) Also, the data could be plot by\nfs = 200 dt = 1/fs t = np.arange(0,acce.shape[0]/fs,dt) # Plot the IMU readings ## Accelermoter plt.figure(figsize=(15, 10)) plt.title(\u0026#34;Accelermoter\u0026#34;) plt.subplot(3, 1, 1) plt.plot(t, acc[1:, 0], label=\u0026#39;Acc x\u0026#39;, color=\u0026#39;b\u0026#39;) plt.legend(loc=\u0026#34;upper right\u0026#34;) plt.xlabel(\u0026#39;Time (s)\u0026#39;) plt.ylabel(\u0026#39;Acceleration in X-Axis ($m/s^2$)\u0026#39;) plt.subplot(3, 1, 2) plt.plot(t, acc[1:, 1], label=\u0026#39;Acc y\u0026#39;, color=\u0026#39;g\u0026#39;) plt.legend(loc=\u0026#34;upper right\u0026#34;) plt.xlabel(\u0026#39;Time (s)\u0026#39;) plt.ylabel(\u0026#39;Acceleration in Y-Axis ($m/s^2$)\u0026#39;) plt.subplot(3, 1, 3) plt.plot(t, acc[1:, 2], label=\u0026#39;Acc z\u0026#39;, color=\u0026#39;r\u0026#39;) plt.xlabel(\u0026#39;Time (s)\u0026#39;) plt.ylabel(\u0026#39;Acceleration in Z-Axis ($m/s^2$)\u0026#39;) plt.legend(loc=\u0026#34;upper right\u0026#34;) plt.suptitle(\u0026#34;Accelermoter\u0026#34;, fontsize=25) plt.savefig(\u0026#39;RoNIN_Acc.png\u0026#39;, dpi=300) ## Gyroscope # Plotting the three axis of the gyroscope in one figure. plt.figure(figsize=(15, 10)) plt.subplot(3, 1, 1) plt.plot(t, gyro[1:, 0], label=\u0026#39;Gyro x\u0026#39;, color=\u0026#39;b\u0026#39;) plt.legend(loc=\u0026#34;upper right\u0026#34;) plt.xlabel(\u0026#39;Time (s)\u0026#39;) plt.ylabel(\u0026#39;Angular Velocity in X-Axis ($rad/s$)\u0026#39;) plt.subplot(3, 1, 2) plt.plot(t, gyro[1:, 1], label=\u0026#39;Gyro y\u0026#39;, color=\u0026#39;g\u0026#39;) plt.legend(loc=\u0026#34;upper right\u0026#34;) plt.xlabel(\u0026#39;Time (s)\u0026#39;) ‚Ä¶","date":1666915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666915200,"objectID":"ff44140334de7ad77589f1f3f45659aa","permalink":"https://armanasq.github.io/datsets/ronin-datset/","publishdate":"2022-10-28T00:00:00Z","relpermalink":"/datsets/ronin-datset/","section":"post","summary":"Dataset\nIntroduction RoNIN How to use RoNIN dataset HDF5 data format Use RoNIN Dataset in Python References Introduction RoNIN The RoNIN dataset [1] contains over 40 hours of IMU sensor data from 100 human subjects with 3D ground-truth trajectories under natural human movements.","tags":["IMU Dataset","Inertial Navigation Dataset","Inertial Odometry Dataset"],"title":"RoNIN IMU Dataset","type":"post"},{"authors":null,"categories":null,"content":" Abstract Introduction Problem definition Background Attitude Attitude Determination from Inertial Sensors Methodology Deep Learning Model Loss Function Experiment Dataset RepoIMU T-stick RepoIMU T-pendulum Sassari OxIOD MAV Dataset EuRoC MAV TUM-VI KITTI RIDI RoNIN BROAD Training Evaluation Abstract This article discusses the importance of accurate and precise attitude determination in navigation for air and space vehicles. Various instruments and sensors have been developed over the last few decades to achieve this goal. However, the cost and complexity of these instruments can be prohibitive. To address this issue, Multi-Data Sensor Fusion (MSDF) techniques have been developed, which allow for the use of multiple sensors to sense a quantity from different perspectives or sense multiple quantities to reduce errors and uncertainties. This article explores the use of MEMS-based Inertial Measurement Units (IMUs) in attitude determination and discusses the challenges associated with noise and bias. Finally, the article describes different forms of attitude representation, including Tait-Bryan angles, rotation matrices, and quaternions.\nIntroduction Achieving accurate and precise attitude determination or estimation is essential for successful navigation of air and space vehicles. To achieve this goal, each vehicle must determine and control its attitude based on mission requirements. A wide variety of instruments, sensors, and algorithms have been developed over the last few decades, distinguished by their cost and complexity. However, using an accurate sensor can exponentially increase the cost, which may exceed the budget.\nA low-cost solution for achieving high accuracy is to use multiple sensors (homogeneous or heterogeneous) to sense a quantity from different perspectives or sense multiple quantities to reduce errors and uncertainties. Multiple sensors fuse their data to achieve a more accurate quantity, a technique called Multi-Data Sensor Fusion (MSDF). MSDF uses mathematical methods to reduce noise and uncertainty and to estimate the quantity based on prior data. MSDF can also be used for attitude determination.\nAttitude determination methods can be broadly divided into two classes: single-point and recursive estimation. The first method calculates the attitude by using two or more vector measurements at a single point in time. In contrast, recursive methods use the combination of measurements over time and the system‚Äôs mathematical model. Obtaining precise attitude determination is challenging due to errors in system modeling, processes, and measurements. Increasing the sensor‚Äôs precision may exponentially increase the cost, and sometimes, achieving the required precision may only be possible at an exorbitant cost.\nOne approach for determining attitude is to use inertial navigation algorithms based on inertial sensors. Inertial navigation is based on the Dead Reckoning method, which uses different types of inertial sensors, such as accelerometers and gyroscopes, known as Inertial Measurement Units (IMUs). The position, velocity, and attitude of a moving object can be determined using numerical integration of IMU measurements.\nThe use of low-cost Micro Electro Mechanical Systems (MEMS) based IMUs has grown in the past decade. Due to recent advances in MEMS technology, IMUs have become smaller, cheaper, and more accurate, making them available for use in mobile robots, smartphones, drones, and autonomous vehicles. However, these sensors suffer from noise and bias, which directly affect the performance of attitude estimation algorithms.\nTo tackle this problem and increase the accuracy and reliability of attitude estimation techniques, different MSDF techniques and Deep Learning models have been developed in the past few decades.\nAttitude can be represented in many different forms. The Tait-Bryan angles (also called Euler angles) are the most familiar form and are known as yaw, pitch, and roll (or heading, elevation, and bank). Engineers widely use rotation matrices and quaternions, but quaternions are less intuitive.\nRelated Work In the past decade, extensive research has been conducted on inertial navigation techniques. These studies can be roughly divided into three categories: estimation methods, Multi-Data Sensor Fusion (MSDF) techniques, and evolutionary/AI algorithms. The Kalman Filter family (i.e., EKF, UKF, MEKF), as well as other commonly used algorithms such as Madgwick and Mahony, are based on the dynamic model of the system. The Kalman filter was first introduced in [], and its variants such as EKF, UKF, and MEKF have been implemented for attitude estimation applications.\nIn [], Carsuo et al. compared different sensor fusion algorithms for inertial attitude estimation. This comparative study showed that the performance of Sensor Fusion Algorithms (SFA) is highly dependent on parameter tuning, and fixed parameter values are not suitable for all applications. Therefore, parameter tuning is one ‚Ä¶","date":1666656000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666656000,"objectID":"f11b2beebaf1c8d86ec484bd1263706f","permalink":"https://armanasq.github.io/Attitude-Estimation-CNN/","publishdate":"2022-10-25T00:00:00Z","relpermalink":"/Attitude-Estimation-CNN/","section":"post","summary":"Abstract Introduction Problem definition Background Attitude Attitude Determination from Inertial Sensors Methodology Deep Learning Model Loss Function Experiment Dataset RepoIMU T-stick RepoIMU T-pendulum Sassari OxIOD MAV Dataset EuRoC MAV TUM-VI KITTI RIDI RoNIN BROAD Training Evaluation Abstract This article discusses the importance of accurate and precise attitude determination in navigation for air and space vehicles.","tags":["Navigation","Attitude Estimation","Orientaion Estimation"],"title":"CNN Based Attitude Estimation","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 4: Advanced Model Building Techniques Introduction Step 1: Ensemble Learning Step 2: Hyperparameter Tuning Step 3: Feature Selection Step 4: Model Stacking Conclusion Tutorial 4: Advanced Model Building Techniques Introduction Welcome to Tutorial 4 of our Kaggle series! In this tutorial, we will explore advanced model building techniques that can help you improve your performance in Kaggle competitions. We will cover various topics, including ensemble learning, hyperparameter tuning, feature selection, and model stacking. By the end of this tutorial, you will have a deeper understanding of these advanced techniques and how to apply them effectively. Let‚Äôs dive in!\nStep 1: Ensemble Learning Ensemble learning involves combining multiple models to improve predictive performance. Here are a few popular ensemble techniques:\nVoting: Combine predictions from multiple models by majority voting (classification) or averaging (regression). Bagging: Train multiple models on different subsets of the training data and average their predictions. Boosting: Train models sequentially, where each subsequent model focuses on the examples the previous models struggled with. Stacking: Combine predictions from multiple models as input to a meta-model, which makes the final prediction. Ensemble learning can help improve the robustness and generalization of your models by leveraging the strengths of different algorithms.\nfrom sklearn.ensemble import VotingClassifier from sklearn.ensemble import BaggingRegressor from sklearn.ensemble import AdaBoostClassifier from sklearn.ensemble import StackingRegressor from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier # Create an ensemble of classifiers clf1 = DecisionTreeClassifier() clf2 = LogisticRegression() voting_clf = VotingClassifier(estimators=[(\u0026#39;dt\u0026#39;, clf1), (\u0026#39;lr\u0026#39;, clf2)], voting=\u0026#39;hard\u0026#39;) # Create an ensemble of bagged regressors bagging_regressor = BaggingRegressor(base_estimator=DecisionTreeRegressor()) # Create an ensemble of boosted classifiers boosted_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier()) # Create a stacked ensemble regressor stacked_regressor = StackingRegressor(estimators=[(\u0026#39;dt\u0026#39;, clf1), (\u0026#39;lr\u0026#39;, clf2)], final_estimator=RandomForestRegressor()) Step 2: Hyperparameter Tuning Hyperparameters are the settings that define how a model is trained. Tuning these hyperparameters can significantly impact model performance. Here‚Äôs how you can perform hyperparameter tuning:\nGrid Search: Define a grid of hyperparameter values and exhaustively search through all possible combinations. Random Search: Define a distribution for each hyperparameter and randomly sample combinations. Bayesian Optimization: Use Bayesian methods to efficiently search the hyperparameter space. Hyperparameter tuning can be computationally expensive, but it‚Äôs essential for finding the best configurations for your models.\nfrom sklearn.model_selection import GridSearchCV from sklearn.model_selection import RandomizedSearchCV from sklearn.ensemble import RandomForestClassifier # Grid Search param_grid = {\u0026#39;n_estimators\u0026#39;: [100, 200, 300], \u0026#39;max_depth\u0026#39;: [None, 5, 10]} grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5) grid_search.fit(X_train, y_train) # Random Search param_dist = {\u0026#39;n_estimators\u0026#39;: [100, 200, 300], \u0026#39;max_depth\u0026#39;: [None, 5, 10]} random_search = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=param_dist, cv=5) random_search.fit(X_train, y_train) Step 3: Feature Selection Feature selection is the process of selecting the most relevant features for model training. It helps reduce dimensionality, improve model interpretability, and avoid overfitting. Consider the following techniques:\nFilter Methods: Use statistical tests or correlation analysis to rank features based on their relevance. 2. Wrapper Methods: Train models with different subsets of features and select the best subset based on model performance. 3. Embedded Methods: Select features as part of the model training process (e.g., L1 regularization).\nFeature selection can be performed before or during model training, depending on the approach used.\nfrom sklearn.feature_selection import SelectKBest from sklearn.feature_selection import RFECV from sklearn.linear_model import LogisticRegression # Filter Methods - SelectKBest selector = SelectKBest(k=10) X_train_selected = selector.fit_transform(X_train, y_train) # Wrapper Methods - Recursive Feature Elimination (RFE) estimator = LogisticRegression() rfe = RFECV(estimator=estimator, step=1, cv=5) X_train_selected = rfe.fit_transform(X_train, y_train) # Embedded Methods - L1 Regularization estimator = LogisticRegression(penalty=\u0026#39;l1\u0026#39;, solver=\u0026#39;liblinear\u0026#39;) estimator.fit(X_train, y_train) Step 4: Model Stacking Model stacking is a powerful technique where predictions from multiple models are used as input to a meta-model, which then makes the final prediction. ‚Ä¶","date":1666224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666224000,"objectID":"c480aedcaca833f2b74a12ae3426fa76","permalink":"https://armanasq.github.io/kaggle/tutorial-04/","publishdate":"2022-10-20T00:00:00Z","relpermalink":"/kaggle/tutorial-04/","section":"post","summary":"‚áê Kaggle\nTutorial 4: Advanced Model Building Techniques Introduction Step 1: Ensemble Learning Step 2: Hyperparameter Tuning Step 3: Feature Selection Step 4: Model Stacking Conclusion Tutorial 4: Advanced Model Building Techniques Introduction Welcome to Tutorial 4 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 4","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 3: Participating in Kaggle Competitions Introduction Step 1: Finding Competitions Step 2: Joining a Competition Step 3: Understanding the Problem Statement Step 4: Exploring the Data Step 5: Preprocessing and Feature Engineering Step 6: Building and Training Models Step 7: Making Submissions Step 8: Learning from the Community Conclusion Tutorial 3: Participating in Kaggle Competitions Introduction Welcome to Tutorial 3 of our Kaggle series! In this tutorial, we will guide you through the process of participating in Kaggle competitions. Kaggle competitions provide a platform for data scientists to showcase their skills, learn from others, and compete for prizes. We will cover the steps involved in joining a competition, understanding the problem statement, preparing data, building models, and making submissions. By the end of this tutorial, you will have a solid understanding of how to effectively participate in Kaggle competitions. Let‚Äôs dive in!\nStep 1: Finding Competitions To participate in Kaggle competitions, you first need to find the competitions that interest you. Kaggle offers a wide range of competitions on various topics. Here‚Äôs how you can discover competitions on Kaggle:\nVisit the Kaggle website at https://www.kaggle.com and log in to your account. Click on the ‚ÄúCompetitions‚Äù tab in the top navigation bar. Browse through the list of ongoing and past competitions. Use the search bar or apply filters to find competitions based on specific criteria such as category, prize amount, or deadline. Take your time to explore the competitions, read their descriptions, and select the ones that align with your interests and expertise.\nStep 2: Joining a Competition Once you have found a competition of interest, follow these steps to join it:\nClick on the competition to view its details page. Read the competition overview, which provides information about the problem statement, evaluation metric, and rules. Review the data description, which explains the format and features of the dataset(s) provided. Click on the ‚ÄúJoin Competition‚Äù button to officially join the competition. By joining a competition, you gain access to the competition forum, datasets, and other resources specific to that competition. It‚Äôs important to carefully read and understand the competition rules and guidelines.\nStep 3: Understanding the Problem Statement Understanding the problem statement is crucial for building a successful solution. Here are the key steps to grasp the problem:\nRead the competition overview and problem statement carefully. Understand the goal and objectives of the competition. Identify the evaluation metric, which determines how your submissions will be scored. Analyze any additional constraints or specific requirements mentioned in the problem statement. A clear understanding of the problem will guide your approach and help you make informed decisions throughout the competition.\nStep 4: Exploring the Data Exploring and understanding the competition data is essential for building effective models. Follow these steps to analyze the dataset:\nDownload the competition dataset(s) from the competition‚Äôs data page. Load the data into your preferred analysis environment (e.g., Python, R, or Jupyter Notebook). Analyze the data by examining the features, distributions, relationships, and missing values. Visualize the data using appropriate plots and graphs to gain insights. Thorough data exploration will provide a solid foundation for feature engineering and model development.\nimport pandas as pd # Load the competition data train_data = pd.read_csv(\u0026#39;train.csv\u0026#39;) test_data = pd.read_csv(\u0026#39;test.csv\u0026#39;) # Explore the data print(train_data.head()) print(train_data.describe()) Step 5: Preprocessing and Feature Engineering Preprocessing and feature engineering play a critical role in improving model performance. Consider the following steps:\nHandle missing values by imputation or other techniques. Encode categorical variables using methods like one-hot encoding or label encoding. Scale numerical variables to ensure they are on a similar scale. Create new features by combining or transforming existing features. 5. Split the data into training and validation sets.\nThese preprocessing steps will prepare the data for model training and evaluation.\nimport pandas as pd from sklearn.preprocessing import StandardScaler # Preprocess the data def preprocess_data(data): # Handle missing values data.fillna(0, inplace=True) # Perform feature scaling scaler = StandardScaler() scaled_data = scaler.fit_transform(data) return scaled_data # Preprocess the training data X_train = preprocess_data(train_data.drop(\u0026#39;target\u0026#39;, axis=1)) y_train = train_data[\u0026#39;target\u0026#39;] # Preprocess the test data X_test = preprocess_data(test_data) Step 6: Building and Training Models Building and training models is a crucial step in the competition process. Consider the following steps:\nSelect appropriate algorithms based on the problem type (e.g., ‚Ä¶","date":1666051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666051200,"objectID":"ab7b9a2694755da21c370bb76ed4b145","permalink":"https://armanasq.github.io/kaggle/tutorial-03/","publishdate":"2022-10-18T00:00:00Z","relpermalink":"/kaggle/tutorial-03/","section":"post","summary":"‚áê Kaggle\nTutorial 3: Participating in Kaggle Competitions Introduction Step 1: Finding Competitions Step 2: Joining a Competition Step 3: Understanding the Problem Statement Step 4: Exploring the Data Step 5: Preprocessing and Feature Engineering Step 6: Building and Training Models Step 7: Making Submissions Step 8: Learning from the Community Conclusion Tutorial 3: Participating in Kaggle Competitions Introduction Welcome to Tutorial 3 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 3","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nTutorial 2: Exploring Datasets on Kaggle Introduction Step 1: Finding Datasets on Kaggle Step 2: Understanding Dataset Details Step 3: Previewing and Accessing the Dataset Step 4: Loading and Analyzing the Dataset Step Step 6: Refining the Analysis Conclusion Tutorial 2: Exploring Datasets on Kaggle Introduction Welcome to Tutorial 2 of our Kaggle series! In this tutorial, we will delve into the process of exploring datasets on Kaggle. Datasets form the foundation of data science projects, providing valuable insights and opportunities for analysis. Kaggle offers a vast collection of datasets across various domains, making it an ideal platform for data exploration and practice. We will cover the key aspects of dataset exploration, including finding datasets, understanding their characteristics, and performing basic data analysis. Let‚Äôs get started!\nStep 1: Finding Datasets on Kaggle Kaggle hosts a wide range of datasets, covering diverse topics such as finance, healthcare, sports, and more. Here‚Äôs how you can find datasets on Kaggle:\nVisit the Kaggle website at https://www.kaggle.com and log in to your account. Click on the ‚ÄúDatasets‚Äù tab in the top navigation bar. Explore the featured datasets on the main page or use the search bar to find specific datasets of interest. Refine your search using filters such as popularity, recency, or topic tags to narrow down the results. By browsing through the datasets, you can find interesting projects, public datasets, and valuable resources to enhance your data science skills.\nStep 2: Understanding Dataset Details Before diving into data analysis, it‚Äôs essential to understand the key details of a dataset. Here‚Äôs what you should look for:\nDescription: Read the dataset description to gain insights into its purpose, contents, and potential applications. This information helps you understand the context and scope of the dataset. Size: Check the size of the dataset, which indicates the number of records, variables, and storage requirements. Large datasets may require additional computational resources for analysis. Attributes: Identify the attributes (columns) present in the dataset. Understanding the variables and their data types helps in planning the analysis and preprocessing steps. Associated Competitions or Kernels: Check if the dataset is associated with any competitions or kernels. This provides additional context and potential approaches for analysis. Step 3: Previewing and Accessing the Dataset To explore the dataset further, you can preview its contents and access the data files. Follow these steps:\nClick on a dataset of interest to view its details page. Scroll down to the ‚ÄúData‚Äù section, where you can find the dataset files available for download. Click on a file name to preview its contents. Some datasets may offer a preview of a subset of the data, giving you a glimpse of the structure and values. Once you have an understanding of the dataset and its files, you can proceed to access the data and perform analysis using your preferred tools and programming languages.\nStep 4: Loading and Analyzing the Dataset To analyze the dataset, you need to load it into your data analysis environment. Let‚Äôs consider an example where we load a CSV file using Python and perform basic analysis:\nimport pandas as pd # Load the dataset into a Pandas DataFrame data = pd.read_csv(\u0026#39;dataset.csv\u0026#39;) # Explore the dataset # - Display the first few rows print(data.head()) # - Check the dimensions (rows, columns) print(\u0026#39;Dimensions:\u0026#39;, data.shape) # - Summary statistics print(\u0026#39;Summary Statistics:\u0026#39;, data.describe()) # - Data types of variables print(\u0026#39;Data Types:\u0026#39;, data.dtypes) # - Missing values print(\u0026#39;Missing Values:\u0026#39;, data.isnull().sum()) By loading the dataset into a DataFrame and performing basic analysis, you gain insights into the data structure, summary statistics, data types, and missing values.\nStep 5: Visualizing the Dataset Data visualization is a powerful tool for understanding the patterns and relationships within a dataset. Let‚Äôs visualize a dataset using Python‚Äôs Matplotlib library:\nimport matplotlib.pyplot as plt # Visualize the dataset # - Histogram of a numerical variable plt.hist(data[\u0026#39;age\u0026#39;]) plt.xlabel(\u0026#39;Age\u0026#39;) plt.ylabel(\u0026#39;Frequency\u0026#39;) plt.title(\u0026#39;Distribution of Age\u0026#39;) plt.show() # - Bar chart of a categorical variable plt.bar(data[\u0026#39;gender\u0026#39;].value_counts().index, data[\u0026#39;gender\u0026#39;].value_counts().values) plt.xlabel(\u0026#39;Gender\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Distribution of Gender\u0026#39;) plt.show() Visualizing the dataset provides valuable insights into the distribution, relationships, and trends present in the data.\nStep 6: Refining the Analysis After the initial exploration, you may discover areas of interest or specific questions to investigate further. This could involve advanced analysis techniques, feature engineering, or building machine learning models. Kaggle provides a collaborative environment where you can find code examples, kernels, and discussions related to the ‚Ä¶","date":1665360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665360000,"objectID":"0d55dd3304d684caaaa6e1ef6216e6cf","permalink":"https://armanasq.github.io/kaggle/tutorial-02/","publishdate":"2022-10-10T00:00:00Z","relpermalink":"/kaggle/tutorial-02/","section":"post","summary":"‚áê Kaggle\nTutorial 2: Exploring Datasets on Kaggle Introduction Step 1: Finding Datasets on Kaggle Step 2: Understanding Dataset Details Step 3: Previewing and Accessing the Dataset Step 4: Loading and Analyzing the Dataset Step Step 6: Refining the Analysis Conclusion Tutorial 2: Exploring Datasets on Kaggle Introduction Welcome to Tutorial 2 of our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 2","type":"post"},{"authors":null,"categories":null,"content":" Abstract Introduction Problem Defenition Literature Review Backgroud Methodology Deep Learning Model Loss Function Experiment Dataset Abstract Despite recent advancments in Micro-Electro Mechanical Systems (MEMS) inertial and magnetic sensors, percices and accurate attitude estimation is a challenging task, especillay in the existance of magnetic distubances or high dynamic motions. This problem cannot be significantly tackled by conventional methods and clasical estimators. In this paper, an end-to-end deep learning framework is develped to estimate the attitude and heading using inertial and magentic sensors obtained from a low-cost IMU. The proposed model consists of two-layer stacked bidirectional Long-Short Term Mermory (LSTM) and Feed Forward Neural Network layers. The model is trained using a large dataset of IMU measurements collected from publicly availabe datasets inertial orientaion and inertial odometry datasets.\nIntroduction Problem Defenition Literature Review Backgroud Methodology Deep Learning Model Loss Function Experiment Dataset ","date":1664323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664323200,"objectID":"ce01fc182182ea4c6149d77b456d798a","permalink":"https://armanasq.github.io/Attitude-Heading-Estimation/","publishdate":"2022-09-28T00:00:00Z","relpermalink":"/Attitude-Heading-Estimation/","section":"post","summary":"Abstract Introduction Problem Defenition Literature Review Backgroud Methodology Deep Learning Model Loss Function Experiment Dataset Abstract Despite recent advancments in Micro-Electro Mechanical Systems (MEMS) inertial and magnetic sensors, percices and accurate attitude estimation is a challenging task, especillay in the existance of magnetic distubances or high dynamic motions.","tags":["Navigation","Attitude Estimation","Orientaion Estimation"],"title":"Attitude and Heading Estimation","type":"post"},{"authors":null,"categories":null,"content":" Introduction Related works Problem definition Background Attitude Attitude Determination from Inertial Sensors Methodology Deep Learning Model Loss Function Experiment Dataset Introduction Achieving accurate and precise attitude determination or estimation is needed to perform successful navigation. Each flying vehicle either in air or space, needs to determine and control its attitude based on mission requirements. Vast variety of instruments/sensors and algorithm have been developed in the last decades; they are distinct by their cost and complexity. Use an accurate sensor will exponentially increase the cost which could exceed the budget. A solution for increase the accuracy with low cost is to use multi sensors (homogenous or heterogenous); multiple sensors could sense a quantity from different perspective or sense multi quantities to reduce the error and uncertainty. Multiple sensors fuse their data to achieve more accurate quantity, this method usually called as Multi-Data Sensor Fusion (MSDF). MSDF use mathematical methods to reduce noise, uncertainty and also estimate the quantity based on priori data and it could be utlized for attitude determiation. Attitude determination methods could be broadly divided in two classes, single-point and recursive estimation. First method calculates the attitude by use of two or more vector measurements at a single point of time. Instead, recursive methods use the combination of measurements over time and the system mathematical model. A precise attitude determination is dependent on sensor‚Äôs precision, accurate system modeling, and the information processing method. Obtaining this precision is considered a challenging navigation problem due to system modeling, process, and measurements errors. Increase the sensor‚Äôs precision may exponentially increase the cost; sometimes, achieving the precision requirements will only be possible for an exorbitant cost.\nOne approach for determining the attitude is using inertial navigation algorithms based inertial sensors. Inertial Navigation is based on the Dead Reckoning method. In this method, different types of inertial sensors are used such as accelerometer and gyroscope which called Inertial Measurement Unit (IMU). A moving object‚Äôs position, velocity, and attitude can be determined using numerical integration of IMU measurements.\nUsing low-cost Micro Electro Mechanical Systems (MEMS) based Inertial Measurement Unit (IMU) has been grown in the past decade. Due to recent advances in MEMS technology, IMUs became smaller, cheaper, and more accurate, and they are now available for use in mobile robots, smartphones, drones, and autonomous vehicles. This sensors suffers from noise and bias, which affect dirctly the performance attitude estimation alogrithm. In the past decades, different MSDF techniques and Deep Learning models have been developed to tackle this problem and increase the accuracy and reliability of attitude estimation techniques.\nAttitude can be represented in many different forms. The Tait-Bryan angles (also called Euler angles) are the most familiar form and known as yaw, pitch, and roll (or heading, elevation, and bank). Engineers widely use rotation matrix and quaternions, but the quaternions are less intuitive.\nRelated works In the past decade, much research has been conducted on the inertial navigation techniques. These studies could roughly divided in three categories, estimation methods, Multi-Data Sensor Fusion (MSDF) techinques, and evolutionary/AI algorithms. Kalman Filter family (i.e., EKF, UKF, MEKF) and other commonly used algorithms such as Madgwick, and Mahony are based on the dynamic model of the system. Kalman filter first introduced in [], and its vairents such as EKF, UKF, and MEKF have been implemented for attitude estimation applications.\nIn [] Carsuo et al. compared different sensor fusion algorithms for inertial attitude estimation. this comparative study showed that Sensor Fusion Algorithms (SFA) performance are highly depended to parameters tuning and fixed parameter values are not suitable for all applications. So, the parameter tuning is one the disadvantages of conventioal attitude estimation method. This problem could be tackeld by using evolutionary algorithms such as fuzzy logic and deep learning. Most of deep learning approches in inertial nvigation has focues on inertial odomotery and just few of them try to solve the inertial attitude estimation problem. Deep learning methods usually used for visual or visual-inertial based navigation. Chen et\nRochefort et al., proposed a neural networks-based satellite attitude estimation algorithm by using a quaternion neural network. This study presents a new way of integrating the neural network into the state estimator and develops a training procedure which is easy to implement. This algorithm provides the same accuracy as the EKF with significantly lower computational complexity. In [Chang 2011] a Time Varying Complementary Filter (TVCF) ‚Ä¶","date":1664064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664064000,"objectID":"b80fdd0f52265dc1acfdef79c3d592b5","permalink":"https://armanasq.github.io/attitude-estimation/attitude-estimation/","publishdate":"2022-09-25T00:00:00Z","relpermalink":"/attitude-estimation/attitude-estimation/","section":"post","summary":"Introduction Related works Problem definition Background Attitude Attitude Determination from Inertial Sensors Methodology Deep Learning Model Loss Function Experiment Dataset Introduction Achieving accurate and precise attitude determination or estimation is needed to perform successful navigation.","tags":["Navigation","Attitude Estimation","Orientaion Estimation"],"title":"Attitude Estimation","type":"post"},{"authors":null,"categories":null,"content":" Introduction RepoIMU T-stick RepoIMU T-pendulum Sassari OxIOD MAV Dataset EuRoC MAV TUM-VI KITTI RIDI RoNIN BROAD References Introduction IMU Datasets are used to evaluate the performance of the attitude estimation algorithms. In this post, we will present some of the most popular IMU Datasets. The datasets are divided into two categories: synthetic and real-world. The synthetic datasets are generated by simulating the IMU measurements. The real-world datasets are collected from the real-world experiments. The real-world experiments are divided into two categories: indoor and outdoor. The indoor experiments are conducted in a controlled environment, e.g., a laboratory. The outdoor experiments are conducted in an uncontrolled environment, e.g., a car. Also, to train, validate and test any neural network model, we need a database including accurate input and output. A Deep Learning model‚Äôs performance will be directly affected by the data that is used for it. So, to train the Deep Learning model we need a database containing the input and output parameters with following conditions:\nThe input and output parameters should be accurate. The amount of data must be sufficient to train the Deep Learning model The data should be diverse enough to cover all the possible scenarios. In the following sections, we will present some of the most popular IMU Datasets.\nRepoIMU T-stick The RepoIMU T-stick [1] is a small, low-cost, and high-performance inertial measurement unit (IMU) that can be used for a wide range of applications. The RepoIMU T-stick is a 9-axis IMU that measures the acceleration, angular velocity, and magnetic field. This database contains two separate sets of experiments recorded with a T-stick and a pendulum. A total of 29 trials were collected on the T-stick, and each trial lasted approximately 90 seconds. As the name suggests, the IMU is attached to a T-shaped stick equipped with six reflective markers. Each experiment consists of slow or fast rotation around a principal sensor axis or translation along a principal sensor axis. In this scenario, the data from the Vicon Nexus OMC system and the XSens MTi IMU are synchronized and provided at a frequency of 100 Hz. The authors clearly state that the IMU coordinate system and the ground trace are not aligned and propose a method to compensate for one of the two required rotations based on quaternion averaging. Unfortunately, some experiments contain gyroscope clipping and ground tracking, which significantly affect the obtained errors. Therefore, careful pre-processing and removal of some trials should be considered when using the dataset to evaluate the model‚Äôs accuracy. The dataset is available at Link.\nRepoIMU T-pendulum The second part of the RepoIMU dataset contains data from a triple pendulum on which the IMUs are mounted. Measurement data is provided at 90 Hz or 166 Hz. However, the IMU data contains duplicate samples. This is usually the result of artificial sampling or transmission problems where missed samples are replaced by duplicating the last sample received, effectively reducing the sampling rate. The sampling rate achieved when discarding frequent samples is about 25 Hz and 48 Hz for the accelerometer and gyroscope, respectively. Due to this issue, it is not recommended to use this database for model training and evaluation. Due to this fact, we cannot recommend using pendulum tests to evaluate the accuracy of IOE with high precision.\nSassari The Sassari dataset published in [2] aims to validate a parameter tuning approach based on the orientation difference of two IMUs of the same model. To facilitate this, six IMUs from three manufacturers (Xsens, APDM, Shimmer) are placed on a wooden board. Rotation around specific axes and free rotation around all axes are repeated at three different speeds. Data is synchronized and presented at 100 Hz. Local coordinate frames are aligned by precise manual placement. There are 18 experiments (3 speeds, 3 IMU models, and 2 IMUs of each model) in this dataset.\nAccording to these points, this database seems to be a suitable option for training, evaluating, and testing the model, but some essential points should be paid attention to. The inclusion of different speeds and different types of IMUs helped to diversify the data set. However, all motions occur in a homogeneous magnetic field and do not involve pure translational motions. Therefore, this data set does not have a robust variety in terms of the type of movement and the variety of magnetic data. Therefore, the model trained with it cannot be robust and general. However, it can be used to evaluate the model.\nThe total movement duration of all three trials is 168 seconds, with the most extended movement phase lasting 30 seconds. For this reason, considering the short time, it is not a suitable option for training. The dataset is available at Link.\nOxIOD The Oxford Inertial Odometry Dataset (OxIOD) [3] is a large set of inertial data recorded by ‚Ä¶","date":1663459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663459200,"objectID":"96717597f52c6ebacd5450530e9f8dc0","permalink":"https://armanasq.github.io/datasets/imu-datsets/","publishdate":"2022-09-18T00:00:00Z","relpermalink":"/datasets/imu-datsets/","section":"post","summary":"Introduction RepoIMU T-stick RepoIMU T-pendulum Sassari OxIOD MAV Dataset EuRoC MAV TUM-VI KITTI RIDI RoNIN BROAD References Introduction IMU Datasets are used to evaluate the performance of the attitude estimation algorithms.","tags":["Dataset","IMU","Inertial Measurement Unit"],"title":"IMU Datasets","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nKaggle Tutorial 1: Introduction to Kaggle Introduction Step 1: Creating a Kaggle Account Step 2: Exploring Datasets on Kaggle Step 3: Getting Started with Kaggle Kernels Conclusion Kaggle Tutorial 1: Introduction to Kaggle Introduction Welcome to the first tutorial in our Kaggle series! In this tutorial, we will introduce you to Kaggle, a popular online platform for data science competitions, datasets, and collaborative data science projects. Whether you‚Äôre a beginner or an experienced data scientist, Kaggle offers a wealth of resources to sharpen your skills and showcase your expertise. In this tutorial, we will cover the basics, from creating an account to exploring datasets and getting started with Kaggle Kernels. Let‚Äôs dive in!\nStep 1: Creating a Kaggle Account To get started on Kaggle, you‚Äôll need to create an account. Follow these steps:\nVisit the Kaggle website at https://www.kaggle.com. Click on the ‚ÄúSign Up‚Äù button at the top right corner of the page. Choose to sign up with your Google account or create a new Kaggle account by providing your email address and a strong password. Complete the registration process by following the instructions provided. Creating an account will give you access to a wealth of resources, including datasets, competitions, and the Kaggle community.\nStep 2: Exploring Datasets on Kaggle Kaggle provides a wide range of datasets for practice and exploration. Here‚Äôs how you can find and explore datasets:\nAfter logging in, click on the ‚ÄúDatasets‚Äù tab in the top navigation bar. Browse through the featured datasets or use the search bar to find specific datasets of interest. Click on a dataset to view its details, including the description, size, and any associated competitions or kernels. To download a dataset, click on the ‚ÄúDownload‚Äù button. Some datasets may require you to accept terms and conditions before downloading. For example, let‚Äôs use Python to load and explore a dataset:\nimport pandas as pd # Load the Kaggle datasets datasets = pd.read_csv(\u0026#39;datasets.csv\u0026#39;) # Explore the datasets print(datasets.head()) By exploring different datasets, you can gain insights, practice data preprocessing, and develop models for various data science tasks.\nStep 3: Getting Started with Kaggle Kernels Kaggle Kernels provide an interactive environment to write, run, and collaborate on code, analysis, and visualizations. Here‚Äôs how to get started with Kaggle Kernels:\nClick on the ‚ÄúKernels‚Äù tab in the top navigation bar. Explore existing kernels to gain inspiration or search for specific topics. To create a new kernel, click on the ‚ÄúNew Notebook‚Äù button. Choose a programming language (Python or R) and select a notebook template. Write your code in the provided code cells, add explanations in Markdown cells, and create visualizations. Use the ‚ÄúSave Version‚Äù button to save your work and create a new version of the kernel. You can share your kernels with others, fork existing kernels, and collaborate with the Kaggle community. For example, let‚Äôs create a simple kernel to calculate the mean of a random array using Python:\nimport numpy as np # Generate a random array data = np.random.rand(100) # Calculate the mean mean = np.mean(data) # Print the mean print(\u0026#39;Mean:\u0026#39;, mean) Kaggle Kernels allow you to experiment with different algorithms, analyze data, and share your insights with others.\nConclusion Congratulations on completing the first tutorial in our Kaggle series! In this tutorial, we covered the basics of Kaggle, from creating an account to exploring datasets and getting started with Kaggle Kernels. Kaggle offers a\nvibrant community of data scientists, machine learning enthusiasts, and experts, where you can learn, collaborate, and showcase your skills. In the upcoming tutorials, we will dive deeper into competitions, advanced modeling techniques, collaboration, and more. Stay tuned for more exciting Kaggle learning!\n","date":1663459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663459200,"objectID":"d2a6a3b22a6a96b708a2422161f56243","permalink":"https://armanasq.github.io/kaggle/tutorial-01/","publishdate":"2022-09-18T00:00:00Z","relpermalink":"/kaggle/tutorial-01/","section":"post","summary":"‚áê Kaggle\nKaggle Tutorial 1: Introduction to Kaggle Introduction Step 1: Creating a Kaggle Account Step 2: Exploring Datasets on Kaggle Step 3: Getting Started with Kaggle Kernels Conclusion Kaggle Tutorial 1: Introduction to Kaggle Introduction Welcome to the first tutorial in our Kaggle series!","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial 1","type":"post"},{"authors":null,"categories":null,"content":"‚áê Kaggle\nKaggle Tutorials A Comprehensive Guide to Using Kaggle from Scratch Introduction Step 1: Create a Kaggle Account Step 2: Explore Datasets Step 3: Join Competitions Step 4: Submit Predictions Step 5: Collaborate with Kernels Conclusion Kaggle Tutorials ‚ÄúHow-to-use-Kaggle‚Äù is a GitHub repository that provides a comprehensive guide on how to use the Kaggle platform for data science and machine learning. It covers all aspects of the platform, including creating an account, participating in competitions, using Kaggle‚Äôs cloud-based workbench and datasets, and utilizing the Kaggle API.\nGitHub Repo\nA Comprehensive Guide to Using Kaggle from Scratch Introduction Kaggle is a renowned platform that hosts data science competitions, provides datasets for practice, and offers a collaborative environment for data scientists and machine learning enthusiasts. In this comprehensive tutorial, we will delve into the process of using Kaggle from scratch, covering everything from signing up for an account to participating in competitions. By the end, you will be well-equipped to explore datasets, join competitions, collaborate with others, and enhance your data science skills. Let‚Äôs get started!\nStep 1: Create a Kaggle Account Visit the Kaggle website at https://www.kaggle.com. Click on the ‚ÄúSign Up‚Äù button at the top right corner of the page. Choose to sign up with your Google account or create a new Kaggle account by providing your email address and a strong password. Complete the registration process by following the instructions provided. Step 2: Explore Datasets Once you are logged in, click on the ‚ÄúDatasets‚Äù tab in the top navigation bar. Browse through the available datasets or use the search bar to find specific datasets of interest. Click on a dataset to view its details, including the description, size, and any associated competitions or kernels. To download a dataset, click on the ‚ÄúDownload‚Äù button. Some datasets may require you to accept terms and conditions before downloading. import pandas as pd # Load the Kaggle datasets datasets = pd.read_csv(\u0026#39;datasets.csv\u0026#39;) # Explore the datasets print(datasets.head()) Step 3: Join Competitions Navigate to the ‚ÄúCompetitions‚Äù tab in the top navigation bar. Explore the ongoing and past competitions listed on the page. You can filter them by various criteria such as popularity, deadline, or prize amount. Click on a competition to view its details, including the problem statement, evaluation metric, and dataset. To participate in a competition, click on the ‚ÄúJoin Competition‚Äù button. Read and accept the competition rules and terms to gain access to the competition‚Äôs data and submit predictions. Download the competition data by clicking on the ‚ÄúData‚Äù tab and selecting the desired files. import pandas as pd # Load the Kaggle competitions competitions = pd.read_csv(\u0026#39;competitions.csv\u0026#39;) # Explore the competitions print(competitions.head()) Step 4: Submit Predictions Once you have downloaded the competition data, analyze it, and develop your prediction model using your preferred data science tools. Generate predictions for the test set provided by the competition. Format your predictions according to the competition‚Äôs submission guidelines, typically in CSV format. Return to the competition page and click on the ‚ÄúSubmit Predictions‚Äù button. Follow the instructions to upload your submission file and make your predictions. Kaggle will evaluate your submission based on the competition‚Äôs evaluation metric and provide you with a score. import pandas as pd from sklearn.ensemble import RandomForestClassifier # Load the competition data train_data = pd.read_csv(\u0026#39;train.csv\u0026#39;) test_data = pd.read_csv(\u0026#39;test.csv\u0026#39;) # Prepare the data for training X_train = train_data.drop(\u0026#39;target\u0026#39;, axis=1) y_train = train_data[\u0026#39;target\u0026#39;] # Train a model model = RandomForestClassifier() model.fit(X_train, y_train) # Generate predictions for the test set predictions = model.predict(test_data) # Save predictions to a CSV file submission = pd.DataFrame({\u0026#39;Id\u0026#39;: test_data[\u0026#39;Id\u0026#39;], \u0026#39;Prediction\u0026#39;: predictions}) submission.to_csv(\u0026#39;submission.csv\u0026#39;, index=False) Step 5: Collaborate with Kernels Kaggle Kernels provide a platform to share and collaborate on code, analysis, and visualizations. Click on the ‚ÄúKernels‚Äù tab in the top navigation bar to access the Kaggle Kernel platform. Explore existing kernels or create a new one by clicking on the ‚ÄúNew Notebook‚Äù button. Write your code in the provided code cells and add explanations in Markdown cells. Use the ‚ÄúSave Version‚Äù button to save your work and create a new version of the kernel. You can share your kernels with others, fork existing kernels, and collaborate with the Kaggle community. import numpy as np # Generate a random array data = np.random.rand(100) # Calculate the mean mean = np.mean(data) # Print the mean print(\u0026#39;Mean:\u0026#39;, mean) Conclusion Congratulations! You have completed this comprehensive tutorial on using Kaggle from scratch. You now know how to sign up, explore ‚Ä¶","date":1663459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663459200,"objectID":"38371fcb900b3576ee95775830b25284","permalink":"https://armanasq.github.io/kaggle/tutorial-0/","publishdate":"2022-09-18T00:00:00Z","relpermalink":"/kaggle/tutorial-0/","section":"post","summary":"‚áê Kaggle\nKaggle Tutorials A Comprehensive Guide to Using Kaggle from Scratch Introduction Step 1: Create a Kaggle Account Step 2: Explore Datasets Step 3: Join Competitions Step 4: Submit Predictions Step 5: Collaborate with Kernels Conclusion Kaggle Tutorials ‚ÄúHow-to-use-Kaggle‚Äù is a GitHub repository that provides a comprehensive guide on how to use the Kaggle platform for data science and machine learning.","tags":["Kaggle","Tutorial"],"title":"Kagle Tutorial Series","type":"post"},{"authors":null,"categories":null,"content":"Self-Localization and Odometry Table of Contents Self-Localization and Odometry Table of Contents Introduction Related Work Inertial Navigation Systems Deep Learning Approaches Background Inertial Navigation Principles \\end{bmatrix} \\end{bmatrix} \\end{bmatrix} \\end{bmatrix} 6-DoF relative pose representaion 6 DoF Inertial Odometry Neural Network Network Architecture Error Metrices Quaternion Inner Product Quaternion Multiplicative Error Quaternion Shortest Geodesic Distance Loss Function Experiment Dataset Training Evaluation Simulation Results Conclusion Introduction Self-localization is one of the main challenges in the application of autonomous systems. These strategies can be divided into two major categories, GPS-based and Odometry. Odometry or position tracking is a form of navigation to detect the position and orientation of a robot by measuring the distance and angle of the robot‚Äôs movement using sensor data (e.g., inertial, visual, and radar). Position tracking is a fundamental task in autonomous navigation and it is a key component in many other applications, such as robotics, autonomous vehicles, and augmented reality. There are many odometry sensors, such as wheel encoders, inertial measurement units (IMU), and LiDAR. Odometry can be divided into two categories: dead reckoning and visual odometry. The process of dead reckoning involves calculating the current position from a previously determined position and orientation, taking into account acceleration, speed, and heading direction over a given period of time. Instead, visual odometry uses optical sensor data to analyze image sequences and provide incremental online pose estimation. In recent years, much attention has been drawn to this technique because it has high accuracy and generates less drift error than conventional methods but the high computation cost is one of its main challenges. Also, inertial sensor readings could be fused with visual odometry or can be used alone to estimate a robot‚Äôs position and orientation.\nInertial Odometery techniques typically use a combination of accelerometers and gyroscopes to estimate the 3D motion of a robot. The accelerometer measures the linear acceleration, while the gyroscope measures the angular velocity. IMUs despite all other types of sensors are independent of the environment and are egocentric. Moreover, recent advances in Micro-Electro-Mechanical Systems (MEMS) technology have enabled IMUs to become smaller, cheaper, and more accurate. They are now available for use in mobile robots, smartphones, drones, and autonomous vehicles. Low cost MEMS based IMUs are suffering from noise, drift and bias errors. Machnie learning approaches can be used to compensate these noises and biases.\nThe most challenging part of odometry estimation is trajectory tracking, independent of the type of motion. Estimate direction of gravity vector, noise and bias, simultanously is a challenging task. One of the simple suloition is double integration of accelerometer data. But, it is not accurate enough and could lead to high drift errors in the output. Another sulotion is to use a Kalman filter. The Kalman filter is a recursive Bayesian estimator that uses a series of measurements observed over time, containing statistical noise and other inaccuracies, and produces estimates of unknown variables that tend to be more precise than those based on a single measurement alone, by estimating a joint probability distribution over the variables for each timeframe.\nRecent studies have proposed a new deep learning aproch which known as Inertial Odometry Neuran Network (IONet) which could be used to estimate the 3D motion using IMUs measurements [aboldeoopio]. IONets are based on the idea of using a deep neural network to learn the relationship between the IMU raw data and the ground truth without any handcrafted engineering [chen2019]. The main advantage of this approach is that it can learn the features of the data and can be used to estimate the odometry of a robot but it requires a large amount of data to train the model. Also, it is computationally expensive. Perviouse studies shown that IONets outperforms the conventional methods in terms of accuracy and robustness.\nIn this paper, we propose a novel deep learning approach to estimate the odometry of a robot. The proposed method is based on a deep neural network that uses Long-Short Term Memory (LSTM) layers to learn the complex relationship between the IMU raw data and the ground truth. The proposed method is evaluated on a real-world dataset. The results show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and computational cost.\nRelated Work Inertial Navigation Systems A Strapdown Inertial Navigation System (SINS) works by double integrating accelerometer readings on a rigidly mounted vehicle to determine the postion. MEMS bsaed IMUs which are deployed in mobile robots, smartphones, and drones postioning and navigtion systems are ‚Ä¶","date":1663459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663459200,"objectID":"904c2dcd9cd32d01f623016205246e1b","permalink":"https://armanasq.github.io/Odometry/Self-localization/","publishdate":"2022-09-18T00:00:00Z","relpermalink":"/Odometry/Self-localization/","section":"post","summary":"Self-Localization and Odometry Table of Contents Self-Localization and Odometry Table of Contents Introduction Related Work Inertial Navigation Systems Deep Learning Approaches Background Inertial Navigation Principles \\end{bmatrix} \\end{bmatrix} \\end{bmatrix} \\end{bmatrix} 6-DoF relative pose representaion 6 DoF Inertial Odometry Neural Network Network Architecture Error Metrices Quaternion Inner Product Quaternion Multiplicative Error Quaternion Shortest Geodesic Distance Loss Function Experiment Dataset Training Evaluation Simulation Results Conclusion Introduction Self-localization is one of the main challenges in the application of autonomous systems.","tags":["Navigation","Self-localization","Odometry"],"title":"Self-Localization and Odometry","type":"post"},{"authors":null,"categories":null,"content":" Spacecraft Trajectory Optimization Introduction Prerequisites Goals Table of Contents 1. Problem Formulation 1.1 Define the Mission Parameters 1.1.1 Departure and Arrival Locations 1.1.2 Departure and Arrival Times 1.1.3 Spacecraft Mass and Constraints 1.1.4 Thrust and Propulsion System 1.1.5 Environmental Factors 1.1.6 Mission Constraints 1.2 Decision Variables 1.2.1 Spacecraft State Variables 1.2.2 Control Parameters 1.2.3 Time Parameters 1.2.4 Thrust Profiles 1.2.5 Impulsive Burns 1.3 Objective Function 1.3.1 Performance Measures 1.3.2 Time-Related Objectives 1.3.3 Targeting and Accuracy 1.3.4 Risk and Safety Considerations 1.4 Constraints 1.4.1 Dynamics and Kinematics 1.4.2 Propulsion and Fuel Constraints 1.4.3 Environmental and Safety Constraints 1.4.4 Mission-Specific Constraints 1.4.5 Resource and Operational Constraints 2. Mathematical Models in Spacecraft Trajectory Optimization Mathematical Models in Spacecraft Trajectory Optimization 2. Dynamics Modeling 2.1 Equations of Motion 2.2 Coordinate Systems 2.2.1 Inertial coordinates 2.2.2 Classical orbital elements 2.2.3 Modified equinoctial orbital elements 2.1 Models based on Transfer Type 2.1.1 Impulsive Model 2.1.2 Continuous Model 2.2 Models Based on Equations of Motion 2.2.1 Typical Two-Body Problems Inertial Coordinates Equations of Motion in Scalar and Cylindrical Form \\end{bmatrix} - Classical Orbital Elements - Modified Equinoctial Orbital Elements - 2.2.2 Rendezvous - 2.2.3 Libration points 3. Optimization Algorithms 3.1 Direct Methods 3.1.1 Single-Shooting 3.1.2 Multiple-Shooting 3.2 Indirect Methods 3.2.1 Pontryagin‚Äôs Minimum Principle 3.2.2 Variational Methods 4. Implementation in Python 4.1 Setting up the Environment 4.2 Problem Formulation 4.3 Optimization Algorithm Implementation 4.4 Visualizing and Analyzing the Results Conclusion Spacecraft Trajectory Optimization Introduction Spacecraft trajectory optimization plays a pivotal role in the realm of aerospace engineering, enabling the design of efficient and feasible paths for spacecraft to traverse between different points in space while considering a multitude of constraints and objectives. In this post we aimed to delve into advanced techniques and methodologies for spacecraft trajectory optimization.\nSpace missions demand precise and optimal trajectory planning to achieve desired objectives, such as minimizing fuel consumption, reducing mission duration, reaching specific targets, or avoiding hazardous areas. Additionally, spacecraft dynamics, propulsion systems, and mission constraints impose numerous challenges that necessitate the application of sophisticated optimization methods.\nThis post will offer a comprehensive exploration of spacecraft trajectory optimization, encompassing both theoretical foundations and practical implementation using Python. By investigating diverse optimization algorithms, formulating optimization problems, and employing visualization techniques, readers will develop the necessary expertise to address intricate trajectory optimization challenges.\nTo engage in this tutorial effectively, a solid understanding of astrodynamics is presumed, encompassing key concepts such as orbital mechanics, spacecraft dynamics, and orbital transfers. Proficiency in optimization theory and numerical methods is also advantageous. Furthermore, competence in Python programming is essential, as we will utilize prominent scientific libraries like NumPy, SciPy, and matplotlib to realize and evaluate spacecraft trajectory optimization algorithms.\nThe primary objectives of this tutorial are as follows:\nAttain an encompassing comprehension of spacecraft trajectory optimization, encompassing problem formulation, decision variables, objective functions, and constraints. Explore advanced optimization algorithms suitable for spacecraft trajectory optimization, including both direct and indirect methods. Implement spacecraft trajectory optimization algorithms utilizing Python, employing numerical techniques and optimization libraries. Visualize and analyze optimized trajectories to gain insightful perspectives on spacecraft trajectory performance. By the conclusion of this tutorial, participants will possess the requisite knowledge and competencies to address intricate spacecraft trajectory optimization problems. They will be proficient in formulating trajectory optimization problems, implementing optimization algorithms in Python, and effectively interpreting and visualizing results. This acquired expertise will empower them to contribute to the design and planning of space missions, enabling the realization of efficient and precise spacecraft trajectories for a broad spectrum of applications, ranging from satellite deployments to interplanetary missions and beyond.\nPrerequisites To make the most of this tutorial, you should have a solid understanding of astrodynamics, optimization theory, and Python programming. Familiarity with numerical methods and scientific computing libraries ‚Ä¶","date":1663459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663459200,"objectID":"d781c1a2ec1afb62322041dc9804dcf9","permalink":"https://armanasq.github.io/Orbital-Mechanics/Spacecraft-Trajectory-Optimization/","publishdate":"2022-09-18T00:00:00Z","relpermalink":"/Orbital-Mechanics/Spacecraft-Trajectory-Optimization/","section":"post","summary":"Spacecraft Trajectory Optimization Introduction Prerequisites Goals Table of Contents 1. Problem Formulation 1.1 Define the Mission Parameters 1.1.1 Departure and Arrival Locations 1.1.2 Departure and Arrival Times 1.1.3 Spacecraft Mass and Constraints 1.","tags":["Navigation","Spacecraft","Orbital-Mechanics"],"title":"Spacecraft Trajectory Optimization","type":"post"},{"authors":[],"categories":null,"content":"","date":1659366000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659366000,"objectID":"715d308a0bd5b3792f9375afcd7bdcc5","permalink":"https://armanasq.github.io/talk/neural-network-based-attitude-estimation/","publishdate":"2022-08-04T15:00:00Z","relpermalink":"/talk/neural-network-based-attitude-estimation/","section":"event","summary":"","tags":[],"title":"Neural Network based Attitude Estimation","type":"event"},{"authors":null,"categories":null,"content":"Quaternion\nGibbs Vector / Rodrigues Parameter Representation Modified Rodrigues Parameters Cayley-Klein References: Gibbs Vector / Rodrigues Parameter Representation The Gibbs vector also known as Rodrigues Parameter is a set of three parameters denoted by $ g $ (or $ P $) and can be directly derived from axis-angle $ (e, \\theta) $ or quaternion representation as follows:\n$$ \\mathbf{g} = \\frac{\\mathbf{q}_{v}}{q_0} $$\r$$ \\mathbf{g} = \\frac{e \\sin\\frac{\\theta}{2}}{\\cos\\frac{\\theta}{2}} $$\rwhere $ \\mathbf{q}_{v} $ is the vector part of the quaternion, $ e $ is the unit vector of the axis of rotation, and $ \\theta $ is the angle of rotation.\nThe Gibbs vector is a unit vector that represents the axis of rotation and the magnitude of the vector represents the angle of rotation. The Gibbs vector can be used to represent the rotation matrix $C_{\\psi\\theta\\phi}$ as:\n$$ C_{\\psi\\theta\\phi} = \\begin{bmatrix} 1 - 2(g_2^2 + g_3^2) \u0026amp; 2(g_1g_2 - g_3) \u0026amp; 2(g_1g_3 + g_2) \\\\ 2(g_1g_2 + g_3) \u0026amp; 1 - 2(g_1^2 + g_3^2) \u0026amp; 2(g_2g_3 - g_1) \\\\ 2(g_1g_3 - g_2) \u0026amp; 2(g_2g_3 + g_1) \u0026amp; 1 - 2(g_1^2 + g_2^2) \\end{bmatrix} $$\rwhere $ g_1 $, $ g_2 $, and $ g_3 $ are the components of the Gibbs vector.\nThe Gibbs vector components expressed in DCM can be calculated using the following:\n$$ g_1 = \\frac{R_{23}-R_{32}}{1+R_{11}+R_{22}+R_{33}} $$\r$$ g_2 = \\frac{R_{31}-R_{13}}{1+R_{11}+R_{22}+R_{33}} $$\r$$ g_3 = \\frac{R_{12}-R_{21}}{1+R_{11}+R_{22}+R_{33}} $$\rwhere $ R_{ij} $ is the element of the rotation matrix.\rThe Rodrigues Parameter preferred as an attitude error representation because it is a unit vector and it is easy to calculate the attitude error between two quaternions. The attitude error between two quaternions can be calculated using the following:\n$$ \\mathbf{g}_{error} = \\frac{2\\mathbf{q}_{v}}{q_0} $$\rThe Gibbs vector components expereince a singularity at $ \\theta = \\pi $, which is the same as the Euler angles. The Gibbs vector is not a good representation for small rotations.\rModified Rodrigues Parameters In attitude filter design Modified Rodrigues Parameters (MRP) is preferred for attitude error representation. The MRP is a set of three parameters denoted by $ \\mathbf{m} $ and can be directly derived from axis-angle $ (e, \\theta) $ or quaternion representation as follows:\n$$ \\mathbf{m} = \\frac{\\mathbf{q}_{v}}{1+q_0} $$\r$$ \\mathbf{m} = \\frac{e \\sin\\frac{\\theta}{2}}{1+\\cos\\frac{\\theta}{2}} $$\rwhere $ \\mathbf{q}_{v} $ is the vector part of the quaternion, $ e $ is the unit vector of the axis of rotation, and $ \\theta $ is the angle of rotation.\rDue to above equation the maximum equivalent rotation to describe is $ \\pm 360^{\\circ}$ (the singularity occurs in $ \\pm 360^{\\circ}$).\nCayley-Klein The Cayley-Klein parameters are consisting of 4 parameters which are closely related to the quaternions and denoted by matrix $ \\mathbf{K}_{2\\times 2} $.\n$$ K = \\begin{bmatrix} \\alpha \u0026amp; \\beta \\\\ \\gamma \u0026amp; \\sigma \\end{bmatrix} $$\rand satisfy the constraints\r$$ \\alpha \\bar{\\alpha} + \\gamma \\bar{\\gamma} = 1 \\\\ \\alpha \\bar{\\alpha} + \\beta \\bar{\\beta} = 1 \\\\ \\alpha \\bar{\\beta} + \\gamma \\bar{\\sigma} = 0 \\\\ \\alpha \\sigma + \\beta \\gamma = 1 \\\\ \\beta = -\\bar{\\gamma} \\\\ \\sigma = \\bar{\\alpha} $$\rwhere $ \\alpha $, $ \\beta $, $ \\gamma $, and $ \\sigma $ are the Cayley-Klein parameters and $ \\bar{\\alpha} $, $ \\bar{\\beta} $, $ \\bar{\\gamma} $, and $ \\bar{\\sigma} $ are the conjugate of the Cayley-Klein parameters.\rThe corresponding quaternions are defined_as:\n$$ \\mathbf{q}_K = \\begin{bmatrix} \\frac{ \\alpha + \\sigma }{2} \\\\ \\frac{ -i(\\beta + \\gamma) }{2} \\\\ \\frac{ \\beta - \\gamma }{2} \\\\ \\frac{ -i(\\alpha - \\sigma) }{2} \\end{bmatrix} $$\rReferences: [1] Markley, F. Landis, and John L. Crassidis. Fundamentals of spacecraft attitude determination and control. Vol. 1286. New York, NY, USA:: Springer New York, 2014. [2] Junkins, John L., and Hanspeter Schaub. Analytical mechanics of space systems. American Institute of Aeronautics and Astronautics, 2009. [3] De Ruiter, Anton H., Christopher Damaren, and James R. Forbes. Spacecraft dynamics and control: an introduction. John Wiley \u0026amp; Sons, 2012. [4] Wertz, James R., ed. Spacecraft attitude determination and control. Vol. 73. Springer Science \u0026amp; Business Media, 2012. [5] Vepa, Ranjan. Dynamics and Control of Autonomous Space Vehicles and Robotics. Cambridge University Press, 2019. [6] Shuster, Malcolm D. ‚ÄúA survey of attitude representations.‚Äù Navigation 8.9 (1993): 439-517. [7] Markley, F. Landis. ‚ÄúAttitude error representations for Kalman filtering.‚Äù Journal of guidance, control, and dynamics 26.2 (2003): 311-317. [8] Markley, F. Landis, and Frank H. Bauer. Attitude representations for Kalman filtering. No. AAS-01-309. 2001. ","date":1648771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648771200,"objectID":"1e9b8375c8c602ac2671193527009256","permalink":"https://armanasq.github.io/attitude-representations-others/","publishdate":"2022-04-01T00:00:00Z","relpermalink":"/attitude-representations-others/","section":"post","summary":"Quaternion\nGibbs Vector / Rodrigues Parameter Representation Modified Rodrigues Parameters Cayley-Klein References: Gibbs Vector / Rodrigues Parameter Representation The Gibbs vector also known as Rodrigues Parameter is a set of three parameters denoted by $ g $ (or $ P $) and can be directly derived from axis-angle $ (e, \\theta) $ or quaternion representation as follows:","tags":["Attitude","Attitude Representation"],"title":"Attitude Representation - Other","type":"post"},{"authors":null,"categories":null,"content":"Euler Angles\nEuler Parameters (Quaternions) representation References: Euler Parameters (Quaternions) representation A four-element vector with three imaginary and one real component is known as Quaternion. These hypercomplex numbers are optimum for numerical stability and memory load. The Euler parameters are a four-dimensional vector that can be used to represent the orientation of a rigid body. The Euler parameters are defined as:\n$$ q = \\begin{bmatrix} q_0 \\\\ q_1 \\\\ q_2 \\\\ q_3 \\end{bmatrix} $$\rwhere $q_0$ is the scalar part and $q_1$, $q_2$, and $q_3$ are the vector part. It could be written as:\n$$ q = q_0 + q_1i + q_2j + q_3k $$\rwhere $i$, $j$, and $k$ are the imaginary unit vectors and\n$$ i^2 = j^2 = k^2 = ijk = -1 $$\r$$ \\mathbb{q} = (q_0 , \\mathbf{q}_v) $$\rwhere $ \\mathbf{q}_v = (q_1 , q_2 , q_3) $ is the vector part of the quaternion. The quaternion is a unit quaternion if $ \\mathbb{q} \\cdot \\mathbb{q}^* = 1 $, where $ \\mathbb{q}^* $ is the conjugate of $ \\mathbb{q} $.\nIt is noticeable that some authors may use left-handed quaternions witch is defined by:\n$$ \\mathbf{q} = iq_1 + jq_2 + kq_3 + q_0 \\\\ ijk = 1 $$\rThis representation has no fundamental implications but will change the details of formulation.\rQuaternions do not have any singularity such as Euler angles. However, due to the lack of independence of components, it may present difficulties in the application of the filter equations. The quaternion is not unique, and the mirror quaternion will result in the same rotation. This is a purely mathematical representation and based upon single rotation theta around vector e with angle. It could not be used for visualization.\nQuaternion also, can be used to describe the axis-angle representation by:\n$$ \\mathbf{q} = \\begin{bmatrix}q_w \\\\ q_x \\\\ q_y \\\\ q_z\\end{bmatrix} = \\begin{bmatrix} \\cos\\frac{\\theta}{2} \\\\ v_x \\sin\\frac{\\theta}{2} \\\\ v_y \\sin\\frac{\\theta}{2} \\\\ v_z \\sin\\frac{\\theta}{2} \\end{bmatrix} = \\begin{bmatrix} \\cos\\frac{\\theta}{2} \\\\ \\mathbf{v} \\sin\\frac {\\theta}{2} \\end{bmatrix} $$\rAlso, the quaternion can be expressed in $4 \\times 4$ skew-symmetric matrix form\n$$ Q = \\begin{bmatrix} q_0 \u0026amp; -q_1 \u0026amp; -q_2 \u0026amp; -q_3 \\\\ q_1 \u0026amp; q_0 \u0026amp; -q_3 \u0026amp; q_2 \\\\ q_2 \u0026amp; q_3 \u0026amp; q_0 \u0026amp; -q_1 \\\\ q_3 \u0026amp; -q_2 \u0026amp; q_1 \u0026amp; q_0 \\end{bmatrix} $$\rThe quaternion represents the attitude of frame $A$ relative to frame $B$ defined by the following equation:\n$$ {}^{A}_{B}\\mathbf{q}={}^{B}_{A}\\mathbf{q}^* $$\rwhere $ {}^{A}_{B}\\mathbf{q} $ is the quaternion that represents the attitude of frame $ A $ relative to frame $ B $.\n$ {}^{B}_{A}\\mathbf{q}^* $ is the conjugate of the quaternion that represents the attitude of frame $ A $ relative to frame $ B $. The $ \\mathbf{q}^* $ (conjugate of the quaternion $ \\mathbf{q} $) gives the inverse rotation.\nThe relationship between quaternions and Euler angles based on $zyx$ sequence can be calculated using the following:\n$$ \\mathbf{q} = \\begin{bmatrix} \\cos\\frac{\\theta_x}{2} \\cos\\frac{\\theta_y}{2} \\cos\\frac{\\theta_z}{2} + \\sin\\frac{\\theta_x}{2} \\sin\\frac{\\theta_y}{2} \\sin\\frac{\\theta_z}{2} \\\\ \\sin\\frac{\\theta_x}{2} \\cos\\frac{\\theta_y}{2} \\cos\\frac{\\theta_z}{2} - \\cos\\frac{\\theta_x}{2} \\sin\\frac{\\theta_y}{2} \\sin\\frac{\\theta_z}{2} \\\\ \\cos\\frac{\\theta_x}{2} \\sin\\frac{\\theta_y}{2} \\cos\\frac{\\theta_z}{2} + \\sin\\frac{\\theta_x}{2} \\cos\\frac{\\theta_y}{2} \\sin\\frac{\\theta_z}{2} \\\\ \\cos\\frac{\\theta_x}{2} \\cos\\frac{\\theta_y}{2} \\sin\\frac{\\theta_z}{2} - \\sin\\frac{\\theta_x}{2} \\sin\\frac{\\theta_y}{2} \\cos\\frac{\\theta_z}{2} \\end{bmatrix} $$\rwhere $ \\theta_x $, $ \\theta_y $, and $ \\theta_z $ are the Euler angles.\nAlso, the Euler angles can be calculated using the following:\n$$ \\phi = \\arctan\\left(\\frac{2(q_0q_1 + q_2q_3)}{1 - 2(q_1^2 + q_2^2)}\\right) $$\r$$ \\theta = \\arcsin\\left(2(q_0q_2 - q_3q_1)\\right) $$\r$$ \\psi = \\arctan\\left(\\frac{2(q_0q_3 + q_1q_2)}{1 - 2(q_2^2 + q_3^2)}\\right) $$\rSince there are 12 different Euler angles sets, there are 12 quaternion to Euler angles conversion equation.\rThe quaternion can be used to represent the rotation matrix $C_{\\psi\\theta\\phi}$ as:\n$$ C_{\\psi\\theta\\phi} = \\begin{bmatrix} q_0^2 + q_1^2 - q_2^2 - q_3^2 \u0026amp; 2(q_1q_2 - q_0q_3) \u0026amp; 2(q_1q_3 + q_0q_2) \\\\ 2(q_1q_2 + q_0q_3) \u0026amp; q_0^2 - q_1^2 + q_2^2 - q_3^2 \u0026amp; 2(q_2q_3 - q_0q_1) \\\\ 2(q_1q_3 - q_0q_2) \u0026amp; 2(q_2q_3 + q_0q_1) \u0026amp; q_0^2 - q_1^2 - q_2^2 + q_3^2 \\end{bmatrix} $$\rOther Attitude Representations\nReferences: [1] Markley, F. Landis, and John L. Crassidis. Fundamentals of spacecraft attitude determination and control. Vol. 1286. New York, NY, USA:: Springer New York, 2014. [2] Junkins, John L., and Hanspeter Schaub. Analytical mechanics of space systems. American Institute of Aeronautics and Astronautics, 2009. [3] De Ruiter, Anton H., Christopher Damaren, and James R. Forbes. Spacecraft dynamics and control: an introduction. John Wiley \u0026amp; Sons, 2012. [4] Wertz, James R., ed. Spacecraft attitude determination and control. Vol. 73. Springer Science \u0026amp; Business Media, 2012. [5] Vepa, Ranjan. Dynamics and Control of Autonomous Space Vehicles ‚Ä¶","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"2cd0ff6287b5f95513aacffc0194affa","permalink":"https://armanasq.github.io/quaternion/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/quaternion/","section":"post","summary":"Euler Angles\nEuler Parameters (Quaternions) representation References: Euler Parameters (Quaternions) representation A four-element vector with three imaginary and one real component is known as Quaternion. These hypercomplex numbers are optimum for numerical stability and memory load.","tags":["Attitude","Attitude Representation"],"title":"Attitude Representation - Quaternions","type":"post"},{"authors":null,"categories":null,"content":"Attitude Representation\nEuler Angles Representation References: Euler Angles Representation A vector of three angles that represent the attitude of the coordinate frame $ i $ with respect to the coordinate frame $ j $ is called Euler angles. Euler angles are the most commonly used attitude representation because it‚Äôs easy to use and understand. One of Euler angles‚Äô obvious advantages is their intuitive representation.\n$$ \\text{Euler angles} = \\begin{bmatrix} \\phi \\\\ \\theta \\\\ \\psi \\end{bmatrix} $$\rwhere $\\phi$, $\\theta$, and $\\psi$ are the rotation angles about the $x$, $y$, and $z$ axes, respectively. The Euler angles are defined as follows:\n$$ \\phi = \\arctan\\left(\\frac{R_{32}}{R_{33}}\\right) \\\\ \\theta = \\arcsin\\left(-R_{31}\\right) \\\\ \\psi = \\arctan\\left(\\frac{R_{21}}{R_{11}}\\right) $$\rwhere $R_{ij}$ is the element of the rotation matrix $R$.\rRoll: Rotation around the x-axis with angle $ \\phi $ $$ C_{\\phi} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; cos(\\phi) \u0026amp; sin(\\phi) \\\\ 0 \u0026amp; -sin(\\phi) \u0026amp; cos(\\phi) \\end{bmatrix} $$\r* **Pitch**: Rotation around the y-axis with angle $ \\theta $\r$$ C_{\\theta} = \\begin{bmatrix} cos(\\theta) \u0026amp; 0 \u0026amp; -sin(\\theta) \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ sin(\\theta) \u0026amp; 0 \u0026amp; cos(\\theta) \\end{bmatrix} $$\r* **Yaw**: Rotation around the z-axis with angle $ \\psi $\r$$ C_{\\psi} = \\begin{bmatrix} cos(\\psi) \u0026amp; sin(\\psi) \u0026amp; 0 \\\\ -sin(\\psi) \u0026amp; cos(\\psi) \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\rEuler angles represent three consecutive rotations, and they could be defined in twelve different orders. The most common order is the yaw-pitch-roll (YPR) order, which is also called the z-y-x order. The rotation matrix can be written as:\r$$ C_{\\psi\\theta\\phi} = C_{\\psi}C_{\\theta}C_{\\phi} $$\r$$ C_{\\psi\\theta\\phi} = \\begin{bmatrix} cos(psi)cos(\\theta) \u0026amp; cos(psi)sin(\\theta)sin(\\phi)-sin(psi)cos(\\phi) \u0026amp; cos(psi)sin(\\theta)cos(\\phi)+sin(psi)sin(\\phi) \\\\ sin(psi)cos(\\theta) \u0026amp; sin(psi)sin(\\theta)sin(\\phi)+cos(psi)cos(\\phi) \u0026amp; sin(psi)sin(\\theta)cos(\\phi)-cos(psi)sin(\\phi) \\\\ -sin(\\theta) \u0026amp; cos(\\theta)sin(\\phi) \u0026amp; cos(\\theta)cos(\\phi) \\end{bmatrix} $$\rThe Euler angles of the rotation matrix $C_{\\phi\\theta\\psi}$ can be written as:\r$$ \\phi = \\arctan\\left(\\frac{C_{32}}{C_{33}}\\right) $$\r$$ \\theta = \\arctan\\left(\\frac{C_{32}}{\\sqrt{1-C_{32}^2}}\\right) $$\r$$ \\psi = \\arctan\\left(\\frac{C_{31}}{C_{33}}\\right) $$\rThe Euler angles are not unique. For example, the Euler angles $ (0,0,0) $ and $ (2\\pi,2\\pi,2\\pi) $ represent the same rotation. The Euler angles are also not invariant to the order of the rotations. For example, the Euler angles $ R_{x,y,z}(0,0,0) $ and $ R_{z,y,x}(0,0,0) $ represent the same rotation, but the rotation matrix is different.\rThree rotation angles $\\phi$, $\\theta$, and $\\psi$ are about the sequential displaced body-fixed axes, and twelve different sequences are possible that can be used for the same rotation. The location of each sequential rotation depends on the preceding rotation, and there are divided into two main categories:\nSymmetric sequences: The first and third rotations are performed around the same axis, second rotation is performed around one of the two others:\r$$ R_{i,j,i}(\\alpha, \\beta, \\gamma) = R_i(\\alpha)R_j(\\beta)R_i(\\gamma) $$\rSymmetric sequence $ (i,j,i)$, $ i \\ne j$, $ \\alpha, \\beta, \\gamma \\in \\mathbb{R}$ Asymmetric sequences: All rotations performed around three different axes: $$ R_{i,j,k}(\\alpha, \\beta, \\gamma) = R_i(\\alpha)R_j(\\beta)R_k(\\gamma) $$\rAsymmetric sequence $ (i,j,k)$, $ i \\ne j \\ne k \\ne i$, $ \\alpha, \\beta, \\gamma \\in \\mathbb{R}$ These angles are not unique, and the mirror angles will result in the same rotations.\nFor Symmetric sequences: $ R(\\alpha, \\beta, \\gamma) = R(\\alpha + \\pi, -\\beta,\\gamma - \\pi) $ For Asymmetric sequences: $ R(\\alpha, \\beta, \\gamma) = R(\\alpha + \\pi, \\pi -\\beta,\\gamma - \\pi) $ The main disadvantages of Euler angles are:\nSingularity\rNon-uniqueness Non-invariance Less accuracy for integration of attitude incremental changes over time\rAt $ \\theta = \\left(\\pm\\frac{\\pi}{2}\\right) $ the singularities will occur and usually known as mathematical gimble lock where to axes are parallel to each other.\nQuaternion\nReferences: [1] Markley, F. Landis, and John L. Crassidis. Fundamentals of spacecraft attitude determination and control. Vol. 1286. New York, NY, USA:: Springer New York, 2014. [2] Junkins, John L., and Hanspeter Schaub. Analytical mechanics of space systems. American Institute of Aeronautics and Astronautics, 2009. [3] De Ruiter, Anton H., Christopher Damaren, and James R. Forbes. Spacecraft dynamics and control: an introduction. John Wiley \u0026amp; Sons, 2012. [4] Wertz, James R., ed. Spacecraft attitude determination and control. Vol. 73. Springer Science \u0026amp; Business Media, 2012. [5] Vepa, Ranjan. Dynamics and Control of Autonomous Space Vehicles and Robotics. Cambridge University Press, 2019. [6] Shuster, Malcolm D. ‚ÄúA survey of attitude representations.‚Äù Navigation 8.9 (1993): 439-517. [7] Markley, F. Landis. ‚ÄúAttitude error representations for Kalman filtering.‚Äù Journal of ‚Ä¶","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"a4e058888fd5a0ecb7a3f732cd62f978","permalink":"https://armanasq.github.io/euler-angles/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/euler-angles/","section":"post","summary":"Attitude Representation\nEuler Angles Representation References: Euler Angles Representation A vector of three angles that represent the attitude of the coordinate frame $ i $ with respect to the coordinate frame $ j $ is called Euler angles.","tags":["Attitude","Attitude Representation"],"title":"Attitude Representation - Euler Angles","type":"post"},{"authors":null,"categories":null,"content":" Direction Cosine Matrix (DCM) Axis-Angle Representation References: Attitude\nAttitude representation is a set of coordinates that fully describe a rigid body‚Äôs orientation with respect to a reference frame. There are an infinite number of attitude representations, each of which has strengths and weaknesses. Choosing the proper attitude representation depends on the estimation algorithm, type of the moving object (e.g. satellite, spacecraft), type of mission, and reference frame selection. Attitude representation impacts mathematical complexity, geometrical singularities, and operational range, so it‚Äôs crucial to choose the proper representation for the objectives. At least, three coordinates are needed to describe the attitude in a 3D space that has at least one singularity. Singularities can be avoided by using four or more coordinates, but even the use of four coordinates does not guarantee their avoidance.\nThere are various attitude representations that are common in the industry, such as Direction Cosine Matrix, Euler angles, Euler Parameters (Quaternions), Gibb‚Äôs vectors, and so on. We will describe a few of them below.\nTo maintain consistency in mathematical notations, two reference frames (as a reference frame) and (as a body frame) have been defined as follows:\n$$ N \\equiv \\begin{bmatrix} n_1 \\\\ n_2 \\\\ n_3 \\end{bmatrix}, B \\equiv \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} $$\rDirection Cosine Matrix (DCM) In mathematics, a direction cosine matrix (DCM) is a matrix that transforms coordinate reference frames. Attitude Matrix, also known as DCM, is the most fundamental and redundant method of describing relative attitudes.\n$$\r\\mathbf{R} =\r\\begin{bmatrix}\rr_{11} \u0026amp; r_{12} \u0026amp; r_{13} \\\\\rr_{21} \u0026amp; r_{22} \u0026amp; r_{23} \\\\\rr_{31} \u0026amp; r_{32} \u0026amp; r_{33}\r\\end{bmatrix} \\in \\mathbb{R}^{3\\times 3}\r$$\rThere are nine parameters, of which six are redundant due to orthogonality. The DCM elements can be described as the dot product of coordinate system axes, which express the base vector as follows:\n$$ DCM = \\begin{bmatrix} b_1 \\cdot n_1 \u0026amp; b_1 \\cdot n_2 \u0026amp; b_1 \\cdot n_3 \\\\ b_2 \\cdot n_1 \u0026amp; b_2 \\cdot n_2 \u0026amp; b_2 \\cdot n_3 \\\\ b_3 \\cdot n_1 \u0026amp; b_3 \\cdot n_2 \u0026amp; b_3 \\cdot n_3\\end{bmatrix} $$\rIn the other hand, the cosine of three angles between each body vector $ b_i, (i=1,2,3) $ and three axes $ n_i, (i=1,2,3) $ are called the direction cosine matrix.\r$$ b_i = cos(\\alpha_{i1}\\mathbf{n}_1) + cos(\\alpha_{i2}\\mathbf{n}_2) + cos(\\alpha_{i3}\\mathbf{n}_3) \\\\ i=1,2,3 $$\rSo, the direction cosine matrix can be rewritten by:\n$$ DCM = \\begin{bmatrix} cos(\\alpha_{11}) \u0026amp; cos(\\alpha_{12}) \u0026amp; cos(\\alpha_{13}) \\\\ cos(\\alpha_{21}) \u0026amp; cos(\\alpha_{22}) \u0026amp; cos(\\alpha_{23}) \\\\ cos(\\alpha_{31}) \u0026amp; cos(\\alpha_{32}) \u0026amp; cos(\\alpha_{33}) \\end{bmatrix} $$\rSo,\n$$ \\hat{\\mathbf{b}} = \\text{DCM} \\hat{\\mathbf{n}} $$\rwhere $\\hat{\\mathbf{b}}$ and $\\hat{\\mathbf{n}}$ are the unit vectors of the body and reference frames, respectively.\nAxis-Angle Representation Euler‚Äôs theorem states that all rotations of a solid object can be expressed as single rotation $ \\theta $ about a unit length axis $ e $ in the rotation plane. In other words, each orthogonal matrix $ R $ has a specified unit vector rotation axis donated $ e $, known as Euler axis, and a single rotation angle $ \\theta $ is called Euler angle. The axis angle representation can be written as:\n$$ \\theta \\mathbf{e}= \\begin{bmatrix} \\theta e_1 \\ \\theta e_2 \\ \\theta e_3 \\end{bmatrix} $$\nwhere\n$$ \\mathbf{e} = \\begin{bmatrix} e_1 \\ e_2 \\ e_3 \\end{bmatrix} $$\nand\n$$ |\\mathbf{e}| = 1 $$\nSince, $(e,\\theta)$ and $(-e,-\\theta)$ correspond to the same rotation, it‚Äôs not a unique representation. The axis-angle representation is not a good choice for attitude estimation because it has a singularity at $\\theta = \\pi$. The axis-angle representation is also not a good choice for attitude control because it is not a linear representation. The axis-angle representation is a good choice for attitude visualization. The axis-angle representation is also a good choice for attitude initialization.\nEuler Angles\nReferences: [1] Markley, F. Landis, and John L. Crassidis. Fundamentals of spacecraft attitude determination and control. Vol. 1286. New York, NY, USA:: Springer New York, 2014. [2] Junkins, John L., and Hanspeter Schaub. Analytical mechanics of space systems. American Institute of Aeronautics and Astronautics, 2009. [3] De Ruiter, Anton H., Christopher Damaren, and James R. Forbes. Spacecraft dynamics and control: an introduction. John Wiley \u0026amp; Sons, 2012. [4] Wertz, James R., ed. Spacecraft attitude determination and control. Vol. 73. Springer Science \u0026amp; Business Media, 2012. [5] Vepa, Ranjan. Dynamics and Control of Autonomous Space Vehicles and Robotics. Cambridge University Press, 2019. [6] Shuster, Malcolm D. ‚ÄúA survey of attitude representations.‚Äù Navigation 8.9 (1993): 439-517. [7] Markley, F. Landis. ‚ÄúAttitude error representations for Kalman filtering.‚Äù Journal of guidance, control, and dynamics 26.2 (2003): 311-317. [8] Markley, ‚Ä¶","date":1642377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642377600,"objectID":"12e3d84d021f296df8d612f754b6ad6f","permalink":"https://armanasq.github.io/attitude-representation/","publishdate":"2022-01-17T00:00:00Z","relpermalink":"/attitude-representation/","section":"post","summary":"Direction Cosine Matrix (DCM) Axis-Angle Representation References: Attitude\nAttitude representation is a set of coordinates that fully describe a rigid body‚Äôs orientation with respect to a reference frame. There are an infinite number of attitude representations, each of which has strengths and weaknesses.","tags":["Attitude","Attitude Representation"],"title":"Attitude Representation","type":"post"},{"authors":null,"categories":null,"content":"Table of Contents Table of Contents Introduction Attitude and Attitude Terminology References Introduction Attitude determination and control play a vital role in Aerospace engineering. Most aerial or space vehicles have subsystem(s) that must be pointed to a specific direction, known as pointing modes, e.g., Sun pointing, Earth pointing. For example, communications satellites, keeping satellites antenna pointed to the Earth continuously, is the key to the successful mission. That will be achieved only if we have proper knowledge of the vehicle‚Äôs orientation; in other words, the attitude must be determined. In this post, the fundamental concepts for defining the attitude of an object in the three-dimensional space will be presented. It is necessary to have a clear view of the exact meaning of the attitude or orientation. So, at first, the attitude and attitude terminology will be defined. Then the mathematical relationships between the attitude and the angular velocity will be presented. Finally, the attitude and angular velocity will be used to define the attitude dynamics.\nAttitude and Attitude Terminology Attitude is the mathematical representation of the orientation in space related to the reference frames. Attitude parameters (attitude coordinates) refer to sets of parameters (coordinates) that fully describe a rigid body‚Äôs attitude, which is not unique expressions. At least three parameters are required to describe the orientation uniquely. The process of determining these parameters is called attitude determination. Attitude determination methods can be divided in two categories: static and dynamic.\nStatic Attitude Determination is a point-to-point time-independent attitude-determining method with the memoryless approach, also known as attitude determination. It is the observations or measurements processing to obtain the information for describing the object‚Äôs orientation relative to a reference frame. It could be determined by measuring the directions from the vehicle to the known points, i.e., Attitude Knowledge. Due to accuracy limit, measurement noise, model error, and process error, most deterministic approaches are inefficient for accurate prospects; in this situation, using statistical methods will be a good solution.\nDynamic Attitude Determination methods, also known as Attitude estimation, refer to using mathematical methods and techniques (e.g., statistical and probabilistic) to predict and estimate the future attitude based on a dynamic model and prior measurements. These techniques fuse data that retain a series of measurements using algorithms such as filtering, Multi-Sensor-Data-Fusion.\nSuppose we consider attitude estimation as mathematical methods and attitude determination as instruments and measurements. In that case, we could find that no such works had been done in attitude estimation until the eighteenth or nineteenth century, as M.D. Shuster mentioned in [ 1, 2] attitude estimation is a young and underdeveloped field such that Sputnik 1 (the first artificial satellite) and Echo 1 (the first passive communications satellite experiment) did not have attitude determination and control system (ADCS). Also, the next generation of spacecraft has an attitude control system without any attitude estimation. Those spacecraft used passive attitude control methods such as gravity gradient attitude stabilization.\nAt first, two frames must be defined to formulate the attitude, the body frame $B$ and the Observer frame $O$. Then we can define the attitude as the orientation of the $B$ frame with respect to the $O$ frame. Usually, the rigid body orientation is given with respect to an inertial frame called Inertial Fixed References System (IFRS). As mentioned before, attitude is a set of coordinates which defines the orientation. It could be a 3D vector which is represented by a 3D rotation matrix. The basic rotation matrix (also called elemental rotation) is a 3x3 matrix which is used to rotate the coordinate system by an angle $\\theta$ about $x$, $y$, or $z$ axis and defined by the following equation:\n$$ R_x = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\cos(\\theta) \u0026amp; -\\sin(\\theta) \\\\ 0 \u0026amp; \\sin(\\theta) \u0026amp; \\cos(\\theta) \\end{bmatrix} $$\r$$ R_y = \\begin{bmatrix} \\cos(\\theta) \u0026amp; 0 \u0026amp; \\sin(\\theta) \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ -\\sin(\\theta) \u0026amp; 0 \u0026amp; \\cos(\\theta) \\end{bmatrix} $$\r$$ R_z = \\begin{bmatrix} \\cos(\\theta) \u0026amp; -\\sin(\\theta) \u0026amp; 0 \\\\ \\sin(\\theta) \u0026amp; \\cos(\\theta) \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} $$\rwhere $\\theta$ is the angle of rotation.\nAttitude Representation\nReferences [ 1 ] M. D. Shuster, ‚ÄúIn my estimation,‚Äù The Journal of the Astronautical Sciences, 2006. [ 2 ]\tM. D. Shuster, ‚ÄúBeyond estimation,‚Äù Advances in the Astronautical Sciences, 2006. ","date":1642032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642032000,"objectID":"862b5161d1fc5dae29e531634e34b9bf","permalink":"https://armanasq.github.io/attitude/","publishdate":"2022-01-13T00:00:00Z","relpermalink":"/attitude/","section":"post","summary":"Table of Contents Table of Contents Introduction Attitude and Attitude Terminology References Introduction Attitude determination and control play a vital role in Aerospace engineering. Most aerial or space vehicles have subsystem(s) that must be pointed to a specific direction, known as pointing modes, e.","tags":["Attitude","Attitude Estimation","Attitude Determination"],"title":"Attitude","type":"post"},{"authors":[],"categories":null,"content":"","date":1627830000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627830000,"objectID":"44d558fe077eec1cd9735efe799294c6","permalink":"https://armanasq.github.io/talk/quantum-computation/","publishdate":"2021-08-04T15:00:00Z","relpermalink":"/talk/quantum-computation/","section":"event","summary":"Information Battle: Classical Vs Quantum Computers","tags":[],"title":"Quantum Computation","type":"event"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://armanasq.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Features Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"a24a6ce1a396bf9198e4ef098724b98a","permalink":"https://armanasq.github.io/slides/oxml2023/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/oxml2023/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"\rAI-driven Space Exploration Researcher Arman Asgharpoor Golroudbari\rAI-driven Space Exploration Researcher\nEmail: a.asgharpoor1993@gmail.com\nEducation M.Sc. in Space Engineering\nUniversity of Tehran, Tehran, Iran (2019-2022)\nThesis: Design and Simulation of Attitude and Heading Estimation Algorithm\nMBA\nAcademic Center for Education, Culture and Research, Tehran, Iran (2019-2020)\nProject: Utilizing AI for personalized medicine and diagnosis\nB. Eng. in Aircraft Avionics Technology\nUniversity of Applied Science and Technology, Tehran, Iran (2016-2019)\nAssociate in Avionics\nCivil Aviation Technology College, Tehran, Iran (2013-2016)\nResearch Focus Advancing Space Exploration through AI\rI am Arman Asgharpoor Golroudbari, an AI-driven Space Exploration Researcher with a relentless drive to unlock the mysteries of the cosmos. With a profound passion for the intersection of artificial intelligence and space exploration, I am committed to pushing the boundaries of human knowledge and revolutionizing the future of space missions.\nAt the esteemed University of Tehran\u0026#39;s Space Lab, under the mentorship of Dr. M.H. Sabour, I am at the forefront of pioneering research endeavors aimed at enhancing deep space exploration missions. My primary focus lies in leveraging artificial intelligence, machine learning, and estimation theory to advance sensor fusion algorithms for accurate attitude estimation in inertial navigation systems.\nBy harnessing state-of-the-art techniques such as deep learning, neural networks, and Bayesian estimation, I am pushing the boundaries of sensor data integration, enabling precise and robust navigation capabilities in the harsh and dynamic deep space environment. My research aims to revolutionize the reliability and efficiency of future space missions, paving the way for unprecedented discoveries and advancements.\nProfessional Experience Contributing to the Scientific Community\rIn addition to my groundbreaking research, I actively contribute to the scientific community and professional networks in the following capacities:\nReferee, Research Council, Students\u0026#39; Scientific Research Center (SSRC)\rMember, Universal Scientific Education and Research Network (USERN)\rLeveraging my expertise and knowledge, I contribute to the evaluation and review process of scientific research projects, ensuring the highest standards of quality and rigor. As a member of this esteemed international network, I collaborate with like-minded professionals, fostering innovation and fostering interdisciplinary collaborations.\nGet in Touch I welcome the opportunity to connect with fellow researchers, space enthusiasts, and potential collaborators. Feel free to reach out to me for any inquiries or exciting discussions:\nEmail: yonkov.atanas@gmail.com\nStay Connected Stay updated with my latest discoveries, publications, and insights by following me on social media:\nFacebook Twitter Instagram GitHub ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6083a88ee3411b0d17ce02d738f69d47","permalink":"https://armanasq.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"AI-driven Space Exploration Researcher Arman Asgharpoor Golroudbari\rAI-driven Space Exploration Researcher\nEmail: a.asgharpoor1993@gmail.com\nEducation M.Sc. in Space Engineering\nUniversity of Tehran, Tehran, Iran (2019-2022)\nThesis: Design and Simulation of Attitude and Heading Estimation Algorithm","tags":null,"title":"About Me","type":"page"},{"authors":null,"categories":null,"content":"Education M.Sc. in Space Engineering, University of Tehran, 2022 (GPA:4.0/4.0) School Ranking: 1st in Iranian Universities (U.S, News) Oxford Machine Learning Summer School (OxML 2022) Thesis: AI Application in Inertial Navigation Sensor fusion algorithms were combined with Deep Learning to improve inertial attitude estimation accuracy Ray and Sherpa were used for Hyperparameter Optimization (PBT, Grid \u0026amp; Random Search) in Python (Keras \u0026amp; Pytorch) End-to-End ANN Frameworks were developed for Inertial Odometry (6DoF \u0026amp; 9DoF) End-to-End ANN Frameworks were developed for Attitude Estimation (2DoF \u0026amp; 3DoF) MBA in Healthcare, Academic Center for Education, Culture and Research, 2020 (GPA:4.0/4.0) Projects: Elderly tourism Application of AI in personalized medicine B. Eng. in Avionics, Aviation Industry Training Center, 2019 (GPA:3.8/4.0) A.E.T in Avionics, Civil Aviation Technology College, 2016 Work experience Research Assistant, Aviation Industry Training Center - Fall 2018 ~ Fall 2020 Supervised undergraduate students working on the research project by: Conducted literature reviews; collected, managed, and analyzed data Provided ready access to all experimental data for the faculty researcher and supervisor Martial Arts Instructor, Iran Martial Arts Federation - Mar. 2016 ‚Äì Present Black Belt Dan II Improve communication skills by teaching students from various backgrounds Manager, Arman Imen Passargad - Jan. 2013 ‚Äì Present Improve leadership and management skills by working with different people in harsh work environments Skills AI Deep Learning (LSTM, CNN, TCN, etc.), PBT Hyperparameter Optimization Fuzzy Logic / Fuzzy Inference System Robotics Programming: Python (Matplotlib, NumPy, Pandas, TensorFlow, Keras), MATLAB, Simulink, Arduino, C++, LaTeX CAD-CAM: CATIA, SolidWorks, Inventor, Proteus, Altium Designer, AutoCAD CAE: CST, ADS, ANSYS Workbench, Abaqus, COMSOL Publications {{ range .Site.RegularPages.ByType \u0026#34;publication\u0026#34; }}\r{{ .Title }}\r{{ end }}\rTalks {% for post in site.talks %}\r{% include archive-single-talk-cv.html %}\r{% endfor %}\rTeaching {% for post in site.teaching %}\r{% include archive-single-cv.html %}\r{% endfor %}\rResearch Experience Summer Project, University College London - Jul.2022 ‚Äì Present Use Generative Adversarial Imitation Learning and Reinforcement Learning Created path planning of a ground robot via Python in the ROS environment. Former of Fuzzy Logic Lab - Nov. 2020 Universal Scientific Education \u0026amp; Research Network Interest Group Aimed to do research on Multi-Criteria Decision Making ShadX Team Leader, AIAA Aircraft Design Competition - Aug. 2020 As a Graduate Team Aircraft Design, designed a modern regional jet family Referee of Research Council, Students‚Äô Scientific Research Center - Apr. 2019 ‚Äì Present Evaluated research proposals CNC Milling Machine Design and Fabrication, Aviation Industry Training Center - Jul. 2019 ‚Äì Jan. 2020 Used embedded systems to control CNC Used SolidWorks and Inventor to design the structure CanSat Competition Design and Fabrication, University of Tehran - Sep. 2019 ‚Äì Mar. 2020 Used Raspberry Pi for Computer Vision and Pattern Detection Sensor Fusion implemented for Navigation and State Estimation (KF Family) Wireless Power Transmission (WPT) for Medical Purposes - Sep. 2018 ‚Äì Jul. 2019 Circuit Designer and Analyzer Used ANSYS, ADS, CST, and Altium Designer to design and analyze the circuit Lab Experience Space Lab, University of Tehran - Sep. 2019 ‚Äì Sep. 2022 Used 3-DoF experimental test bed for integrated attitude dynamics and control Used LabView and ARM development boards Fuzzy Logic Lab, University of Tehran, and USERN - Nov. 2019 ‚Äì Present Done research on Fuzzy Inference Systems based projects, such as Fuzzy tuned complementary filters for IMU-based attitude estimation. Advisor: Dr. M. H. Sabour Websites University of Tehran Fuzzy Logic Lab USERN Fuzzy Logic Lab Interest Group ResearchGate Avionics Lab, Aviation Industry Training Center - Sep. 2018 ‚Äì Sep. 2020 Worked with Aircraft Instrument Panels and different flight instruments Altimeter, Attitude, Airspeed, Vertical Speed, and Heading Indicator Electronics Lab, Aviation Industry Training Center - Sep. 2018 ‚Äì Sep. 2020 Designed and assembled PCBs: Fire extinguisher, Flight Management System (FMS) simulator and etc. Used various measuring tools: Function Generator, Oscilloscope, LCR meter, etc. Aircraft Instruments Lab, Civil Aviation Technology College - Oct. 2015 ‚Äì Aug. 2016 Lab redesigned, and inventory management was done to improve student performance Repaired different flight instruments Leadership Experience Universal Scientific Education \u0026amp; Research Network - Jan. 2021 ‚Äì Jan. 2022 6th International USERN Congress \u0026amp; Prize Awarding Festival, Executive Member Research Week, Executive Member Lab Techniques School, Executive Member Minatare Talk, Executive Member USERN Health \u0026amp; Art, 7th International Festival of Paintings for Pediatric Patients, Executive Member R\u0026amp;D, Publicity, Media, and IT, ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7e6115c4d209379befeb3847e8c4905b","permalink":"https://armanasq.github.io/cv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cv/","section":"","summary":"Education M.Sc. in Space Engineering, University of Tehran, 2022 (GPA:4.0/4.0) School Ranking: 1st in Iranian Universities (U.S, News) Oxford Machine Learning Summer School (OxML 2022) Thesis: AI Application in Inertial Navigation Sensor fusion algorithms were combined with Deep Learning to improve inertial attitude estimation accuracy Ray and Sherpa were used for Hyperparameter Optimization (PBT, Grid \u0026 Random Search) in Python (Keras \u0026 Pytorch) End-to-End ANN Frameworks were developed for Inertial Odometry (6DoF \u0026 9DoF) End-to-End ANN Frameworks were developed for Attitude Estimation (2DoF \u0026 3DoF) MBA in Healthcare, Academic Center for Education, Culture and Research, 2020 (GPA:4.","tags":null,"title":"CV","type":"page"},{"authors":null,"categories":null,"content":" At the Students‚Äô Scientific Research Center: office Innovation Center, Number 56, Vesal Shirazi St, Tehran, Iran\nAt University of Tehran: office Space Lab, North Kargar St, Tehran, Iran\nAt Education Development Center (EDC): No.57, Hojatdust St, Keshavarz Blvd, Tehran, Iran\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fbfe9718c8b1ffc1f742e3d3af89cce8","permalink":"https://armanasq.github.io/location/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/location/","section":"","summary":"At the Students‚Äô Scientific Research Center: office Innovation Center, Number 56, Vesal Shirazi St, Tehran, Iran\nAt University of Tehran: office Space Lab, North Kargar St, Tehran, Iran\nAt Education Development Center (EDC): No.","tags":null,"title":"Office and Mailing Address","type":"page"}]